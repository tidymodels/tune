<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Support for parallel processing in tune — parallelism • tune</title><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet"><script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/Source_Sans_Pro-0.4.10/font.css" rel="stylesheet"><link href="../deps/Source_Code_Pro-0.4.10/font.css" rel="stylesheet"><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet"><link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet"><script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Support for parallel processing in tune — parallelism"><meta name="description" content="tune can enable simultaneous parallel computations. Tierney (2008)
defined different classes of parallel processing techniques:
Implicit is when a function uses low-level tools to perform a
calculation that is small in scope in parallel. Examples are using
multithreaded linear algebra libraries (e.g., BLAS) or basic R vectorization
functions.
Explicit parallelization occurs when the user requests that some
calculations should be run by generating multiple new R (sub)processes. These
calculations can be more complex than those for implicit parallel
processing.


For example, some decision tree libraries can implicitly parallelize their
search for the optimal splitting routine using multiple threads.
Alternatively, if you are resampling a model B times, you can explicitly
create B new R jobs to train B boosted trees in parallel and return their
resampling results to the main R process (e.g., fit_resamples()).
There are two frameworks that can be used to explicitly parallel process
your work in tune: the future package and the
mirai package. Previously, you could use the
foreach package, but this has been deprecated as of
version 1.2.1 of tune.
By default, no parallelism is used to process models in tune; you have
to opt-in.
Using future


You should install the package and choose your flavor of parallelism using
the plan function. This allows you to specify the number of
worker processes and the specific technology to use.
For example, you can use:
   library(future)
   plan(multisession, workers = 4)

and work will be conducted simultaneously (unless there is an exception; see
the section below).
If you had previously used foreach, this would replace your existing
code that probably looked like:
   library(doBackend)
   registerDoBackend(cores = 4)

See future::plan() for possible options other than multisession.
Note that tune resets the maximum limit of memory of global variables
(e.g., attached packages) to be greater than the default when the package is
loaded. This value can be altered using options(future.globals.maxSize).
If you want future to use mirai parallel workers, you can
install and load the future.mirai package.



Using mirai


To set the specific for parallel processing with mirai, use the
mirai::daemons() function. The first argument, n, determines the number
of parallel workers. Using daemons(0) reverts to sequential processing.
The arguments url and remote are used to set up and launch parallel
processes over the network for distributed computing. See mirai::daemons()
documentation for more details.



Reverting to sequential processing


There are a few times when you might specify that you wish to use parallel
processing, but it will revert to sequential execution:
Many of the control functions (e.g. control_grid()) have an argument
called allow_par. If this is set to FALSE, parallel backends will
always be ignored.
Some packages, such as rJava and keras are not compatible with
explicit parallelization. If any of these packages are used, sequential
processing occurs.
If you specify fewer than two workers, or if there is only a single task,
the computations will occur sequentially.





Expectations for reproducibility


We advise that you always run set.seed() with a seed value just prior to
using a function that uses (or might use) random numbers. Given this:
You should expect to get the same results if you run that section of code
repeatedly, conditional on using version 1.4.0 of tune.
You should expect differences in results between version 1.4.0 of tune and
previous versions.
When using last_fit(), you should be able to get the same results as
manually using generics::fit() and predict() to do the same work.
When running with or without parallel processing (using any backend
package), you should be able to achieve the same results from
fit_resamples() and the various tuning functions.


Specific exceptions:
For SVM classification models using the kernlab package, the random
number generator is independent of R, and there is no argument to control
it. Unfortunately, it is likely to give you different results from
run-to-run.
For some deep learning packages (e.g., tensorflow, keras, and
torch), it is very difficult to achieve reproducible results. This
is especially true when using GPUs for computations. Additionally, we have
seen differences in computations (stochastic or non-random) between
platforms due to the packages' use of different numerical tolerance
constants across operating systems.





Handling package dependencies


tune knows what packages are required to fit a workflow object.
When computations are run sequentially, an initial check is made to see if
they are installed. This triggers the packages to be loaded but not visible
in the search path.
In parallel, the required packages are fully loaded (i.e., loaded and seen
in the search path), as they were previously with foreach, in the
worker processes (but not the main R session).

"><meta property="og:description" content="tune can enable simultaneous parallel computations. Tierney (2008)
defined different classes of parallel processing techniques:
Implicit is when a function uses low-level tools to perform a
calculation that is small in scope in parallel. Examples are using
multithreaded linear algebra libraries (e.g., BLAS) or basic R vectorization
functions.
Explicit parallelization occurs when the user requests that some
calculations should be run by generating multiple new R (sub)processes. These
calculations can be more complex than those for implicit parallel
processing.


For example, some decision tree libraries can implicitly parallelize their
search for the optimal splitting routine using multiple threads.
Alternatively, if you are resampling a model B times, you can explicitly
create B new R jobs to train B boosted trees in parallel and return their
resampling results to the main R process (e.g., fit_resamples()).
There are two frameworks that can be used to explicitly parallel process
your work in tune: the future package and the
mirai package. Previously, you could use the
foreach package, but this has been deprecated as of
version 1.2.1 of tune.
By default, no parallelism is used to process models in tune; you have
to opt-in.
Using future


You should install the package and choose your flavor of parallelism using
the plan function. This allows you to specify the number of
worker processes and the specific technology to use.
For example, you can use:
   library(future)
   plan(multisession, workers = 4)

and work will be conducted simultaneously (unless there is an exception; see
the section below).
If you had previously used foreach, this would replace your existing
code that probably looked like:
   library(doBackend)
   registerDoBackend(cores = 4)

See future::plan() for possible options other than multisession.
Note that tune resets the maximum limit of memory of global variables
(e.g., attached packages) to be greater than the default when the package is
loaded. This value can be altered using options(future.globals.maxSize).
If you want future to use mirai parallel workers, you can
install and load the future.mirai package.



Using mirai


To set the specific for parallel processing with mirai, use the
mirai::daemons() function. The first argument, n, determines the number
of parallel workers. Using daemons(0) reverts to sequential processing.
The arguments url and remote are used to set up and launch parallel
processes over the network for distributed computing. See mirai::daemons()
documentation for more details.



Reverting to sequential processing


There are a few times when you might specify that you wish to use parallel
processing, but it will revert to sequential execution:
Many of the control functions (e.g. control_grid()) have an argument
called allow_par. If this is set to FALSE, parallel backends will
always be ignored.
Some packages, such as rJava and keras are not compatible with
explicit parallelization. If any of these packages are used, sequential
processing occurs.
If you specify fewer than two workers, or if there is only a single task,
the computations will occur sequentially.





Expectations for reproducibility


We advise that you always run set.seed() with a seed value just prior to
using a function that uses (or might use) random numbers. Given this:
You should expect to get the same results if you run that section of code
repeatedly, conditional on using version 1.4.0 of tune.
You should expect differences in results between version 1.4.0 of tune and
previous versions.
When using last_fit(), you should be able to get the same results as
manually using generics::fit() and predict() to do the same work.
When running with or without parallel processing (using any backend
package), you should be able to achieve the same results from
fit_resamples() and the various tuning functions.


Specific exceptions:
For SVM classification models using the kernlab package, the random
number generator is independent of R, and there is no argument to control
it. Unfortunately, it is likely to give you different results from
run-to-run.
For some deep learning packages (e.g., tensorflow, keras, and
torch), it is very difficult to achieve reproducible results. This
is especially true when using GPUs for computations. Additionally, we have
seen differences in computations (stochastic or non-random) between
platforms due to the packages' use of different numerical tolerance
constants across operating systems.





Handling package dependencies


tune knows what packages are required to fit a workflow object.
When computations are run sequentially, an initial check is made to see if
they are installed. This triggers the packages to be loaded but not visible
in the search path.
In parallel, the required packages are fully loaded (i.e., loaded and seen
in the search path), as they were previously with foreach, in the
worker processes (but not the main R session).

"><meta property="og:image" content="https://tune.tidymodels.org/logo.png"><meta name="robots" content="noindex"><script src="https://cdn.jsdelivr.net/gh/posit-dev/supported-by-posit/js/badge.min.js" data-max-height="43" data-hide-below="1200" data-light-bg="#666f76" data-light-fg="#f9f9f9"></script><script defer data-domain="tune.tidymodels.org,all.tidymodels.org" src="https://plausible.io/js/plausible.js"></script></head><body>
    <a href="#container" class="visually-hidden-focusable">Skip to content</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-none" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">tune</a>

    <small class="nav-text text-danger me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="In-development version">2.0.1.9001</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="nav-item"><a class="nav-link" href="../articles/tune.html">Get started</a></li>
<li class="active nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles"><li><a class="dropdown-item" href="../articles/acquisition_functions.html">Acquisition functions</a></li>
    <li><a class="dropdown-item" href="../articles/extras/optimizations.html">Optimizations and parallel processing</a></li>
  </ul></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-learn-more" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Learn more</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-learn-more"><li><a class="external-link dropdown-item" href="https://www.tidymodels.org/learn/work/tune-svm/">Grid search</a></li>
    <li><a class="external-link dropdown-item" href="https://www.tidymodels.org/learn/work/bayes-opt/">Bayesian optimization of classification model</a></li>
    <li><a class="external-link dropdown-item" href="https://www.tidymodels.org/learn/work/tune-text/">Tuning text models</a></li>
  </ul></li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul><ul class="navbar-nav"><li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json"></form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/tidymodels/tune/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul></div>


  </div>
</nav><div class="container template-reference-topic" id="container">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.png" class="logo" alt=""><h1>Support for parallel processing in tune</h1>
      <small class="dont-index">Source: <a href="https://github.com/tidymodels/tune/blob/main/R/parallel.R" class="external-link"><code>R/parallel.R</code></a></small>
      <div class="d-none name"><code>parallelism.Rd</code></div>
    </div>

    <div class="ref-description section level2">
    <p><span class="pkg">tune</span> can enable simultaneous parallel computations. Tierney (2008)
defined different classes of parallel processing techniques:</p><ul><li><p><em>Implicit</em> is when a function uses low-level tools to perform a
calculation that is small in scope in parallel. Examples are using
multithreaded linear algebra libraries (e.g., BLAS) or basic R vectorization
functions.</p></li>
<li><p><em>Explicit</em> parallelization occurs when the user requests that some
calculations should be run by generating multiple new R (sub)processes. These
calculations can be more complex than those for implicit parallel
processing.</p></li>
</ul><p>For example, some decision tree libraries can implicitly parallelize their
search for the optimal splitting routine using multiple threads.</p>
<p>Alternatively, if you are resampling a model <em>B</em> times, you can explicitly
create <em>B</em> new R jobs to train <em>B</em> boosted trees in parallel and return their
resampling results to the main R process (e.g., <code><a href="fit_resamples.html">fit_resamples()</a></code>).</p>
<p>There are two frameworks that can be used to explicitly parallel process
your work in <span class="pkg">tune</span>: the <a href="https://future.futureverse.org/reference/future.html" class="external-link">future</a> package and the
<a href="https://mirai.r-lib.org/reference/mirai.html" class="external-link">mirai</a> package. Previously, you could use the
<a href="https://rdrr.io/pkg/foreach/man/foreach.html" class="external-link">foreach</a> package, but this has been deprecated as of
version 1.2.1 of tune.</p>
<p>By default, no parallelism is used to process models in <span class="pkg">tune</span>; you have
to opt-in.</p><div class="section">
<h3 id="using-future">Using future<a class="anchor" aria-label="anchor" href="#using-future"></a></h3>


<p>You should install the package and choose your flavor of parallelism using
the <a href="https://future.futureverse.org/reference/plan.html" class="external-link">plan</a> function. This allows you to specify the number of
worker processes and the specific technology to use.</p>
<p>For example, you can use:</p>
<p></p><div class="sourceCode r"><pre><code><span>   <span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://future.futureverse.org" class="external-link">future</a></span><span class="op">)</span></span>
<span>   <span class="fu"><a href="https://future.futureverse.org/reference/plan.html" class="external-link">plan</a></span><span class="op">(</span><span class="va">multisession</span>, workers <span class="op">=</span> <span class="fl">4</span><span class="op">)</span></span></code></pre><p></p></div>
<p>and work will be conducted simultaneously (unless there is an exception; see
the section below).</p>
<p>If you had previously used <span class="pkg">foreach</span>, this would replace your existing
code that probably looked like:</p>
<p></p><div class="sourceCode r"><pre><code><span>   <span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va">doBackend</span><span class="op">)</span></span>
<span>   <span class="fu">registerDoBackend</span><span class="op">(</span>cores <span class="op">=</span> <span class="fl">4</span><span class="op">)</span></span></code></pre><p></p></div>
<p>See <code><a href="https://future.futureverse.org/reference/plan.html" class="external-link">future::plan()</a></code> for possible options other than <code>multisession</code>.</p>
<p>Note that <span class="pkg">tune</span> resets the <em>maximum</em> limit of memory of global variables
(e.g., attached packages) to be greater than the default when the package is
loaded. This value can be altered using <code>options(future.globals.maxSize)</code>.</p>
<p>If you want <span class="pkg">future</span> to use <span class="pkg">mirai</span> parallel workers, you can
install and load the <span class="pkg">future.mirai</span> package.</p>
</div>

<div class="section">
<h3 id="using-mirai">Using mirai<a class="anchor" aria-label="anchor" href="#using-mirai"></a></h3>


<p>To set the specific for parallel processing with <span class="pkg">mirai</span>, use the
<code><a href="https://mirai.r-lib.org/reference/daemons.html" class="external-link">mirai::daemons()</a></code> function. The first argument, <code>n</code>, determines the number
of parallel workers. Using <code>daemons(0)</code> reverts to sequential processing.</p>
<p>The arguments <code>url</code> and <code>remote</code> are used to set up and launch parallel
processes over the network for distributed computing. See <code><a href="https://mirai.r-lib.org/reference/daemons.html" class="external-link">mirai::daemons()</a></code>
documentation for more details.</p>
</div>

<div class="section">
<h3 id="reverting-to-sequential-processing">Reverting to sequential processing<a class="anchor" aria-label="anchor" href="#reverting-to-sequential-processing"></a></h3>


<p>There are a few times when you might specify that you wish to use parallel
processing, but it will revert to sequential execution:</p><ul><li><p>Many of the control functions (e.g. <code><a href="control_grid.html">control_grid()</a></code>) have an argument
called <code>allow_par</code>. If this is set to <code>FALSE</code>, parallel backends will
always be ignored.</p></li>
<li><p>Some packages, such as <span class="pkg">rJava</span> and <span class="pkg">keras</span> are not compatible with
explicit parallelization. If any of these packages are used, sequential
processing occurs.</p></li>
<li><p>If you specify fewer than two workers, or if there is only a single task,
the computations will occur sequentially.</p></li>
</ul></div>

<div class="section">
<h3 id="expectations-for-reproducibility">Expectations for reproducibility<a class="anchor" aria-label="anchor" href="#expectations-for-reproducibility"></a></h3>


<p>We advise that you <em>always</em> run <code><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed()</a></code> with a seed value just prior to
using a function that uses (or might use) random numbers. Given this:</p><ul><li><p>You should expect to get the same results if you run that section of code
repeatedly, conditional on using version 1.4.0 of tune.</p></li>
<li><p>You should expect differences in results between version 1.4.0 of tune and
previous versions.</p></li>
<li><p>When using <code><a href="last_fit.html">last_fit()</a></code>, you should be able to get the same results as
manually using <code><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">generics::fit()</a></code> and <code><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict()</a></code> to do the same work.</p></li>
<li><p>When running with or without parallel processing (using any backend
package), you should be able to achieve the same results from
<code><a href="fit_resamples.html">fit_resamples()</a></code> and the various tuning functions.</p></li>
</ul><p>Specific exceptions:</p><ul><li><p>For SVM classification models using the <span class="pkg">kernlab</span> package, the random
number generator is independent of R, and there is no argument to control
it. Unfortunately, it is likely to give you different results from
run-to-run.</p></li>
<li><p>For some deep learning packages (e.g., <span class="pkg">tensorflow</span>, <span class="pkg">keras</span>, and
<span class="pkg">torch</span>), it is very difficult to achieve reproducible results. This
is especially true when using GPUs for computations. Additionally, we have
seen differences in computations (stochastic or non-random) between
platforms due to the packages' use of different numerical tolerance
constants across operating systems.</p></li>
</ul></div>

<div class="section">
<h3 id="handling-package-dependencies">Handling package dependencies<a class="anchor" aria-label="anchor" href="#handling-package-dependencies"></a></h3>


<p><span class="pkg">tune</span> knows what packages are required to fit a workflow object.</p>
<p>When computations are run sequentially, an initial check is made to see if
they are installed. This triggers the packages to be loaded but not visible
in the search path.</p>
<p>In parallel, the required packages are fully loaded (i.e., loaded and seen
in the search path), as they were previously with <span class="pkg">foreach</span>, in the
worker processes (but not the main R session).</p>
</div>

    </div>


    <div class="section level2">
    <h2 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a></h2>
    <p><a href="https://www.tmwr.org/grid-search#parallel-processing" class="external-link">https://www.tmwr.org/grid-search#parallel-processing</a></p>
<p>Tierney, Luke. "Implicit and explicit parallel computing in R." COMPSTAT
2008: Proceedings in Computational Statistics. Physica-Verlag HD, 2008.</p>
    </div>

  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside></div>


   </div>
  <footer><div class="container">
  <div class="pkgdown-footer-left">
  <p>Developed by <a href="https://github.com/topepo" class="external-link">Max Kuhn</a>, <a href="https://www.posit.co" class="external-link"><img src="https://www.tidyverse.org/posit-logo.svg" alt="Posit" height="16" width="62" style="margin-bottom: 3px;"></a>.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.2.0.</p>
</div>

  </div></footer></body></html>

