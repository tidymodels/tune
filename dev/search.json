[{"path":[]},{"path":"https://tune.tidymodels.org/dev/CODE_OF_CONDUCT.html","id":"our-pledge","dir":"","previous_headings":"","what":"Our Pledge","title":"Contributor Covenant Code of Conduct","text":"members, contributors, leaders pledge make participation community harassment-free experience everyone, regardless age, body size, visible invisible disability, ethnicity, sex characteristics, gender identity expression, level experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, sexual identity orientation. pledge act interact ways contribute open, welcoming, diverse, inclusive, healthy community.","code":""},{"path":"https://tune.tidymodels.org/dev/CODE_OF_CONDUCT.html","id":"our-standards","dir":"","previous_headings":"","what":"Our Standards","title":"Contributor Covenant Code of Conduct","text":"Examples behavior contributes positive environment community include: Demonstrating empathy kindness toward people respectful differing opinions, viewpoints, experiences Giving gracefully accepting constructive feedback Accepting responsibility apologizing affected mistakes, learning experience Focusing best just us individuals, overall community Examples unacceptable behavior include: use sexualized language imagery, sexual attention advances kind Trolling, insulting derogatory comments, personal political attacks Public private harassment Publishing others’ private information, physical email address, without explicit permission conduct reasonably considered inappropriate professional setting","code":""},{"path":"https://tune.tidymodels.org/dev/CODE_OF_CONDUCT.html","id":"enforcement-responsibilities","dir":"","previous_headings":"","what":"Enforcement Responsibilities","title":"Contributor Covenant Code of Conduct","text":"Community leaders responsible clarifying enforcing standards acceptable behavior take appropriate fair corrective action response behavior deem inappropriate, threatening, offensive, harmful. Community leaders right responsibility remove, edit, reject comments, commits, code, wiki edits, issues, contributions aligned Code Conduct, communicate reasons moderation decisions appropriate.","code":""},{"path":"https://tune.tidymodels.org/dev/CODE_OF_CONDUCT.html","id":"scope","dir":"","previous_headings":"","what":"Scope","title":"Contributor Covenant Code of Conduct","text":"Code Conduct applies within community spaces, also applies individual officially representing community public spaces. Examples representing community include using official e-mail address, posting via official social media account, acting appointed representative online offline event.","code":""},{"path":"https://tune.tidymodels.org/dev/CODE_OF_CONDUCT.html","id":"enforcement","dir":"","previous_headings":"","what":"Enforcement","title":"Contributor Covenant Code of Conduct","text":"Instances abusive, harassing, otherwise unacceptable behavior may reported community leaders responsible enforcement codeofconduct@posit.co. complaints reviewed investigated promptly fairly. community leaders obligated respect privacy security reporter incident.","code":""},{"path":"https://tune.tidymodels.org/dev/CODE_OF_CONDUCT.html","id":"enforcement-guidelines","dir":"","previous_headings":"","what":"Enforcement Guidelines","title":"Contributor Covenant Code of Conduct","text":"Community leaders follow Community Impact Guidelines determining consequences action deem violation Code Conduct:","code":""},{"path":"https://tune.tidymodels.org/dev/CODE_OF_CONDUCT.html","id":"id_1-correction","dir":"","previous_headings":"Enforcement Guidelines","what":"1. Correction","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Use inappropriate language behavior deemed unprofessional unwelcome community. Consequence: private, written warning community leaders, providing clarity around nature violation explanation behavior inappropriate. public apology may requested.","code":""},{"path":"https://tune.tidymodels.org/dev/CODE_OF_CONDUCT.html","id":"id_2-warning","dir":"","previous_headings":"Enforcement Guidelines","what":"2. Warning","title":"Contributor Covenant Code of Conduct","text":"Community Impact: violation single incident series actions. Consequence: warning consequences continued behavior. interaction people involved, including unsolicited interaction enforcing Code Conduct, specified period time. includes avoiding interactions community spaces well external channels like social media. Violating terms may lead temporary permanent ban.","code":""},{"path":"https://tune.tidymodels.org/dev/CODE_OF_CONDUCT.html","id":"id_3-temporary-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"3. Temporary Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: serious violation community standards, including sustained inappropriate behavior. Consequence: temporary ban sort interaction public communication community specified period time. public private interaction people involved, including unsolicited interaction enforcing Code Conduct, allowed period. Violating terms may lead permanent ban.","code":""},{"path":"https://tune.tidymodels.org/dev/CODE_OF_CONDUCT.html","id":"id_4-permanent-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"4. Permanent Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Demonstrating pattern violation community standards, including sustained inappropriate behavior, harassment individual, aggression toward disparagement classes individuals. Consequence: permanent ban sort public interaction within community.","code":""},{"path":"https://tune.tidymodels.org/dev/CODE_OF_CONDUCT.html","id":"attribution","dir":"","previous_headings":"","what":"Attribution","title":"Contributor Covenant Code of Conduct","text":"Code Conduct adapted Contributor Covenant, version 2.1, available https://www.contributor-covenant.org/version/2/1/code_of_conduct.html. Community Impact Guidelines inspired [Mozilla’s code conduct enforcement ladder][https://github.com/mozilla/inclusion]. answers common questions code conduct, see FAQ https://www.contributor-covenant.org/faq. Translations available https://www.contributor-covenant.org/translations.","code":""},{"path":"https://tune.tidymodels.org/dev/CONTRIBUTING.html","id":null,"dir":"","previous_headings":"","what":"Contributing to tidymodels","title":"Contributing to tidymodels","text":"detailed information contributing tidymodels packages, see development contributing guide.","code":""},{"path":"https://tune.tidymodels.org/dev/CONTRIBUTING.html","id":"documentation","dir":"","previous_headings":"","what":"Documentation","title":"Contributing to tidymodels","text":"Typos grammatical errors documentation may edited directly using GitHub web interface, long changes made source file. YES ✅: edit roxygen comment .R file R/ directory. 🚫: edit .Rd file man/ directory. use roxygen2, Markdown syntax, documentation.","code":""},{"path":"https://tune.tidymodels.org/dev/CONTRIBUTING.html","id":"code","dir":"","previous_headings":"","what":"Code","title":"Contributing to tidymodels","text":"submit 🎯 pull request tidymodels package, always file issue confirm tidymodels team agrees idea happy basic proposal. tidymodels packages work together. package contains unit tests, integration tests tests using packages contained extratests. pull requests, recommend create fork repo usethis::create_from_github(), initiate new branch usethis::pr_init(). Look build status making changes. README contains badges continuous integration services used package. New code follow tidyverse style guide. can use styler package apply styles, please don’t restyle code nothing PR. user-facing changes, add bullet top NEWS.md current development version header describing changes made followed GitHub username, links relevant issue(s)/PR(s). use testthat. Contributions test cases included easier accept. contribution spans use one package, consider building extratests changes check breakages /adding new tests . Let us know PR ran extra tests. tune package, test objects take long build every commit. contribution changes structure tuning resampling objects, regenerate test objects using scripts inst/examples/ follows: terminal, run: , move .Rout files inst/examples/. Check files committing .","code":"R CMD BATCH --vanilla inst/examples/test_objects.R R CMD BATCH --vanilla inst/examples/ames_knn.R"},{"path":"https://tune.tidymodels.org/dev/CONTRIBUTING.html","id":"code-of-conduct","dir":"","previous_headings":"Code","what":"Code of Conduct","title":"Contributing to tidymodels","text":"project released Contributor Code Conduct. contributing project, agree abide terms.","code":""},{"path":"https://tune.tidymodels.org/dev/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2023 tune authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://tune.tidymodels.org/dev/articles/extras/optimizations.html","id":"sub-model-speed-ups","dir":"Articles > Extras","previous_headings":"","what":"Sub-model speed-ups","title":"Optimizations and parallel processing","text":"types models, boosted models regularized models, number models actually fit can far less number models evaluated. example, suppose boosted tree fit 1000 trees. Many boosting implementations let user make predictions number trees less originally fit (1000 example). “sub-model trick” can greatly speed training time many models (e.g. see example caret documentation). order know models allow , parsnip package contains multi_predict() function enables feature. Printing S3 methods lists possible models: feature exist recipes though.","code":"library(tidymodels) methods(\"multi_predict\") ##  [1] multi_predict._C5.0*        multi_predict._earth*       ##  [3] multi_predict._elnet*       multi_predict._glmnetfit*   ##  [5] multi_predict._lognet*      multi_predict._multnet*     ##  [7] multi_predict._torch_mlp*   multi_predict._train.kknn*  ##  [9] multi_predict._xgb.Booster* multi_predict.default*      ## see '?methods' for accessing help and source code # There are arguments for the parameter(s) that can create multiple predictions. # For xgboost, `trees` are cheap to evaluate:  parsnip:::multi_predict._xgb.Booster %>%    formals() %>%    names() ## [1] \"object\"   \"new_data\" \"type\"     \"trees\"    \"...\""},{"path":"https://tune.tidymodels.org/dev/articles/extras/optimizations.html","id":"expensive-pre-processing","dir":"Articles > Extras","previous_headings":"","what":"Expensive pre-processing","title":"Optimizations and parallel processing","text":"tuning recipe model, makes sense avoid recreating recipe model. example, suppose Isomap multi-dimensional scaling used pre-process data prior tuning K-nearest neighbor regression: following grid: evaluate 350 candidate models, compute recipe 70 times per resample. Since Isomap expensive, really inefficient. tune_grid() determines occurs fits 70 candidate models unique configuration recipe. essence, nests model parameters inside unique parameters recipe: parallel_over = \"resamples\", default, 5 recipes prepared , within , appropriate models fit recipe. example, recipe num_terms = 1 created, model parameters iteratively tuned: true post-processing parameters tuned. unique set recipe model parameters, post-processing parameters evaluated without unnecessary re-fitting. Also, using model formula, model matrix created per resample.","code":"data(Chicago) iso_rec <-    recipe(ridership ~ ., data = Chicago) %>%    step_dummy(all_nominal()) %>%    step_isomap(all_predictors(), num_terms = tune()) ## 3 packages (dimRed, RSpectra, and RANN) are needed for this step but ## are not installed. ## To install run: `install.packages(c(\"dimRed\", \"RSpectra\", \"RANN\"))` knn_mod <-    nearest_neighbor(neighbors = tune(), weight_func = tune()) %>%    set_engine(\"kknn\") %>%    set_mode(\"regression\") grid <-    parameters(num_terms(c(1, 9)), neighbors(), weight_func()) %>%    grid_regular(levels = c(5, 10, 7)) %>%    arrange(num_terms, neighbors, weight_func) grid ## # A tibble: 350 × 3 ##    num_terms neighbors weight_func  ##        <int>     <int> <chr>        ##  1         1         1 biweight     ##  2         1         1 cos          ##  3         1         1 epanechnikov ##  4         1         1 inv          ##  5         1         1 rectangular  ##  6         1         1 triangular   ##  7         1         1 triweight    ##  8         1         2 biweight     ##  9         1         2 cos          ## 10         1         2 epanechnikov ## # ℹ 340 more rows ## # A tibble: 5 × 2 ##   num_terms data              ##       <int> <list>            ## 1         1 <tibble [70 × 2]> ## 2         3 <tibble [70 × 2]> ## 3         5 <tibble [70 × 2]> ## 4         7 <tibble [70 × 2]> ## 5         9 <tibble [70 × 2]> ## # A tibble: 70 × 2 ##    neighbors weight_func  ##        <int> <chr>        ##  1         1 biweight     ##  2         1 cos          ##  3         1 epanechnikov ##  4         1 inv          ##  5         1 rectangular  ##  6         1 triangular   ##  7         1 triweight    ##  8         2 biweight     ##  9         2 cos          ## 10         2 epanechnikov ## # ℹ 60 more rows"},{"path":"https://tune.tidymodels.org/dev/articles/extras/optimizations.html","id":"parallel-processing","dir":"Articles > Extras","previous_headings":"","what":"Parallel processing","title":"Optimizations and parallel processing","text":"tune package allows users, possible, use multiple cores separate machines fit models. package currently able parallelize either resampling loop grid search (via parallel_over = \"resamples\" control_grid(), default) resampling preprocessing loops (via parallel_over = \"everything\"). parallel_over = \"everything\", outer parallel loop iterate resamples inner parallel loop iterate unique combinations preprocessor model tuning parameters specific resample. result preprocessor re-processed multiple times, can faster preprocessing extremely fast. tune supports parallel processing using future framework. future supports variety technologies share computations choice technology determined chosen plan. run tuning code parallel, just provide plan() tune take care rest. example:","code":"library(future) plan(multisession)"},{"path":"https://tune.tidymodels.org/dev/articles/extras/optimizations.html","id":"foreach-legacy","dir":"Articles > Extras","previous_headings":"Parallel processing","what":"foreach (legacy)","title":"Optimizations and parallel processing","text":"1.2.0 release, tune supported parallelism using foreach framework. Support deprecated backends registered foreach fully removed upcoming release. foreach can use variety technologies share computations choice technology determined parallel backend package chosen (aka {technology} packages). example, doMC package uses forking mechanism Unix-like systems split computations across multiple cores. writing, backend packages doMC, doMPI, doParallel, doRedis, doSNOW, doAzureParallel (GitHub ). Registering parallel backend also somewhat dependent package. doParallel, one use: transition foreach future, remove lines loading foreach backend packages library(*) well lines registering technology registerDo*(), add following lines: Switch multisession another strategy desired.","code":"all_cores <- parallel::detectCores(logical = FALSE)  library(doParallel) cl <- makePSOCKcluster(all_cores) registerDoParallel(cl) library(future) plan(multisession)"},{"path":"https://tune.tidymodels.org/dev/articles/extras/optimizations.html","id":"in-practice","dir":"Articles > Extras","previous_headings":"Parallel processing","what":"In practice","title":"Optimizations and parallel processing","text":"One downside parallel processing different technologies handle inputs outputs differently. example, multicore forking tends carry loaded packages objects worker processes. Others . make sure correct packages loaded (attached) workers use pkg option control_grid(). helpful advice avoid errors parallel processing use variables global environment. may found code run inside worker process. example: issue likely occur dplyr::one_of() used sector. Also note almost logging provided tune_grid() seen running parallel. , dependent backend package technology used.","code":"num_pcs <- 3  recipe(mpg ~ ., data = mtcars) %>%    # Bad since num_pcs might not be found by a worker process   step_pca(all_predictors(), num_comp = num_pcs)  recipe(mpg ~ ., data = mtcars) %>%    # Good since the value is injected into the object   step_pca(all_predictors(), num_comp = !!num_pcs)"},{"path":"https://tune.tidymodels.org/dev/articles/tune.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Introduction to tune","text":"tune package helps optimize modeling process. Users can tag arguments recipes model objects optimization. search routines tune can discover arguments evaluate candidate values combination good performance found. example, let’s model Ames housing data: simplicity, sale price house modeled function geo-location. predictors appear nonlinear relationships outcome:  two predictors modeled using natural splines conjunction linear model. amount “wiggliness” splines determined degrees freedom. appropriate value parameter analytically determined data, tuning parameter (.k.. hyper-parameter). common approach use resampling estimate model performance different values parameters use results set reasonable values. can tag parameters optimization using tune() function: package can detect parameters optimize . However, based plot , potential amount non-linearity sale price predictors might different. example, longitude might require flexibility latitude. recipe constrain nonlinearity predictors . can probably better . accomplish , individual step_spline_natural() terms can added recipe predictor. However, want identifiable; using syntax , can’t tell difference two deg_free parameters. tune() option provide text annotation tuning parameter unique identifier: function extract_parameter_set_dials() can detect collect parameters flagged tuning. dials package default ranges many parameters. generic parameter function deg_free() fairly small range: However, function dials appropriate splines: parameter objects can easily changed using update() function:","code":"library(tidymodels)  data(ames)  set.seed(4595) data_split <- ames %>%   mutate(Sale_Price = log10(Sale_Price)) %>%   initial_split(strata = Sale_Price) ames_train <- training(data_split) ames_test  <- testing(data_split) ames_train %>%    dplyr::select(Sale_Price, Longitude, Latitude) %>%    tidyr::pivot_longer(cols = c(Longitude, Latitude),                        names_to = \"predictor\", values_to = \"value\") %>%    ggplot(aes(x = value, Sale_Price)) +    geom_point(alpha = .2) +    geom_smooth(se = FALSE) +    facet_wrap(~ predictor, scales = \"free_x\") #> `geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = #> \"cs\")' ames_rec <-    recipe(Sale_Price ~ Gr_Liv_Area + Longitude + Latitude, data = ames_train) %>%    step_log(Gr_Liv_Area, base = 10) %>%    step_spline_natural(Longitude, Latitude, deg_free = tune()) ames_rec <-    recipe(Sale_Price ~ Gr_Liv_Area + Longitude + Latitude, data = ames_train) %>%    step_log(Gr_Liv_Area, base = 10) %>%    step_spline_natural(Longitude, deg_free = tune(\"long df\")) %>%    step_spline_natural(Latitude,  deg_free = tune(\"lat df\")) extract_parameter_set_dials(ames_rec) #> Collection of 2 parameters for tuning #>  #>  identifier     type    object #>     long df deg_free nparam[+] #>      lat df deg_free nparam[+] #> deg_free() #> Degrees of Freedom (quantitative) #> Range: [1, 5] spline_degree() #> Spline Degrees of Freedom (quantitative) #> Range: [1, 10] ames_param <-    ames_rec %>%    extract_parameter_set_dials() %>%    update(     `long df` = spline_degree(),      `lat df` = spline_degree()   ) ames_param #> Collection of 2 parameters for tuning #>  #>  identifier     type    object #>     long df deg_free nparam[+] #>      lat df deg_free nparam[+] #>"},{"path":"https://tune.tidymodels.org/dev/articles/tune.html","id":"grid-search","dir":"Articles","previous_headings":"","what":"Grid Search","title":"Introduction to tune","text":"Grid search uses pre-defined set candidate parameters evaluates using resampling. basic ingredients : grid candidate values evaluate. One performance metrics quantifying well model works. resampling scheme can used appropriately measure performance (simple validation set). make grid, data frame needed column names matching “identifier” column . several functions dials created grids (named grid_*()). example, space-filling design can created : Alternately, expand.grid() also works create regular grid: Note 2-degree--freedom model simple quadratic fit. two ingredients required tuning. First model specification. Using functions parsnip, basic linear model can used: tuning parameters . mentioned , resampling specification also needed. Ames data set large enough use simple 10-fold cross-validation: root mean squared error used measure performance (default regression problems). Using objects, tune_grid() can used1: object similar rsample object one extra columns: .metrics column holdout performance estimates2 parameter combination: get average metric value parameter combination, collect_metrics() can put use: values mean column averages 10 resamples. best RMSE values corresponded : Smaller degrees freedom values correspond linear functions, grid search indicates nonlinearity better. relationship two parameters RMSE?  Interestingly, latitude well degrees freedom less 8. nonlinear optimal degrees freedom? Let’s plot spline functions data good bad values deg_free:  Looking plots, smaller degrees freedom (red) clearly -fitting. Visually, complex splines (blue) might indicate overfitting result poor RMSE values computed hold-data. Based results, new recipe created optimized values (using entire training set) combined linear model created form entire training set.","code":"spline_grid <- grid_max_entropy(ames_param, size = 10) #> Warning: `grid_max_entropy()` was deprecated in dials 1.3.0. #> ℹ Please use `grid_space_filling()` instead. #> This warning is displayed once every 8 hours. #> Call `lifecycle::last_lifecycle_warnings()` to see where this warning #> was generated. spline_grid #> # A tibble: 10 × 2 #>    `long df` `lat df` #>        <int>    <int> #>  1         6        5 #>  2         7        1 #>  3        10        2 #>  4         3        1 #>  5         1        9 #>  6         9        6 #>  7         1        4 #>  8         3        7 #>  9         9       10 #> 10         5        9 df_vals <- seq(2, 18, by = 2) # A regular grid: spline_grid <- expand.grid(`long df` = df_vals, `lat df` = df_vals) lm_mod <- linear_reg() %>% set_engine(\"lm\") set.seed(2453) cv_splits <- vfold_cv(ames_train, v = 10, strata = Sale_Price) ames_res <- tune_grid(lm_mod, ames_rec, resamples = cv_splits, grid = spline_grid) #> Warning: ! tune detected a parallel backend registered with foreach but no #>   backend registered with future. #> ℹ Support for parallel processing with foreach was soft-deprecated in #>   tune 1.2.1. #> ℹ See ?parallelism (`?tune::parallelism()`) to learn more. ames_res #> # Tuning results #> # 10-fold cross-validation using stratification  #> # A tibble: 10 × 4 #>    splits             id     .metrics           .notes           #>    <list>             <chr>  <list>             <list>           #>  1 <split [1976/221]> Fold01 <tibble [162 × 6]> <tibble [0 × 4]> #>  2 <split [1976/221]> Fold02 <tibble [162 × 6]> <tibble [0 × 4]> #>  3 <split [1976/221]> Fold03 <tibble [162 × 6]> <tibble [0 × 4]> #>  4 <split [1976/221]> Fold04 <tibble [162 × 6]> <tibble [0 × 4]> #>  5 <split [1977/220]> Fold05 <tibble [162 × 6]> <tibble [0 × 4]> #>  6 <split [1977/220]> Fold06 <tibble [162 × 6]> <tibble [0 × 4]> #>  7 <split [1978/219]> Fold07 <tibble [162 × 6]> <tibble [0 × 4]> #>  8 <split [1978/219]> Fold08 <tibble [162 × 6]> <tibble [0 × 4]> #>  9 <split [1979/218]> Fold09 <tibble [162 × 6]> <tibble [0 × 4]> #> 10 <split [1980/217]> Fold10 <tibble [162 × 6]> <tibble [0 × 4]> ames_res$.metrics[[1]] #> # A tibble: 162 × 6 #>    `long df` `lat df` .metric .estimator .estimate .config              #>        <dbl>    <dbl> <chr>   <chr>          <dbl> <chr>                #>  1         2        2 rmse    standard      0.0980 Preprocessor01_Mode… #>  2         2        2 rsq     standard      0.686  Preprocessor01_Mode… #>  3         2        4 rmse    standard      0.0973 Preprocessor02_Mode… #>  4         2        4 rsq     standard      0.691  Preprocessor02_Mode… #>  5         2        6 rmse    standard      0.0961 Preprocessor03_Mode… #>  6         2        6 rsq     standard      0.701  Preprocessor03_Mode… #>  7         2        8 rmse    standard      0.0929 Preprocessor04_Mode… #>  8         2        8 rsq     standard      0.722  Preprocessor04_Mode… #>  9         2       10 rmse    standard      0.0921 Preprocessor05_Mode… #> 10         2       10 rsq     standard      0.727  Preprocessor05_Mode… #> # ℹ 152 more rows estimates <- collect_metrics(ames_res) estimates #> # A tibble: 162 × 8 #>    `long df` `lat df` .metric .estimator   mean     n std_err .config   #>        <dbl>    <dbl> <chr>   <chr>       <dbl> <int>   <dbl> <chr>     #>  1         2        2 rmse    standard   0.0998    10 0.00140 Preproce… #>  2         2        2 rsq     standard   0.678     10 0.00737 Preproce… #>  3         2        4 rmse    standard   0.0998    10 0.00133 Preproce… #>  4         2        4 rsq     standard   0.678     10 0.00665 Preproce… #>  5         2        6 rmse    standard   0.0989    10 0.00124 Preproce… #>  6         2        6 rsq     standard   0.685     10 0.00733 Preproce… #>  7         2        8 rmse    standard   0.0968    10 0.00119 Preproce… #>  8         2        8 rsq     standard   0.698     10 0.00666 Preproce… #>  9         2       10 rmse    standard   0.0967    10 0.00114 Preproce… #> 10         2       10 rsq     standard   0.699     10 0.00659 Preproce… #> # ℹ 152 more rows rmse_vals <-    estimates %>%    dplyr::filter(.metric == \"rmse\") %>%    arrange(mean) rmse_vals #> # A tibble: 81 × 8 #>    `long df` `lat df` .metric .estimator   mean     n std_err .config   #>        <dbl>    <dbl> <chr>   <chr>       <dbl> <int>   <dbl> <chr>     #>  1        16       12 rmse    standard   0.0948    10 0.00120 Preproce… #>  2        18       12 rmse    standard   0.0948    10 0.00121 Preproce… #>  3        16        8 rmse    standard   0.0949    10 0.00118 Preproce… #>  4        16       10 rmse    standard   0.0949    10 0.00118 Preproce… #>  5        16       18 rmse    standard   0.0949    10 0.00119 Preproce… #>  6        16       16 rmse    standard   0.0949    10 0.00122 Preproce… #>  7        18       10 rmse    standard   0.0949    10 0.00119 Preproce… #>  8        18        8 rmse    standard   0.0949    10 0.00119 Preproce… #>  9        18       18 rmse    standard   0.0950    10 0.00120 Preproce… #> 10        16       14 rmse    standard   0.0950    10 0.00117 Preproce… #> # ℹ 71 more rows autoplot(ames_res, metric = \"rmse\") ames_train %>%    dplyr::select(Sale_Price, Longitude, Latitude) %>%    tidyr::pivot_longer(cols = c(Longitude, Latitude),                        names_to = \"predictor\", values_to = \"value\") %>%    ggplot(aes(x = value, Sale_Price)) +    geom_point(alpha = .2) +    geom_smooth(se = FALSE, method = lm, formula = y ~ splines::ns(x, df = 3),  col = \"red\")  +    geom_smooth(se = FALSE, method = lm, formula = y ~ splines::ns(x, df = 16)) +   scale_y_log10() +   facet_wrap(~ predictor, scales = \"free_x\")"},{"path":"https://tune.tidymodels.org/dev/articles/tune.html","id":"model-optimization","dir":"Articles","previous_headings":"","what":"Model Optimization","title":"Introduction to tune","text":"Instead linear regression, nonlinear model might provide good performance. K-nearest-neighbor fit also optimized. example, number neighbors distance weighting function optimized: easiest approach optimize pre-processing model parameters bundle objects workflow: , parameter set can used modify range values parameters optimized3: parameter collection can used grid functions tune_grid() via param_info argument. Instead using grid search, iterative method called Bayesian optimization can used. takes initial set results tries predict next tuning parameters evaluate. Although grid required, process requires additional pieces information: description search space. minimum, consist ranges numeric values list values categorical tuning parameters. acquisition function helps score potential tuning parameter values. model analyzing making predictions best tuning parameter values. Gaussian Process model typical used . code conduct search : Visually, performance gain :  best results : intrinsically nonlinear model less reliance nonlinear terms created recipe.","code":"# requires the kknn package knn_mod <-    nearest_neighbor(neighbors = tune(), weight_func = tune()) %>%    set_engine(\"kknn\") %>%    set_mode(\"regression\") library(workflows) knn_wflow <-    workflow() %>%    add_model(knn_mod) %>%    add_recipe(ames_rec) knn_param <-    knn_wflow %>%    extract_parameter_set_dials() %>%      update(     `long df` = spline_degree(c(2, 18)),      `lat df` = spline_degree(c(2, 18)),     neighbors = neighbors(c(3, 50)),     weight_func = weight_func(values = c(\"rectangular\", \"inv\", \"gaussian\", \"triangular\"))   ) ctrl <- control_bayes(verbose = TRUE) set.seed(8154) knn_search <- tune_bayes(knn_wflow, resamples = cv_splits, initial = 5, iter = 20,                          param_info = knn_param, control = ctrl) #>  #> ❯  Generating a set of 5 initial parameter results #> Warning: ! tune detected a parallel backend registered with foreach but no #>   backend registered with future. #> ℹ Support for parallel processing with foreach was soft-deprecated in #>   tune 1.2.1. #> ℹ See ?parallelism (`?tune::parallelism()`) to learn more. #> ✓ Initialization complete #>  #> i Gaussian process model #> ! The Gaussian process model is being fit using 7 features but only #>   has 5 data points to do so. This may cause errors or a poor model #>   fit. #> ✓ Gaussian process model #> i Generating 4774 candidates #> i Predicted candidates #> i Estimating performance #> Warning: ! tune detected a parallel backend registered with foreach but no #>   backend registered with future. #> ℹ Support for parallel processing with foreach was soft-deprecated in #>   tune 1.2.1. #> ℹ See ?parallelism (`?tune::parallelism()`) to learn more. #> ✓ Estimating performance #> i Gaussian process model #> ! The Gaussian process model is being fit using 7 features but only #>   has 6 data points to do so. This may cause errors or a poor model #>   fit. #> ✓ Gaussian process model #> i Generating 4770 candidates #> i Predicted candidates #> i Estimating performance #> Warning: ! tune detected a parallel backend registered with foreach but no #>   backend registered with future. #> ℹ Support for parallel processing with foreach was soft-deprecated in #>   tune 1.2.1. #> ℹ See ?parallelism (`?tune::parallelism()`) to learn more. #> ✓ Estimating performance #> i Gaussian process model #> ! The Gaussian process model is being fit using 7 features but only #>   has 7 data points to do so. This may cause errors or a poor model #>   fit. #> ✓ Gaussian process model #> i Generating 4777 candidates #> i Predicted candidates #> i Estimating performance #> Warning: ! tune detected a parallel backend registered with foreach but no #>   backend registered with future. #> ℹ Support for parallel processing with foreach was soft-deprecated in #>   tune 1.2.1. #> ℹ See ?parallelism (`?tune::parallelism()`) to learn more. #> ✓ Estimating performance #> i Gaussian process model #> ! The Gaussian process model is being fit using 7 features but only #>   has 8 data points to do so. This may cause errors or a poor model #>   fit. #> ✓ Gaussian process model #> i Generating 4766 candidates #> i Predicted candidates #> i Estimating performance #> Warning: ! tune detected a parallel backend registered with foreach but no #>   backend registered with future. #> ℹ Support for parallel processing with foreach was soft-deprecated in #>   tune 1.2.1. #> ℹ See ?parallelism (`?tune::parallelism()`) to learn more. #> ✓ Estimating performance #> i Gaussian process model #> ✓ Gaussian process model #> i Generating 4786 candidates #> i Predicted candidates #> i Estimating performance #> Warning: ! tune detected a parallel backend registered with foreach but no #>   backend registered with future. #> ℹ Support for parallel processing with foreach was soft-deprecated in #>   tune 1.2.1. #> ℹ See ?parallelism (`?tune::parallelism()`) to learn more. #> ✓ Estimating performance #> i Gaussian process model #> ✓ Gaussian process model #> i Generating 4792 candidates #> i Predicted candidates #> i Estimating performance #> Warning: ! tune detected a parallel backend registered with foreach but no #>   backend registered with future. #> ℹ Support for parallel processing with foreach was soft-deprecated in #>   tune 1.2.1. #> ℹ See ?parallelism (`?tune::parallelism()`) to learn more. #> ✓ Estimating performance #> i Gaussian process model #> ✓ Gaussian process model #> i Generating 4776 candidates #> i Predicted candidates #> i Estimating performance #> Warning: ! tune detected a parallel backend registered with foreach but no #>   backend registered with future. #> ℹ Support for parallel processing with foreach was soft-deprecated in #>   tune 1.2.1. #> ℹ See ?parallelism (`?tune::parallelism()`) to learn more. #> ✓ Estimating performance #> i Gaussian process model #> ✓ Gaussian process model #> i Generating 4771 candidates #> i Predicted candidates #> i Estimating performance #> Warning: ! tune detected a parallel backend registered with foreach but no #>   backend registered with future. #> ℹ Support for parallel processing with foreach was soft-deprecated in #>   tune 1.2.1. #> ℹ See ?parallelism (`?tune::parallelism()`) to learn more. #> ✓ Estimating performance #> i Gaussian process model #> ✓ Gaussian process model #> i Generating 4773 candidates #> i Predicted candidates #> i Estimating performance #> Warning: ! tune detected a parallel backend registered with foreach but no #>   backend registered with future. #> ℹ Support for parallel processing with foreach was soft-deprecated in #>   tune 1.2.1. #> ℹ See ?parallelism (`?tune::parallelism()`) to learn more. #> ✓ Estimating performance #> i Gaussian process model #> ✓ Gaussian process model #> i Generating 4744 candidates #> i Predicted candidates #> i Estimating performance #> Warning: ! tune detected a parallel backend registered with foreach but no #>   backend registered with future. #> ℹ Support for parallel processing with foreach was soft-deprecated in #>   tune 1.2.1. #> ℹ See ?parallelism (`?tune::parallelism()`) to learn more. #> ✓ Estimating performance #> i Gaussian process model #> ✓ Gaussian process model #> i Generating 4753 candidates #> i Predicted candidates #> i Estimating performance #> Warning: ! tune detected a parallel backend registered with foreach but no #>   backend registered with future. #> ℹ Support for parallel processing with foreach was soft-deprecated in #>   tune 1.2.1. #> ℹ See ?parallelism (`?tune::parallelism()`) to learn more. #> ✓ Estimating performance #> i Gaussian process model #> ✓ Gaussian process model #> i Generating 4782 candidates #> i Predicted candidates #> i Estimating performance #> Warning: ! tune detected a parallel backend registered with foreach but no #>   backend registered with future. #> ℹ Support for parallel processing with foreach was soft-deprecated in #>   tune 1.2.1. #> ℹ See ?parallelism (`?tune::parallelism()`) to learn more. #> ✓ Estimating performance #> i Gaussian process model #> ✓ Gaussian process model #> i Generating 4789 candidates #> i Predicted candidates #> i Estimating performance #> Warning: ! tune detected a parallel backend registered with foreach but no #>   backend registered with future. #> ℹ Support for parallel processing with foreach was soft-deprecated in #>   tune 1.2.1. #> ℹ See ?parallelism (`?tune::parallelism()`) to learn more. #> ✓ Estimating performance #> i Gaussian process model #> ✓ Gaussian process model #> i Generating 4789 candidates #> i Predicted candidates #> i Estimating performance #> Warning: ! tune detected a parallel backend registered with foreach but no #>   backend registered with future. #> ℹ Support for parallel processing with foreach was soft-deprecated in #>   tune 1.2.1. #> ℹ See ?parallelism (`?tune::parallelism()`) to learn more. #> ✓ Estimating performance #> i Gaussian process model #> ✓ Gaussian process model #> i Generating 4775 candidates #> i Predicted candidates #> i Estimating performance #> Warning: ! tune detected a parallel backend registered with foreach but no #>   backend registered with future. #> ℹ Support for parallel processing with foreach was soft-deprecated in #>   tune 1.2.1. #> ℹ See ?parallelism (`?tune::parallelism()`) to learn more. #> ✓ Estimating performance #> i Gaussian process model #> ✓ Gaussian process model #> i Generating 4763 candidates #> i Predicted candidates #> i Estimating performance #> Warning: ! tune detected a parallel backend registered with foreach but no #>   backend registered with future. #> ℹ Support for parallel processing with foreach was soft-deprecated in #>   tune 1.2.1. #> ℹ See ?parallelism (`?tune::parallelism()`) to learn more. #> ✓ Estimating performance #> i Gaussian process model #> ✓ Gaussian process model #> i Generating 4784 candidates #> i Predicted candidates #> i Estimating performance #> Warning: ! tune detected a parallel backend registered with foreach but no #>   backend registered with future. #> ℹ Support for parallel processing with foreach was soft-deprecated in #>   tune 1.2.1. #> ℹ See ?parallelism (`?tune::parallelism()`) to learn more. #> ✓ Estimating performance #> i Gaussian process model #> ✓ Gaussian process model #> i Generating 4758 candidates #> i Predicted candidates #> i Estimating performance #> Warning: ! tune detected a parallel backend registered with foreach but no #>   backend registered with future. #> ℹ Support for parallel processing with foreach was soft-deprecated in #>   tune 1.2.1. #> ℹ See ?parallelism (`?tune::parallelism()`) to learn more. #> ✓ Estimating performance #> ! No improvement for 10 iterations; returning current results. autoplot(knn_search, type = \"performance\", metric = \"rmse\") collect_metrics(knn_search) %>%    dplyr::filter(.metric == \"rmse\") %>%    arrange(mean) #> # A tibble: 23 × 11 #>    neighbors weight_func `long df` `lat df` .metric .estimator   mean #>        <int> <chr>           <int>    <int> <chr>   <chr>       <dbl> #>  1        11 triangular          9        8 rmse    standard   0.0821 #>  2         7 triangular          8        7 rmse    standard   0.0823 #>  3        15 triangular          7        7 rmse    standard   0.0824 #>  4        10 inv                 8        8 rmse    standard   0.0826 #>  5        11 inv                 6        8 rmse    standard   0.0827 #>  6        11 triangular          2        6 rmse    standard   0.0830 #>  7        14 triangular          8        9 rmse    standard   0.0832 #>  8         5 inv                 2        6 rmse    standard   0.0836 #>  9         3 rectangular         6        6 rmse    standard   0.0843 #> 10        29 inv                 7        6 rmse    standard   0.0858 #> # ℹ 13 more rows #> # ℹ 4 more variables: n <int>, std_err <dbl>, .config <chr>, #> #   .iter <int>"},{"path":"https://tune.tidymodels.org/dev/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Max Kuhn. Author, maintainer. . Copyright holder, funder.","code":""},{"path":"https://tune.tidymodels.org/dev/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Kuhn M (2025). tune: Tidy Tuning Tools. R package version 1.2.1.9000, https://github.com/tidymodels/tune, https://tune.tidymodels.org/.","code":"@Manual{,   title = {tune: Tidy Tuning Tools},   author = {Max Kuhn},   year = {2025},   note = {R package version 1.2.1.9000, https://github.com/tidymodels/tune},   url = {https://tune.tidymodels.org/}, }"},{"path":[]},{"path":"https://tune.tidymodels.org/dev/index.html","id":"overview","dir":"","previous_headings":"","what":"Overview","title":"Tidy Tuning Tools","text":"goal tune facilitate hyperparameter tuning tidymodels packages. relies heavily recipes, parsnip, dials.","code":""},{"path":"https://tune.tidymodels.org/dev/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Tidy Tuning Tools","text":"Install CRAN: can install current development version using:","code":"install.packages(\"tune\", repos = \"http://cran.r-project.org\") #or your local mirror # install.packages(\"pak\") pak::pak(\"tidymodels/tune\")"},{"path":"https://tune.tidymodels.org/dev/index.html","id":"examples","dir":"","previous_headings":"","what":"Examples","title":"Tidy Tuning Tools","text":"several package vignettes, well articles available tidymodels.org, demonstrating use tune. Good places begin include: Getting started cell segmentation data Getting started Ames housing data advanced resources available : Basic grid search SVM model Iterative Bayesian optimization classification model Advanced text mining example Notes optimizations parallel processing Details acquisition function scoring parameter combinations","code":""},{"path":"https://tune.tidymodels.org/dev/index.html","id":"contributing","dir":"","previous_headings":"","what":"Contributing","title":"Tidy Tuning Tools","text":"project released Contributor Code Conduct. contributing project, agree abide terms. questions discussions tidymodels packages, modeling, machine learning, please post Posit Community. think encountered bug, please submit issue. Either way, learn create share reprex (minimal, reproducible example), clearly communicate code. Check details contributing guidelines tidymodels packages get help.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/augment.tune_results.html","id":null,"dir":"Reference","previous_headings":"","what":"Augment data with holdout predictions — augment.tune_results","title":"Augment data with holdout predictions — augment.tune_results","text":"tune objects use resampling, augment() methods add one columns hold-predictions (.e. assessment set(s)).","code":""},{"path":"https://tune.tidymodels.org/dev/reference/augment.tune_results.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Augment data with holdout predictions — augment.tune_results","text":"","code":"# S3 method for class 'tune_results' augment(x, ..., parameters = NULL)  # S3 method for class 'resample_results' augment(x, ...)  # S3 method for class 'last_fit' augment(x, ...)"},{"path":"https://tune.tidymodels.org/dev/reference/augment.tune_results.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Augment data with holdout predictions — augment.tune_results","text":"x object resulting one tune_*() functions, fit_resamples(), last_fit(). control specifications objects used option save_pred = TRUE. ... currently used. parameters data frame single row indicates tuning parameters used generate predictions (tune_*() objects ). NULL, select_best(x) used first metric , applicable, first evaluation time point, used create x.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/augment.tune_results.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Augment data with holdout predictions — augment.tune_results","text":"data frame one additional columns model predictions.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/augment.tune_results.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Augment data with holdout predictions — augment.tune_results","text":"resampling methods rows may replicated multiple assessment sets, prediction columns averages holdout results. Also, methods, possible rows original data holdout predictions (like single bootstrap resample). case, rows return warning issued. objects created last_fit(), test set data predictions returned. Unlike augment() methods, predicted values regression models column called .pred instead .fitted (consistent tidymodels conventions). regression problems, additional .resid column added results.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/autoplot.tune_results.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot tuning search results — autoplot.tune_results","title":"Plot tuning search results — autoplot.tune_results","text":"Plot tuning search results","code":""},{"path":"https://tune.tidymodels.org/dev/reference/autoplot.tune_results.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot tuning search results — autoplot.tune_results","text":"","code":"# S3 method for class 'tune_results' autoplot(   object,   type = c(\"marginals\", \"parameters\", \"performance\"),   metric = NULL,   eval_time = NULL,   width = NULL,   call = rlang::current_env(),   ... )"},{"path":"https://tune.tidymodels.org/dev/reference/autoplot.tune_results.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot tuning search results — autoplot.tune_results","text":"object tibble results tune_grid() tune_bayes(). type single character value. Choices \"marginals\" (plot predictor versus performance; see Details ), \"parameters\" (parameter versus search iteration), \"performance\" (performance versus iteration). latter two choices used tune_bayes(). metric character vector NULL metric plot. default, metrics shown via facets. Possible options entries .metric column collect_metrics(object). eval_time numeric vector time points dynamic event time metrics chosen (e.g. time-dependent ROC curve, etc). values consistent values used create object. width number width confidence interval bars type = \"performance\". value zero prevents shown. call call displayed warnings errors. ... plots regular grid, passed format() applied parameter used color points. Otherwise, used.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/autoplot.tune_results.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot tuning search results — autoplot.tune_results","text":"ggplot2 object.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/autoplot.tune_results.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot tuning search results — autoplot.tune_results","text":"results tune_grid() used autoplot(), tries determine whether regular grid used.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/autoplot.tune_results.html","id":"regular-grids","dir":"Reference","previous_headings":"","what":"Regular grids","title":"Plot tuning search results — autoplot.tune_results","text":"regular grids one numeric tuning parameters, parameter unique values used x-axis. categorical parameters, first used color geometries. parameters used column faceting. plot performance metric(s) y-axis. multiple metrics, row-faceted. five tuning parameters, \"marginal effects\" plots used instead.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/autoplot.tune_results.html","id":"irregular-grids","dir":"Reference","previous_headings":"","what":"Irregular grids","title":"Plot tuning search results — autoplot.tune_results","text":"space-filling random grids, marginal effect plot created. panel made numeric parameter parameter x-axis performance y-xis. multiple metrics, row-faceted. single categorical parameter shown colors. two non-numeric parameters, error given. similar result occurs non-numeric parameters grid. cases, suggest using collect_metrics() ggplot() create plot appropriate data. parameter associated transformation associated (determined parameter object used create ), plot shows values transformed units (labeled transformation type). Parameters labeled using labels found parameter object except identifier used (e.g. neighbors = tune(\"K\")).","code":""},{"path":[]},{"path":"https://tune.tidymodels.org/dev/reference/autoplot.tune_results.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot tuning search results — autoplot.tune_results","text":"","code":"# For grid search: data(\"example_ames_knn\")  # Plot the tuning parameter values versus performance autoplot(ames_grid_search, metric = \"rmse\")    # For iterative search: # Plot the tuning parameter values versus performance autoplot(ames_iter_search, metric = \"rmse\", type = \"marginals\")   # Plot tuning parameters versus iterations autoplot(ames_iter_search, metric = \"rmse\", type = \"parameters\")   # Plot performance over iterations autoplot(ames_iter_search, metric = \"rmse\", type = \"performance\")"},{"path":"https://tune.tidymodels.org/dev/reference/choose_metric.html","id":null,"dir":"Reference","previous_headings":"","what":"Tools for selecting metrics and evaluation times — choose_metric","title":"Tools for selecting metrics and evaluation times — choose_metric","text":"Tools selecting metrics evaluation times","code":""},{"path":"https://tune.tidymodels.org/dev/reference/choose_metric.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tools for selecting metrics and evaluation times — choose_metric","text":"","code":"choose_metric(x, metric, ..., call = rlang::caller_env())  check_metric_in_tune_results(mtr_info, metric, ..., call = rlang::caller_env())  choose_eval_time(   x,   metric,   ...,   eval_time = NULL,   quietly = FALSE,   call = rlang::caller_env() )  maybe_choose_eval_time(x, mtr_set, eval_time)  first_metric(mtr_set)  first_eval_time(   mtr_set,   ...,   metric = NULL,   eval_time = NULL,   quietly = FALSE,   call = rlang::caller_env() )  .filter_perf_metrics(x, metric, eval_time)  check_metrics_arg(mtr_set, wflow, ..., call = rlang::caller_env())  check_eval_time_arg(eval_time, mtr_set, ..., call = rlang::caller_env())"},{"path":"https://tune.tidymodels.org/dev/reference/choose_metric.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tools for selecting metrics and evaluation times — choose_metric","text":"x object class tune_results. metric character value metric used. ... dots future extensions must empty. call call displayed warnings errors. eval_time optional vector times compute dynamic /integrated metrics. quietly Logical. warnings muffled? mtr_set yardstick::metric_set(). wflow workflows::workflow().","code":""},{"path":"https://tune.tidymodels.org/dev/reference/choose_metric.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Tools for selecting metrics and evaluation times — choose_metric","text":"developer-facing functions used compute validate choices performance metrics. survival analysis models, similar functions evaluation time(s) required dynamic /integrated metrics. choose_metric() used functions show_best() select_best() single valid metric required rank models. value given user, first metric value used (warning). evaluation times, one required metric type dynamic (e.g. yardstick::brier_survival() yardstick::roc_auc_survival()). metrics, require single numeric value originally given function used produce x (tune_grid()). time required none given, first value vector originally given eval_time argument used (warning). maybe_choose_eval_time() cases multiple evaluation times acceptable need choose good default. \"maybe\" function use maybe_choose_eval_time() can accept multiple metrics (like autoplot()).","code":""},{"path":"https://tune.tidymodels.org/dev/reference/collect_predictions.html","id":null,"dir":"Reference","previous_headings":"","what":"Obtain and format results produced by tuning functions — collect_predictions","title":"Obtain and format results produced by tuning functions — collect_predictions","text":"Obtain format results produced tuning functions","code":""},{"path":"https://tune.tidymodels.org/dev/reference/collect_predictions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Obtain and format results produced by tuning functions — collect_predictions","text":"","code":"collect_predictions(x, ...)  # Default S3 method collect_predictions(x, ...)  # S3 method for class 'tune_results' collect_predictions(x, ..., summarize = FALSE, parameters = NULL)  collect_metrics(x, ...)  # S3 method for class 'tune_results' collect_metrics(x, ..., summarize = TRUE, type = c(\"long\", \"wide\"))  collect_notes(x, ...)  # S3 method for class 'tune_results' collect_notes(x, ...)  collect_extracts(x, ...)  # S3 method for class 'tune_results' collect_extracts(x, ...)"},{"path":"https://tune.tidymodels.org/dev/reference/collect_predictions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Obtain and format results produced by tuning functions — collect_predictions","text":"x results tune_grid(), tune_bayes(), fit_resamples(), last_fit(). collect_predictions(), control option save_pred = TRUE used. ... currently used. summarize logical; metrics summarized resamples (TRUE) return values individual resample. Note , x created last_fit(), summarize effect. object types, method summarizing predictions detailed . parameters optional tibble tuning parameter values can used filter predicted values processing. tibble columns tuning parameter identifier (e.g. \"my_param\" tune(\"my_param\") used). type One \"long\" (default) \"wide\". type = \"long\", output columns .metric one .estimate mean. .estimate/mean gives values .metric. type = \"wide\", metric column n std_err columns removed, exist.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/collect_predictions.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Obtain and format results produced by tuning functions — collect_predictions","text":"tibble. column names depend results mode model. collect_metrics() collect_predictions(), unsummarized, columns tuning parameter (using id tune(), ). collect_metrics() also columns .metric, .estimator default. collect_metrics() methods type argument, supplying type = \"wide\" pivot output metric column. results summarized, columns mean, n, std_err. summarized, additional columns resampling identifier(s) .estimate. collect_predictions(), additional columns resampling identifier(s), columns predicted values (e.g., .pred, .pred_class, etc.), column outcome(s) using original column name(s) data. collect_predictions() can summarize various results replicate --sample predictions. example, using bootstrap, row original training set multiple holdout predictions (across assessment sets). convert results format every training set single predicted value, results averaged replicate predictions. regression cases, numeric predictions simply averaged. classification models, problem complex. class probabilities used, averaged re-normalized make sure add one. hard class predictions also exist data, determined summarized probability estimates (match). hard class predictions results, mode used summarize. censored outcome models, predicted survival probabilities () averaged static predicted event times summarized using median. collect_notes() returns tibble columns resampling indicators, location (preprocessor, model, etc.), type (error warning), notes. collect_extracts() collects objects extracted fitted workflows via extract argument control functions. function returns tibble columns resampling indicators, location (preprocessor, model, etc.), extracted objects.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/collect_predictions.html","id":"hyperparameters-and-extracted-objects","dir":"Reference","previous_headings":"","what":"Hyperparameters and extracted objects","title":"Obtain and format results produced by tuning functions — collect_predictions","text":"making use submodels, tune can generate predictions calculate metrics multiple model .configurations using one model fit. However, means function supplied control function's extract argument, tune can execute extraction one model fitted. result, collect_extracts() output, tune opts associate extracted objects hyperparameter combination used fit one model workflow, rather hyperparameter combination submodel. output, appears like hyperparameter entry recycled across many .config entries—intentional. See https://parsnip.tidymodels.org/articles/Submodels.html learn submodels.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/collect_predictions.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Obtain and format results produced by tuning functions — collect_predictions","text":"","code":"data(\"example_ames_knn\") # The parameters for the model: extract_parameter_set_dials(ames_wflow) #> Collection of 5 parameters for tuning #>  #>   identifier        type    object #>            K   neighbors nparam[+] #>  weight_func weight_func dparam[+] #>   dist_power  dist_power nparam[+] #>          lon    deg_free nparam[+] #>          lat    deg_free nparam[+] #>   # Summarized over resamples collect_metrics(ames_grid_search) #> # A tibble: 20 × 11 #>        K weight_func  dist_power   lon   lat .metric .estimator   mean #>    <int> <chr>             <dbl> <int> <int> <chr>   <chr>       <dbl> #>  1    35 optimal           1.32      8     1 rmse    standard   0.0785 #>  2    35 optimal           1.32      8     1 rsq     standard   0.823  #>  3    35 rank              1.29      3    13 rmse    standard   0.0809 #>  4    35 rank              1.29      3    13 rsq     standard   0.814  #>  5    21 cos               0.626     1     4 rmse    standard   0.0746 #>  6    21 cos               0.626     1     4 rsq     standard   0.836  #>  7     4 biweight          0.311     8     4 rmse    standard   0.0777 #>  8     4 biweight          0.311     8     4 rsq     standard   0.814  #>  9    32 triangular        0.165     9    15 rmse    standard   0.0770 #> 10    32 triangular        0.165     9    15 rsq     standard   0.826  #> 11     3 rank              1.86     10    15 rmse    standard   0.0875 #> 12     3 rank              1.86     10    15 rsq     standard   0.762  #> 13    40 triangular        0.167    11     7 rmse    standard   0.0778 #> 14    40 triangular        0.167    11     7 rsq     standard   0.822  #> 15    12 epanechnikov      1.53      4     7 rmse    standard   0.0774 #> 16    12 epanechnikov      1.53      4     7 rsq     standard   0.820  #> 17     5 rank              0.411     2     7 rmse    standard   0.0740 #> 18     5 rank              0.411     2     7 rsq     standard   0.833  #> 19    33 triweight         0.511    10     3 rmse    standard   0.0728 #> 20    33 triweight         0.511    10     3 rsq     standard   0.842  #> # ℹ 3 more variables: n <int>, std_err <dbl>, .config <chr>  # Per-resample values collect_metrics(ames_grid_search, summarize = FALSE) #> # A tibble: 200 × 10 #>    id         K weight_func dist_power   lon   lat .metric .estimator #>    <chr>  <int> <chr>            <dbl> <int> <int> <chr>   <chr>      #>  1 Fold01    35 optimal           1.32     8     1 rmse    standard   #>  2 Fold01    35 optimal           1.32     8     1 rsq     standard   #>  3 Fold02    35 optimal           1.32     8     1 rmse    standard   #>  4 Fold02    35 optimal           1.32     8     1 rsq     standard   #>  5 Fold03    35 optimal           1.32     8     1 rmse    standard   #>  6 Fold03    35 optimal           1.32     8     1 rsq     standard   #>  7 Fold04    35 optimal           1.32     8     1 rmse    standard   #>  8 Fold04    35 optimal           1.32     8     1 rsq     standard   #>  9 Fold05    35 optimal           1.32     8     1 rmse    standard   #> 10 Fold05    35 optimal           1.32     8     1 rsq     standard   #> # ℹ 190 more rows #> # ℹ 2 more variables: .estimate <dbl>, .config <chr>   # ---------------------------------------------------------------------------  library(parsnip) library(rsample) library(dplyr) #>  #> Attaching package: ‘dplyr’ #> The following objects are masked from ‘package:stats’: #>  #>     filter, lag #> The following objects are masked from ‘package:base’: #>  #>     intersect, setdiff, setequal, union library(recipes) #>  #> Attaching package: ‘recipes’ #> The following object is masked from ‘package:stats’: #>  #>     step library(tibble)  lm_mod <- linear_reg() %>% set_engine(\"lm\") set.seed(93599150) car_folds <- vfold_cv(mtcars, v = 2, repeats = 3) ctrl <- control_resamples(save_pred = TRUE, extract = extract_fit_engine)  spline_rec <-   recipe(mpg ~ ., data = mtcars) %>%   step_spline_natural(disp, deg_free = tune(\"df\"))  grid <- tibble(df = 3:6)  resampled <-   lm_mod %>%   tune_grid(spline_rec, resamples = car_folds, control = ctrl, grid = grid)  collect_predictions(resampled) %>% arrange(.row) #> # A tibble: 384 × 7 #>    .pred id      id2    .row    df   mpg .config              #>    <dbl> <chr>   <chr> <int> <int> <dbl> <chr>                #>  1  16.5 Repeat1 Fold2     1     3    21 Preprocessor1_Model1 #>  2  19.0 Repeat2 Fold1     1     3    21 Preprocessor1_Model1 #>  3  20.0 Repeat3 Fold1     1     3    21 Preprocessor1_Model1 #>  4  15.1 Repeat1 Fold2     1     4    21 Preprocessor2_Model1 #>  5  17.7 Repeat2 Fold1     1     4    21 Preprocessor2_Model1 #>  6  20.1 Repeat3 Fold1     1     4    21 Preprocessor2_Model1 #>  7  17.9 Repeat1 Fold2     1     5    21 Preprocessor3_Model1 #>  8  18.3 Repeat2 Fold1     1     5    21 Preprocessor3_Model1 #>  9  20.4 Repeat3 Fold1     1     5    21 Preprocessor3_Model1 #> 10  15.1 Repeat1 Fold2     1     6    21 Preprocessor4_Model1 #> # ℹ 374 more rows collect_predictions(resampled, summarize = TRUE) %>% arrange(.row) #> # A tibble: 128 × 5 #>    .pred  .row    df   mpg .config              #>    <dbl> <int> <int> <dbl> <chr>                #>  1  18.5     1     3  21   Preprocessor1_Model1 #>  2  17.6     1     4  21   Preprocessor2_Model1 #>  3  18.9     1     5  21   Preprocessor3_Model1 #>  4  16.7     1     6  21   Preprocessor4_Model1 #>  5  19.4     2     3  21   Preprocessor1_Model1 #>  6  19.0     2     4  21   Preprocessor2_Model1 #>  7  18.7     2     5  21   Preprocessor3_Model1 #>  8  16.4     2     6  21   Preprocessor4_Model1 #>  9  31.8     3     3  22.8 Preprocessor1_Model1 #> 10  23.8     3     4  22.8 Preprocessor2_Model1 #> # ℹ 118 more rows collect_predictions(   resampled,   summarize = TRUE,   parameters = grid[1, ] ) %>% arrange(.row) #> # A tibble: 32 × 5 #>    .pred  .row    df   mpg .config              #>    <dbl> <int> <int> <dbl> <chr>                #>  1  18.5     1     3  21   Preprocessor1_Model1 #>  2  19.4     2     3  21   Preprocessor1_Model1 #>  3  31.8     3     3  22.8 Preprocessor1_Model1 #>  4  20.2     4     3  21.4 Preprocessor1_Model1 #>  5  18.4     5     3  18.7 Preprocessor1_Model1 #>  6  20.6     6     3  18.1 Preprocessor1_Model1 #>  7  13.5     7     3  14.3 Preprocessor1_Model1 #>  8  19.2     8     3  24.4 Preprocessor1_Model1 #>  9  34.8     9     3  22.8 Preprocessor1_Model1 #> 10  16.6    10     3  19.2 Preprocessor1_Model1 #> # ℹ 22 more rows  collect_extracts(resampled) #> # A tibble: 24 × 5 #>    id      id2      df .extracts .config              #>    <chr>   <chr> <int> <list>    <chr>                #>  1 Repeat1 Fold1     3 <lm>      Preprocessor1_Model1 #>  2 Repeat1 Fold1     4 <lm>      Preprocessor2_Model1 #>  3 Repeat1 Fold1     5 <lm>      Preprocessor3_Model1 #>  4 Repeat1 Fold1     6 <lm>      Preprocessor4_Model1 #>  5 Repeat1 Fold2     3 <lm>      Preprocessor1_Model1 #>  6 Repeat1 Fold2     4 <lm>      Preprocessor2_Model1 #>  7 Repeat1 Fold2     5 <lm>      Preprocessor3_Model1 #>  8 Repeat1 Fold2     6 <lm>      Preprocessor4_Model1 #>  9 Repeat2 Fold1     3 <lm>      Preprocessor1_Model1 #> 10 Repeat2 Fold1     4 <lm>      Preprocessor2_Model1 #> # ℹ 14 more rows"},{"path":"https://tune.tidymodels.org/dev/reference/compute_metrics.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate and format metrics from tuning functions — compute_metrics","title":"Calculate and format metrics from tuning functions — compute_metrics","text":"function computes metrics tuning results. arguments output formats closely related collect_metrics(), function additionally takes metrics argument metric set new metrics compute. allows computing new performance metrics without requiring users re-evaluate models resamples. Note control option save_pred = TRUE must supplied generating x.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/compute_metrics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate and format metrics from tuning functions — compute_metrics","text":"","code":"compute_metrics(x, metrics, summarize, event_level, ...)  # Default S3 method compute_metrics(x, metrics, summarize = TRUE, event_level = \"first\", ...)  # S3 method for class 'tune_results' compute_metrics(x, metrics, ..., summarize = TRUE, event_level = \"first\")"},{"path":"https://tune.tidymodels.org/dev/reference/compute_metrics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate and format metrics from tuning functions — compute_metrics","text":"x results tuning function like tune_grid() fit_resamples(), generated control option save_pred = TRUE. metrics metric set new metrics compute. See \"Details\" section information. summarize single logical value indicating whether metrics summarized resamples (TRUE) return values individual resample. See collect_metrics() details metrics summarized. event_level single string containing either \"first\" \"second\". argument passed yardstick metric functions type class prediction made, specifies level outcome considered \"event\". ... currently used.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/compute_metrics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate and format metrics from tuning functions — compute_metrics","text":"tibble. See collect_metrics() details return value.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/compute_metrics.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate and format metrics from tuning functions — compute_metrics","text":"metric set supplied metrics argument must metric type (usually \"numeric\", \"class\", \"prob\") matches metric evaluated generating x. e.g. example, x generated hard \"class\" metrics, function compute metrics take class probabilities (\"prob\".) default, tuning functions used generate x compute metrics needed types.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/compute_metrics.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate and format metrics from tuning functions — compute_metrics","text":"","code":"# load needed packages: library(parsnip) library(rsample) library(yardstick)  # evaluate a linear regression against resamples. # note that we pass `save_pred = TRUE`: res <-   fit_resamples(     linear_reg(),     mpg ~ cyl + hp,     bootstraps(mtcars, 5),     control = control_grid(save_pred = TRUE)   )  # to return the metrics supplied to `fit_resamples()`: collect_metrics(res) #> # A tibble: 2 × 6 #>   .metric .estimator  mean     n std_err .config              #>   <chr>   <chr>      <dbl> <int>   <dbl> <chr>                #> 1 rmse    standard   3.21      5  0.363  Preprocessor1_Model1 #> 2 rsq     standard   0.732     5  0.0495 Preprocessor1_Model1  # to compute new metrics: compute_metrics(res, metric_set(mae)) #> # A tibble: 1 × 6 #>   .metric .estimator  mean     n std_err .config              #>   <chr>   <chr>      <dbl> <int>   <dbl> <chr>                #> 1 mae     standard    2.57     5   0.347 Preprocessor1_Model1  # if `metrics` is the same as that passed to `fit_resamples()`, # then `collect_metrics()` and `compute_metrics()` give the same # output, though `compute_metrics()` is quite a bit slower: all.equal(   collect_metrics(res),   compute_metrics(res, metric_set(rmse, rsq)) ) #> [1] TRUE"},{"path":"https://tune.tidymodels.org/dev/reference/conf_mat_resampled.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute average confusion matrix across resamples — conf_mat_resampled","title":"Compute average confusion matrix across resamples — conf_mat_resampled","text":"classification problems, conf_mat_resampled() computes separate confusion matrix resample averages cell counts.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/conf_mat_resampled.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute average confusion matrix across resamples — conf_mat_resampled","text":"","code":"conf_mat_resampled(x, ..., parameters = NULL, tidy = TRUE)"},{"path":"https://tune.tidymodels.org/dev/reference/conf_mat_resampled.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute average confusion matrix across resamples — conf_mat_resampled","text":"x object class tune_results used classification model run control_*(save_pred = TRUE). ... Currently unused, must empty. parameters tibble single tuning parameter combination. one tuning parameter combination (used) allowed . tidy results come back tibble (TRUE) conf_mat object like yardstick::conf_mat() (FALSE)?","code":""},{"path":"https://tune.tidymodels.org/dev/reference/conf_mat_resampled.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute average confusion matrix across resamples — conf_mat_resampled","text":"tibble conf_mat average cell count across resamples.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/conf_mat_resampled.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute average confusion matrix across resamples — conf_mat_resampled","text":"","code":"# example code  library(parsnip) library(rsample) library(dplyr)  data(two_class_dat, package = \"modeldata\")  set.seed(2393) res <-   logistic_reg() %>%   set_engine(\"glm\") %>%   fit_resamples(     Class ~ .,     resamples = vfold_cv(two_class_dat, v = 3),     control = control_resamples(save_pred = TRUE)   )  conf_mat_resampled(res) #> # A tibble: 4 × 3 #>   Prediction Truth   Freq #>   <fct>      <fct>  <dbl> #> 1 Class1     Class1 123   #> 2 Class1     Class2  25.7 #> 3 Class2     Class1  22.7 #> 4 Class2     Class2  92.3 conf_mat_resampled(res, tidy = FALSE) #>           Class1    Class2 #> Class1 123.00000  25.66667 #> Class2  22.66667  92.33333"},{"path":"https://tune.tidymodels.org/dev/reference/control_bayes.html","id":null,"dir":"Reference","previous_headings":"","what":"Control aspects of the Bayesian search process — control_bayes","title":"Control aspects of the Bayesian search process — control_bayes","text":"Control aspects Bayesian search process","code":""},{"path":"https://tune.tidymodels.org/dev/reference/control_bayes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Control aspects of the Bayesian search process — control_bayes","text":"","code":"control_bayes(   verbose = FALSE,   verbose_iter = FALSE,   no_improve = 10L,   uncertain = Inf,   seed = sample.int(10^5, 1),   extract = NULL,   save_pred = FALSE,   time_limit = NA,   pkgs = NULL,   save_workflow = FALSE,   save_gp_scoring = FALSE,   event_level = \"first\",   parallel_over = NULL,   backend_options = NULL,   allow_par = TRUE )"},{"path":"https://tune.tidymodels.org/dev/reference/control_bayes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Control aspects of the Bayesian search process — control_bayes","text":"verbose logical logging results (warnings errors, always shown) generated training single R process. using parallel backends, argument typically result logging. using dark IDE theme, logging messages might hard see; try setting tidymodels.dark option options(tidymodels.dark = TRUE) print lighter colors. verbose_iter logical logging results Bayesian search process. Defaults FALSE. using dark IDE theme, logging messages might hard see; try setting tidymodels.dark option options(tidymodels.dark = TRUE) print lighter colors. no_improve integer cutoff number iterations without better results. uncertain number iterations improvement uncertainty sample created sample high predicted variance chosen (.e., region yet explored). iteration counter reset uncertainty sample. example, uncertain = 10, condition triggered every 10 samples improvement. seed integer controlling random number stream. Tuning functions sensitive state RNG set outside tuning functions set.seed() well value set . value former determines RNG higher-level tuning process, like grid generation setting value argument left default. value argument determines RNG state workers iteration model fitting, determined value parallel_over. extract optional function least one argument (NULL) can used retain arbitrary objects model fit object, recipe, elements workflow. save_pred logical whether --sample predictions saved model evaluated. time_limit number minimum number minutes (elapsed) function execute. elapsed time evaluated internal checkpoints , time, results time returned (warning). means time_limit exact limit, minimum time limit. Note timing begins immediately execution. Thus, initial argument tune_bayes() supplied number, elapsed time include time needed generate initialization results. pkgs optional character string R package names loaded (namespace) parallel processing. save_workflow logical whether workflow appended output attribute. save_gp_scoring logical save intermediate Gaussian process models iteration search. saved tempdir() names gp_candidates_{}.RData iteration. results deleted R session ends. option useful teaching purposes. event_level single string containing either \"first\" \"second\". argument passed yardstick metric functions type class prediction made, specifies level outcome considered \"event\". parallel_over single string containing either \"resamples\" \"everything\" describing use parallel processing. Alternatively, NULL allowed, chooses \"resamples\" \"everything\" automatically. \"resamples\", tuning performed parallel resamples alone. Within resample, preprocessor (.e. recipe formula) processed , reused across models need fit. \"everything\", tuning performed parallel two levels. outer parallel loop iterate resamples. Additionally, inner parallel loop iterate unique combinations preprocessor model tuning parameters specific resample. result preprocessor re-processed multiple times, can faster processing extremely fast. NULL, chooses \"resamples\" one resample, otherwise chooses \"everything\" attempt maximize core utilization. Note switching parallel_over strategies guaranteed use random number generation schemes. However, re-tuning model using parallel_over strategy guaranteed reproducible runs. backend_options object class \"tune_backend_options\" created tune::new_backend_options(), used pass arguments specific tuning backend. Defaults NULL default backend options. allow_par logical allow parallel processing (parallel backend registered).","code":""},{"path":"https://tune.tidymodels.org/dev/reference/control_bayes.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Control aspects of the Bayesian search process — control_bayes","text":"extract, function can used output model object, recipe (used), components either . evaluated, function's sole argument fitted workflow formula method used, recipe element NULL. results extract function added list column output called .extracts. element list tibble tuning parameter column list column (also called .extracts) contains results function. extraction function used, .extracts column resulting object. See tune_bayes() specific details. Note collect_predictions(), possible row original data point might represented multiple times per tuning parameter. example, bootstrap repeated cross-validation used, multiple rows since sample data point evaluated multiple times. may cause issues merging predictions original data.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/control_bayes.html","id":"hyperparameters-and-extracted-objects","dir":"Reference","previous_headings":"","what":"Hyperparameters and extracted objects","title":"Control aspects of the Bayesian search process — control_bayes","text":"making use submodels, tune can generate predictions calculate metrics multiple model .configurations using one model fit. However, means function supplied control function's extract argument, tune can execute extraction one model fitted. result, collect_extracts() output, tune opts associate extracted objects hyperparameter combination used fit one model workflow, rather hyperparameter combination submodel. output, appears like hyperparameter entry recycled across many .config entries—intentional. See https://parsnip.tidymodels.org/articles/Submodels.html learn submodels.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/control_grid.html","id":null,"dir":"Reference","previous_headings":"","what":"Control aspects of the grid search process — control_grid","title":"Control aspects of the grid search process — control_grid","text":"Control aspects grid search process","code":""},{"path":"https://tune.tidymodels.org/dev/reference/control_grid.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Control aspects of the grid search process — control_grid","text":"","code":"control_grid(   verbose = FALSE,   allow_par = TRUE,   extract = NULL,   save_pred = FALSE,   pkgs = NULL,   save_workflow = FALSE,   event_level = \"first\",   parallel_over = NULL,   backend_options = NULL )  control_resamples(   verbose = FALSE,   allow_par = TRUE,   extract = NULL,   save_pred = FALSE,   pkgs = NULL,   save_workflow = FALSE,   event_level = \"first\",   parallel_over = NULL,   backend_options = NULL )  new_backend_options(..., class = character())"},{"path":"https://tune.tidymodels.org/dev/reference/control_grid.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Control aspects of the grid search process — control_grid","text":"verbose logical logging results (warnings errors, always shown) generated training single R process. using parallel backends, argument typically result logging. using dark IDE theme, logging messages might hard see; try setting tidymodels.dark option options(tidymodels.dark = TRUE) print lighter colors. allow_par logical allow parallel processing (parallel backend registered). extract optional function least one argument (NULL) can used retain arbitrary objects model fit object, recipe, elements workflow. save_pred logical whether --sample predictions saved model evaluated. pkgs optional character string R package names loaded (namespace) parallel processing. save_workflow logical whether workflow appended output attribute. event_level single string containing either \"first\" \"second\". argument passed yardstick metric functions type class prediction made, specifies level outcome considered \"event\". parallel_over single string containing either \"resamples\" \"everything\" describing use parallel processing. Alternatively, NULL allowed, chooses \"resamples\" \"everything\" automatically. \"resamples\", tuning performed parallel resamples alone. Within resample, preprocessor (.e. recipe formula) processed , reused across models need fit. \"everything\", tuning performed parallel two levels. outer parallel loop iterate resamples. Additionally, inner parallel loop iterate unique combinations preprocessor model tuning parameters specific resample. result preprocessor re-processed multiple times, can faster processing extremely fast. NULL, chooses \"resamples\" one resample, otherwise chooses \"everything\" attempt maximize core utilization. Note switching parallel_over strategies guaranteed use random number generation schemes. However, re-tuning model using parallel_over strategy guaranteed reproducible runs. backend_options object class \"tune_backend_options\" created tune::new_backend_options(), used pass arguments specific tuning backend. Defaults NULL default backend options.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/control_grid.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Control aspects of the grid search process — control_grid","text":"extract, function can used output model object, recipe (used), components either . evaluated, function's sole argument fitted workflow formula method used, recipe element NULL. results extract function added list column output called .extracts. element list tibble tuning parameter column list column (also called .extracts) contains results function. extraction function used, .extracts column resulting object. See tune_bayes() specific details. Note collect_predictions(), possible row original data point might represented multiple times per tuning parameter. example, bootstrap repeated cross-validation used, multiple rows since sample data point evaluated multiple times. may cause issues merging predictions original data. control_resamples() alias control_grid() meant used fit_resamples().","code":""},{"path":"https://tune.tidymodels.org/dev/reference/control_grid.html","id":"hyperparameters-and-extracted-objects","dir":"Reference","previous_headings":"","what":"Hyperparameters and extracted objects","title":"Control aspects of the grid search process — control_grid","text":"making use submodels, tune can generate predictions calculate metrics multiple model .configurations using one model fit. However, means function supplied control function's extract argument, tune can execute extraction one model fitted. result, collect_extracts() output, tune opts associate extracted objects hyperparameter combination used fit one model workflow, rather hyperparameter combination submodel. output, appears like hyperparameter entry recycled across many .config entries—intentional. See https://parsnip.tidymodels.org/articles/Submodels.html learn submodels.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/control_last_fit.html","id":null,"dir":"Reference","previous_headings":"","what":"Control aspects of the last fit process — control_last_fit","title":"Control aspects of the last fit process — control_last_fit","text":"Control aspects last fit process","code":""},{"path":"https://tune.tidymodels.org/dev/reference/control_last_fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Control aspects of the last fit process — control_last_fit","text":"","code":"control_last_fit(verbose = FALSE, event_level = \"first\", allow_par = FALSE)"},{"path":"https://tune.tidymodels.org/dev/reference/control_last_fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Control aspects of the last fit process — control_last_fit","text":"verbose logical logging results (warnings errors, always shown) generated training single R process. using parallel backends, argument typically result logging. using dark IDE theme, logging messages might hard see; try setting tidymodels.dark option options(tidymodels.dark = TRUE) print lighter colors. event_level single string containing either \"first\" \"second\". argument passed yardstick metric functions type class prediction made, specifies level outcome considered \"event\". allow_par logical allow parallel processing (parallel backend registered).","code":""},{"path":"https://tune.tidymodels.org/dev/reference/control_last_fit.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Control aspects of the last fit process — control_last_fit","text":"control_last_fit() wrapper around control_resamples() meant used last_fit().","code":""},{"path":"https://tune.tidymodels.org/dev/reference/coord_obs_pred.html","id":null,"dir":"Reference","previous_headings":"","what":"Use same scale for plots of observed vs predicted values — coord_obs_pred","title":"Use same scale for plots of observed vs predicted values — coord_obs_pred","text":"regression models, coord_obs_pred() can used ggplot make x- y-axes exact scale along aspect ratio one.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/coord_obs_pred.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Use same scale for plots of observed vs predicted values — coord_obs_pred","text":"","code":"coord_obs_pred(ratio = 1, xlim = NULL, ylim = NULL, expand = TRUE, clip = \"on\")"},{"path":"https://tune.tidymodels.org/dev/reference/coord_obs_pred.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Use same scale for plots of observed vs predicted values — coord_obs_pred","text":"ratio Aspect ratio, expressed y / x. Defaults 1.0. xlim, ylim Limits x y axes. expand currently used. clip drawing clipped extent plot panel? setting \"\" (default) means yes, setting \"\" means . cases, default \"\" changed, setting clip = \"\" can cause unexpected results. allows drawing data points anywhere plot, including plot margins. limits set via xlim ylim data points fall outside limits, data points may show places axes, legend, plot title, plot margins.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/coord_obs_pred.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Use same scale for plots of observed vs predicted values — coord_obs_pred","text":"ggproto object.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/coord_obs_pred.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Use same scale for plots of observed vs predicted values — coord_obs_pred","text":"","code":"# example code data(solubility_test, package = \"modeldata\")  library(ggplot2) p <- ggplot(solubility_test, aes(x = solubility, y = prediction)) +   geom_abline(lty = 2) +   geom_point(alpha = 0.5)  p   p + coord_fixed()   p + coord_obs_pred()"},{"path":"https://tune.tidymodels.org/dev/reference/dot-stash_last_result.html","id":null,"dir":"Reference","previous_headings":"","what":"Save most recent results to search path — .stash_last_result","title":"Save most recent results to search path — .stash_last_result","text":"Save recent results search path","code":""},{"path":"https://tune.tidymodels.org/dev/reference/dot-stash_last_result.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Save most recent results to search path — .stash_last_result","text":"","code":".stash_last_result(x)"},{"path":"https://tune.tidymodels.org/dev/reference/dot-stash_last_result.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Save most recent results to search path — .stash_last_result","text":"x object.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/dot-stash_last_result.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Save most recent results to search path — .stash_last_result","text":"NULL, invisibly.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/dot-stash_last_result.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Save most recent results to search path — .stash_last_result","text":"function assign x .Last.tune.result put search path.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/dot-use_case_weights_with_yardstick.html","id":null,"dir":"Reference","previous_headings":"","what":"Determine if case weights should be passed on to yardstick — .use_case_weights_with_yardstick","title":"Determine if case weights should be passed on to yardstick — .use_case_weights_with_yardstick","text":"S3 method defines logic deciding case weight vector passed yardstick metric functions used measure model performance. current logic frequency weights (.e. hardhat::frequency_weights()) situation occur.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/dot-use_case_weights_with_yardstick.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Determine if case weights should be passed on to yardstick — .use_case_weights_with_yardstick","text":"","code":".use_case_weights_with_yardstick(x)  # S3 method for class 'hardhat_importance_weights' .use_case_weights_with_yardstick(x)  # S3 method for class 'hardhat_frequency_weights' .use_case_weights_with_yardstick(x)"},{"path":"https://tune.tidymodels.org/dev/reference/dot-use_case_weights_with_yardstick.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Determine if case weights should be passed on to yardstick — .use_case_weights_with_yardstick","text":"x vector","code":""},{"path":"https://tune.tidymodels.org/dev/reference/dot-use_case_weights_with_yardstick.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Determine if case weights should be passed on to yardstick — .use_case_weights_with_yardstick","text":"single TRUE FALSE.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/dot-use_case_weights_with_yardstick.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Determine if case weights should be passed on to yardstick — .use_case_weights_with_yardstick","text":"","code":"library(parsnip) library(dplyr)  frequency_weights(1:10) %>%   .use_case_weights_with_yardstick() #> [1] TRUE  importance_weights(seq(1, 10, by = .1))%>%   .use_case_weights_with_yardstick() #> [1] FALSE"},{"path":"https://tune.tidymodels.org/dev/reference/empty_ellipses.html","id":null,"dir":"Reference","previous_headings":"","what":"Get colors for tune text. — check_rset","title":"Get colors for tune text. — check_rset","text":"intended use general public.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/empty_ellipses.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get colors for tune text. — check_rset","text":"","code":"check_rset(x)  check_parameters(wflow, pset = NULL, data, grid_names = character(0))  check_workflow(x, ..., pset = NULL, check_dials = FALSE, call = caller_env())  check_metrics(x, object)  check_initial(   x,   pset,   wflow,   resamples,   metrics,   eval_time,   ctrl,   checks = \"grid\" )  val_class_or_null(x, cls = \"numeric\", where = NULL)  val_class_and_single(x, cls = \"numeric\", where = NULL)  .config_key_from_metrics(x)  estimate_tune_results(x, ..., col_name = \".metrics\")  metrics_info(x)  new_iteration_results(   x,   parameters,   metrics,   eval_time,   eval_time_target,   outcomes = character(0),   rset_info,   workflow )  get_tune_colors()  encode_set(x, pset, ..., as_matrix = FALSE)  check_time(origin, limit)  pull_rset_attributes(x)  empty_ellipses(...)  is_recipe(x)  is_preprocessor(x)  is_workflow(x)"},{"path":"https://tune.tidymodels.org/dev/reference/empty_ellipses.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get colors for tune text. — check_rset","text":"x object. wflow workflow object. pset parameters object. data training data. grid_names character vector column names grid. ... options check_dials logical check NULL parameter object. object workflow object. resamples rset object. metrics metric set. eval_time numeric vector time points dynamic event time metrics computed (e.g. time-dependent ROC curve, etc). ctrl control_grid object. cls character vector possible classes character string calling function. parameters parameters object. outcomes character vector outcome names. rset_info Attributes rset object. workflow workflow used fit iteration results. as_matrix logical return type. origin calculation start time. limit allowable time (minutes).","code":""},{"path":"https://tune.tidymodels.org/dev/reference/example_ames_knn.html","id":null,"dir":"Reference","previous_headings":"","what":"Example Analysis of Ames Housing Data — example_ames_knn","title":"Example Analysis of Ames Housing Data — example_ames_knn","text":"Example Analysis Ames Housing Data","code":""},{"path":"https://tune.tidymodels.org/dev/reference/example_ames_knn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Example Analysis of Ames Housing Data — example_ames_knn","text":"ames_wflow workflow object ames_grid_search,ames_iter_search Results model tuning.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/example_ames_knn.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Example Analysis of Ames Housing Data — example_ames_knn","text":"objects results analysis Ames housing data. K-nearest neighbors model used small predictor set included natural spline transformations Longitude Latitude predictors. code used generate examples :   important note: Since rsample split columns contain reference data, saving disk can results large object sizes object later used. essence, R replaces references actual data. reason, saved zero-row tibbles place. affect use objects examples advised using rsample functions cause issues.","code":"library(tidymodels) library(tune) library(AmesHousing)  # ------------------------------------------------------------------------------  ames <- make_ames()  set.seed(4595) data_split <- initial_split(ames, strata = \"Sale_Price\")  ames_train <- training(data_split)  set.seed(2453) rs_splits <- vfold_cv(ames_train, strata = \"Sale_Price\")  # ------------------------------------------------------------------------------  ames_rec <-   recipe(Sale_Price ~ ., data = ames_train) %>%   step_log(Sale_Price, base = 10) %>%   step_YeoJohnson(Lot_Area, Gr_Liv_Area) %>%   step_other(Neighborhood, threshold = .1)  %>%   step_dummy(all_nominal()) %>%   step_zv(all_predictors()) %>%   step_spline_natural(Longitude, deg_free = tune(\"lon\")) %>%   step_spline_natural(Latitude, deg_free = tune(\"lat\"))  knn_model <-   nearest_neighbor(     mode = \"regression\",     neighbors = tune(\"K\"),     weight_func = tune(),     dist_power = tune()   ) %>%   set_engine(\"kknn\")  ames_wflow <-   workflow() %>%   add_recipe(ames_rec) %>%   add_model(knn_model)  ames_set <-   extract_parameter_set_dials(ames_wflow) %>%   update(K = neighbors(c(1, 50)))  set.seed(7014) ames_grid <-   ames_set %>%   grid_max_entropy(size = 10)  ames_grid_search <-   tune_grid(     ames_wflow,     resamples = rs_splits,     grid = ames_grid   )  set.seed(2082) ames_iter_search <-   tune_bayes(     ames_wflow,     resamples = rs_splits,     param_info = ames_set,     initial = ames_grid_search,     iter = 15   )"},{"path":"https://tune.tidymodels.org/dev/reference/example_ames_knn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Example Analysis of Ames Housing Data — example_ames_knn","text":"","code":"library(tune)  ames_grid_search #> # Tuning results #> # 10-fold cross-validation using stratification  #> # A tibble: 10 × 4 #>    splits           id     .metrics          .notes           #>    <list>           <chr>  <list>            <list>           #>  1 <split [1978/0]> Fold01 <tibble [20 × 9]> <tibble [0 × 1]> #>  2 <split [1979/0]> Fold02 <tibble [20 × 9]> <tibble [0 × 1]> #>  3 <split [1979/0]> Fold03 <tibble [20 × 9]> <tibble [0 × 1]> #>  4 <split [1979/0]> Fold04 <tibble [20 × 9]> <tibble [0 × 1]> #>  5 <split [1979/0]> Fold05 <tibble [20 × 9]> <tibble [0 × 1]> #>  6 <split [1979/0]> Fold06 <tibble [20 × 9]> <tibble [0 × 1]> #>  7 <split [1979/0]> Fold07 <tibble [20 × 9]> <tibble [0 × 1]> #>  8 <split [1979/0]> Fold08 <tibble [20 × 9]> <tibble [0 × 1]> #>  9 <split [1979/0]> Fold09 <tibble [20 × 9]> <tibble [0 × 1]> #> 10 <split [1981/0]> Fold10 <tibble [20 × 9]> <tibble [0 × 1]> ames_iter_search #> # Tuning results #> # 10-fold cross-validation using stratification  #> # A tibble: 110 × 5 #>    splits           id     .metrics          .notes           .iter #>    <list>           <chr>  <list>            <list>           <int> #>  1 <split [1978/0]> Fold01 <tibble [20 × 9]> <tibble [0 × 1]>     0 #>  2 <split [1979/0]> Fold02 <tibble [20 × 9]> <tibble [0 × 1]>     0 #>  3 <split [1979/0]> Fold03 <tibble [20 × 9]> <tibble [0 × 1]>     0 #>  4 <split [1979/0]> Fold04 <tibble [20 × 9]> <tibble [0 × 1]>     0 #>  5 <split [1979/0]> Fold05 <tibble [20 × 9]> <tibble [0 × 1]>     0 #>  6 <split [1979/0]> Fold06 <tibble [20 × 9]> <tibble [0 × 1]>     0 #>  7 <split [1979/0]> Fold07 <tibble [20 × 9]> <tibble [0 × 1]>     0 #>  8 <split [1979/0]> Fold08 <tibble [20 × 9]> <tibble [0 × 1]>     0 #>  9 <split [1979/0]> Fold09 <tibble [20 × 9]> <tibble [0 × 1]>     0 #> 10 <split [1981/0]> Fold10 <tibble [20 × 9]> <tibble [0 × 1]>     0 #> # ℹ 100 more rows"},{"path":"https://tune.tidymodels.org/dev/reference/expo_decay.html","id":null,"dir":"Reference","previous_headings":"","what":"Exponential decay function — expo_decay","title":"Exponential decay function — expo_decay","text":"expo_decay() can used increase decrease function exponentially iterations. can used dynamically set parameters acquisition functions iterations Bayesian optimization proceed.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/expo_decay.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Exponential decay function — expo_decay","text":"","code":"expo_decay(iter, start_val, limit_val, slope = 1/5)"},{"path":"https://tune.tidymodels.org/dev/reference/expo_decay.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Exponential decay function — expo_decay","text":"iter integer current iteration number. start_val number returned first iteration. limit_val number process converges iterations. slope coefficient exponent control rate decay. sign slope controls direction decay.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/expo_decay.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Exponential decay function — expo_decay","text":"single numeric value.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/expo_decay.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Exponential decay function — expo_decay","text":"Note , used acquisition functions tune(), wrapper required since first argument evaluated tuning.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/expo_decay.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Exponential decay function — expo_decay","text":"","code":"library(tibble) library(purrr) library(ggplot2) library(dplyr) tibble(   iter = 1:40,   value = map_dbl(     1:40,     expo_decay,     start_val = .1,     limit_val = 0,     slope = 1 / 5   ) ) %>%   ggplot(aes(x = iter, y = value)) +   geom_path()"},{"path":"https://tune.tidymodels.org/dev/reference/extract-tune.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract elements of tune objects — extract-tune","title":"Extract elements of tune objects — extract-tune","text":"functions extract various elements tune object. exist yet, error thrown. extract_preprocessor() returns formula, recipe, variable expressions used preprocessing. extract_spec_parsnip() returns parsnip model specification. extract_fit_parsnip() returns parsnip model fit object. extract_fit_engine() returns engine specific fit embedded within parsnip model fit. example, using parsnip::linear_reg() \"lm\" engine, returns underlying lm object. extract_mold() returns preprocessed \"mold\" object returned hardhat::mold(). contains information preprocessing, including either prepped recipe, formula terms object, variable selectors. extract_recipe() returns recipe. estimated argument specifies whether fitted original recipe returned. extract_workflow() returns workflow object control option save_workflow = TRUE used. workflow estimated objects produced last_fit().","code":""},{"path":"https://tune.tidymodels.org/dev/reference/extract-tune.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract elements of tune objects — extract-tune","text":"","code":"# S3 method for class 'last_fit' extract_workflow(x, ...)  # S3 method for class 'tune_results' extract_workflow(x, ...)  # S3 method for class 'tune_results' extract_spec_parsnip(x, ...)  # S3 method for class 'tune_results' extract_recipe(x, ..., estimated = TRUE)  # S3 method for class 'tune_results' extract_fit_parsnip(x, ...)  # S3 method for class 'tune_results' extract_fit_engine(x, ...)  # S3 method for class 'tune_results' extract_mold(x, ...)  # S3 method for class 'tune_results' extract_preprocessor(x, ...)"},{"path":"https://tune.tidymodels.org/dev/reference/extract-tune.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract elements of tune objects — extract-tune","text":"x tune_results object. ... currently used. estimated logical whether original (unfit) recipe fitted recipe returned.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/extract-tune.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract elements of tune objects — extract-tune","text":"extracted value tune tune_results, x, described description section.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/extract-tune.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extract elements of tune objects — extract-tune","text":"functions supersede extract_model().","code":""},{"path":"https://tune.tidymodels.org/dev/reference/extract-tune.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract elements of tune objects — extract-tune","text":"","code":"# example code  library(recipes) library(rsample) library(parsnip)  set.seed(6735) tr_te_split <- initial_split(mtcars)  spline_rec <- recipe(mpg ~ ., data = mtcars) %>%   step_spline_natural(disp)  lin_mod <- linear_reg() %>%   set_engine(\"lm\")  spline_res <- last_fit(lin_mod, spline_rec, split = tr_te_split)  extract_preprocessor(spline_res) #>  #> ── Recipe ───────────────────────────────────────────────────────────── #>  #> ── Inputs  #> Number of variables by role #> outcome:    1 #> predictor: 10 #>  #> ── Operations  #> • Natural spline expansion: disp  # The `spec` is the parsnip spec before it has been fit. # The `fit` is the fitted parsnip model. extract_spec_parsnip(spline_res) #> Linear Regression Model Specification (regression) #>  #> Computational engine: lm  #>  extract_fit_parsnip(spline_res) #> parsnip model object #>  #>  #> Call: #> stats::lm(formula = ..y ~ ., data = data) #>  #> Coefficients: #> (Intercept)          cyl           hp         drat           wt   #>   72.720897    -4.748011    -0.004591    -3.009161    -3.729979   #>        qsec           vs           am         gear         carb   #>   -0.075139    -3.193673    -1.049969     1.365458    -0.087406   #>     disp_01      disp_02      disp_03      disp_04      disp_05   #>  -12.974946   -11.920106    -3.731118     1.796121    -8.190165   #>     disp_06      disp_07      disp_08      disp_09      disp_10   #>    6.461960    -2.387850     2.989175    15.749765     6.123262   #>  extract_fit_engine(spline_res) #>  #> Call: #> stats::lm(formula = ..y ~ ., data = data) #>  #> Coefficients: #> (Intercept)          cyl           hp         drat           wt   #>   72.720897    -4.748011    -0.004591    -3.009161    -3.729979   #>        qsec           vs           am         gear         carb   #>   -0.075139    -3.193673    -1.049969     1.365458    -0.087406   #>     disp_01      disp_02      disp_03      disp_04      disp_05   #>  -12.974946   -11.920106    -3.731118     1.796121    -8.190165   #>     disp_06      disp_07      disp_08      disp_09      disp_10   #>    6.461960    -2.387850     2.989175    15.749765     6.123262   #>   # The mold is returned from `hardhat::mold()`, and contains the # predictors, outcomes, and information about the preprocessing # for use on new data at `predict()` time. extract_mold(spline_res) #> $predictors #> # A tibble: 24 × 19 #>      cyl    hp  drat    wt  qsec    vs    am  gear  carb   disp_01 #>    <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>     <dbl> #>  1     8   205  2.93  5.25  18.0     0     0     3     4 0         #>  2     4    95  3.92  3.15  22.9     1     0     4     2 0.00304   #>  3     6   175  3.62  2.77  15.5     0     1     5     6 0.0000580 #>  4     8   245  3.73  3.84  15.4     0     0     3     4 0         #>  5     4    52  4.93  1.62  18.5     1     1     4     2 0.110     #>  6     8   180  3.07  3.78  18       0     0     3     3 0         #>  7     8   215  3     5.42  17.8     0     0     3     4 0         #>  8     8   175  3.15  3.44  17.0     0     0     3     2 0         #>  9     8   180  3.07  4.07  17.4     0     0     3     3 0         #> 10     6   110  3.9   2.88  17.0     0     1     4     4 0         #> # ℹ 14 more rows #> # ℹ 9 more variables: disp_02 <dbl>, disp_03 <dbl>, disp_04 <dbl>, #> #   disp_05 <dbl>, disp_06 <dbl>, disp_07 <dbl>, disp_08 <dbl>, #> #   disp_09 <dbl>, disp_10 <dbl> #>  #> $outcomes #> # A tibble: 24 × 1 #>      mpg #>    <dbl> #>  1  10.4 #>  2  22.8 #>  3  19.7 #>  4  13.3 #>  5  30.4 #>  6  15.2 #>  7  10.4 #>  8  18.7 #>  9  16.4 #> 10  21   #> # ℹ 14 more rows #>  #> $blueprint #> Recipe blueprint: #> # Predictors: 10 #> # Outcomes: 1 #> Intercept: FALSE #> Novel Levels: FALSE #> Composition: tibble #>  #>  #> $extras #> $extras$roles #> NULL #>  #>   # A useful shortcut is to extract the fitted recipe from the workflow extract_recipe(spline_res) #> ── Recipe ───────────────────────────────────────────────────────────── #>  #> ── Inputs  #> Number of variables by role #> outcome:    1 #> predictor: 10 #>  #> ── Training information  #> Training data contained 24 data points and no incomplete rows. #>  #> ── Operations  #> • Natural spline expansion: disp | Trained  # That is identical to identical(   extract_mold(spline_res)$blueprint$recipe,   extract_recipe(spline_res) ) #> [1] TRUE"},{"path":[]},{"path":"https://tune.tidymodels.org/dev/reference/extract_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convenience functions to extract model — extract_model","text":"","code":"extract_model(x)"},{"path":"https://tune.tidymodels.org/dev/reference/extract_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convenience functions to extract model — extract_model","text":"x fitted workflow object.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/extract_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convenience functions to extract model — extract_model","text":"fitted model.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/extract_model.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Convenience functions to extract model — extract_model","text":"Use extract_fit_engine() instead extract_model(). extracting fitted results, workflow easily accessible. interest model, functions can used shortcut","code":""},{"path":"https://tune.tidymodels.org/dev/reference/filter_parameters.html","id":null,"dir":"Reference","previous_headings":"","what":"Remove some tuning parameter results — filter_parameters","title":"Remove some tuning parameter results — filter_parameters","text":"objects produced tune_*() functions, may subset tuning parameter combinations interest. large data sets, might helpful able remove results. function trims .metrics column unwanted results well columns .predictions .extracts (requested).","code":""},{"path":"https://tune.tidymodels.org/dev/reference/filter_parameters.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Remove some tuning parameter results — filter_parameters","text":"","code":"filter_parameters(x, ..., parameters = NULL)"},{"path":"https://tune.tidymodels.org/dev/reference/filter_parameters.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Remove some tuning parameter results — filter_parameters","text":"x object class tune_results multiple tuning parameters. ... Expressions return logical value, defined terms tuning parameter values. multiple expressions included, combined & operator. rows conditions evaluate TRUE kept. parameters tibble tuning parameter values can used filter predicted values processing. tibble columns tuning parameter identifiers (e.g. \"my_param\" tune(\"my_param\") used). can multiple rows one columns. used, parameter must named.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/filter_parameters.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Remove some tuning parameter results — filter_parameters","text":"version x lists columns retain parameter combinations parameters satisfied filtering logic.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/filter_parameters.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Remove some tuning parameter results — filter_parameters","text":"Removing parameter combinations might affect results autoplot() object.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/filter_parameters.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Remove some tuning parameter results — filter_parameters","text":"","code":"library(dplyr) library(tibble)  # For grid search: data(\"example_ames_knn\")  ## ----------------------------------------------------------------------------- # select all combinations using the 'rank' weighting scheme  ames_grid_search %>%   collect_metrics() #> # A tibble: 20 × 11 #>        K weight_func  dist_power   lon   lat .metric .estimator   mean #>    <int> <chr>             <dbl> <int> <int> <chr>   <chr>       <dbl> #>  1    35 optimal           1.32      8     1 rmse    standard   0.0785 #>  2    35 optimal           1.32      8     1 rsq     standard   0.823  #>  3    35 rank              1.29      3    13 rmse    standard   0.0809 #>  4    35 rank              1.29      3    13 rsq     standard   0.814  #>  5    21 cos               0.626     1     4 rmse    standard   0.0746 #>  6    21 cos               0.626     1     4 rsq     standard   0.836  #>  7     4 biweight          0.311     8     4 rmse    standard   0.0777 #>  8     4 biweight          0.311     8     4 rsq     standard   0.814  #>  9    32 triangular        0.165     9    15 rmse    standard   0.0770 #> 10    32 triangular        0.165     9    15 rsq     standard   0.826  #> 11     3 rank              1.86     10    15 rmse    standard   0.0875 #> 12     3 rank              1.86     10    15 rsq     standard   0.762  #> 13    40 triangular        0.167    11     7 rmse    standard   0.0778 #> 14    40 triangular        0.167    11     7 rsq     standard   0.822  #> 15    12 epanechnikov      1.53      4     7 rmse    standard   0.0774 #> 16    12 epanechnikov      1.53      4     7 rsq     standard   0.820  #> 17     5 rank              0.411     2     7 rmse    standard   0.0740 #> 18     5 rank              0.411     2     7 rsq     standard   0.833  #> 19    33 triweight         0.511    10     3 rmse    standard   0.0728 #> 20    33 triweight         0.511    10     3 rsq     standard   0.842  #> # ℹ 3 more variables: n <int>, std_err <dbl>, .config <chr>  filter_parameters(ames_grid_search, weight_func == \"rank\") %>%   collect_metrics() #> # A tibble: 6 × 11 #>       K weight_func dist_power   lon   lat .metric .estimator   mean #>   <int> <chr>            <dbl> <int> <int> <chr>   <chr>       <dbl> #> 1    35 rank             1.29      3    13 rmse    standard   0.0809 #> 2    35 rank             1.29      3    13 rsq     standard   0.814  #> 3     3 rank             1.86     10    15 rmse    standard   0.0875 #> 4     3 rank             1.86     10    15 rsq     standard   0.762  #> 5     5 rank             0.411     2     7 rmse    standard   0.0740 #> 6     5 rank             0.411     2     7 rsq     standard   0.833  #> # ℹ 3 more variables: n <int>, std_err <dbl>, .config <chr>  rank_only <- tibble::tibble(weight_func = \"rank\") filter_parameters(ames_grid_search, parameters = rank_only) %>%   collect_metrics() #> # A tibble: 6 × 11 #>       K weight_func dist_power   lon   lat .metric .estimator   mean #>   <int> <chr>            <dbl> <int> <int> <chr>   <chr>       <dbl> #> 1    35 rank             1.29      3    13 rmse    standard   0.0809 #> 2    35 rank             1.29      3    13 rsq     standard   0.814  #> 3     3 rank             1.86     10    15 rmse    standard   0.0875 #> 4     3 rank             1.86     10    15 rsq     standard   0.762  #> 5     5 rank             0.411     2     7 rmse    standard   0.0740 #> 6     5 rank             0.411     2     7 rsq     standard   0.833  #> # ℹ 3 more variables: n <int>, std_err <dbl>, .config <chr>  ## ----------------------------------------------------------------------------- # Keep only the results from the numerically best combination  ames_iter_search %>%   collect_metrics() #> # A tibble: 40 × 12 #>        K weight_func dist_power   lon   lat .metric .estimator   mean #>    <int> <chr>            <dbl> <int> <int> <chr>   <chr>       <dbl> #>  1    35 optimal          1.32      8     1 rmse    standard   0.0785 #>  2    35 optimal          1.32      8     1 rsq     standard   0.823  #>  3    35 rank             1.29      3    13 rmse    standard   0.0809 #>  4    35 rank             1.29      3    13 rsq     standard   0.814  #>  5    21 cos              0.626     1     4 rmse    standard   0.0746 #>  6    21 cos              0.626     1     4 rsq     standard   0.836  #>  7     4 biweight         0.311     8     4 rmse    standard   0.0777 #>  8     4 biweight         0.311     8     4 rsq     standard   0.814  #>  9    32 triangular       0.165     9    15 rmse    standard   0.0770 #> 10    32 triangular       0.165     9    15 rsq     standard   0.826  #> # ℹ 30 more rows #> # ℹ 4 more variables: n <int>, std_err <dbl>, .config <chr>, #> #   .iter <int>  best_param <- select_best(ames_iter_search, metric = \"rmse\") ames_iter_search %>%   filter_parameters(parameters = best_param) %>%   collect_metrics() #> # A tibble: 2 × 12 #>       K weight_func dist_power   lon   lat .metric .estimator   mean #>   <int> <chr>            <dbl> <int> <int> <chr>   <chr>       <dbl> #> 1    33 triweight        0.511    10     3 rmse    standard   0.0728 #> 2    33 triweight        0.511    10     3 rsq     standard   0.842  #> # ℹ 4 more variables: n <int>, std_err <dbl>, .config <chr>, #> #   .iter <int>"},{"path":"https://tune.tidymodels.org/dev/reference/finalize_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Splice final parameters into objects — finalize_model","title":"Splice final parameters into objects — finalize_model","text":"finalize_* functions take list tibble tuning parameter values update objects values.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/finalize_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Splice final parameters into objects — finalize_model","text":"","code":"finalize_model(x, parameters)  finalize_recipe(x, parameters)  finalize_workflow(x, parameters)  finalize_tailor(x, parameters)"},{"path":"https://tune.tidymodels.org/dev/reference/finalize_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Splice final parameters into objects — finalize_model","text":"x recipe, parsnip model specification, workflow. parameters list 1-row tibble parameter values. Note column names tibble id fields attached tune(). example, Examples section , model tune(\"K\"). case, parameter tibble \"K\" \"neighbors\".","code":""},{"path":"https://tune.tidymodels.org/dev/reference/finalize_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Splice final parameters into objects — finalize_model","text":"updated version x.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/finalize_model.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Splice final parameters into objects — finalize_model","text":"","code":"data(\"example_ames_knn\")  library(parsnip) knn_model <-   nearest_neighbor(     mode = \"regression\",     neighbors = tune(\"K\"),     weight_func = tune(),     dist_power = tune()   ) %>%   set_engine(\"kknn\")  lowest_rmse <- select_best(ames_grid_search, metric = \"rmse\") lowest_rmse #> # A tibble: 1 × 6 #>       K weight_func dist_power   lon   lat .config               #>   <int> <chr>            <dbl> <int> <int> <chr>                 #> 1    33 triweight        0.511    10     3 Preprocessor10_Model1  knn_model #> K-Nearest Neighbor Model Specification (regression) #>  #> Main Arguments: #>   neighbors = tune(\"K\") #>   weight_func = tune() #>   dist_power = tune() #>  #> Computational engine: kknn  #>  finalize_model(knn_model, lowest_rmse) #> K-Nearest Neighbor Model Specification (regression) #>  #> Main Arguments: #>   neighbors = 33 #>   weight_func = triweight #>   dist_power = 0.511191629664972 #>  #> Computational engine: kknn  #>"},{"path":"https://tune.tidymodels.org/dev/reference/fit_best.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit a model to the numerically optimal configuration — fit_best","title":"Fit a model to the numerically optimal configuration — fit_best","text":"fit_best() takes results model tuning fits training set using tuning parameters associated best performance.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/fit_best.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit a model to the numerically optimal configuration — fit_best","text":"","code":"fit_best(x, ...)  # Default S3 method fit_best(x, ...)  # S3 method for class 'tune_results' fit_best(   x,   ...,   metric = NULL,   eval_time = NULL,   parameters = NULL,   verbose = FALSE,   add_validation_set = NULL )"},{"path":"https://tune.tidymodels.org/dev/reference/fit_best.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit a model to the numerically optimal configuration — fit_best","text":"x results class tune_results (coming functions tune_grid(), tune_bayes(), etc). control option save_workflow = TRUE used. ... currently used, must empty. metric character string (NULL) metric optimize. NULL, first metric used. eval_time single numeric time point dynamic event time metrics chosen (e.g., time-dependent ROC curve, etc). values consistent values used create x. NULL default automatically use first evaluation time used x. parameters optional 1-row tibble tuning parameter settings, column tuning parameter. tibble columns tuning parameter identifier (e.g. \"my_param\" tune(\"my_param\") used). NULL, argument set select_best(metric, eval_time). NULL, parameters overwrites specification via metric, eval_time. verbose logical printing logging. add_validation_set resamples embedded x split training set validation set, validation set included data set used train model? , training set used. NULL, validation set used resamples originating rsample::validation_set() used resamples originating rsample::validation_split().","code":""},{"path":"https://tune.tidymodels.org/dev/reference/fit_best.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit a model to the numerically optimal configuration — fit_best","text":"fitted workflow.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/fit_best.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fit a model to the numerically optimal configuration — fit_best","text":"function shortcut manual steps :","code":"best_param <- select_best(tune_results, metric) # or other `select_*()`   wflow <- finalize_workflow(wflow, best_param)  # or just `finalize_model()`   wflow_fit <- fit(wflow, data_set)"},{"path":"https://tune.tidymodels.org/dev/reference/fit_best.html","id":"case-weights","dir":"Reference","previous_headings":"","what":"Case Weights","title":"Fit a model to the numerically optimal configuration — fit_best","text":"models can utilize case weights training. tidymodels currently supports two types case weights: importance weights (doubles) frequency weights (integers). Frequency weights used model fitting evaluation, whereas importance weights used fitting. know model capable using case weights, create model spec test using parsnip::case_weights_allowed(). use , need numeric column data set passed either hardhat:: importance_weights() hardhat::frequency_weights(). functions fit_resamples() tune_*() functions, model must contained inside workflows::workflow(). declare case weights used, invoke workflows::add_case_weights() corresponding (unquoted) column name. , packages appropriately handle weights model fitting (appropriate) performance estimation.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/fit_best.html","id":"see-also","dir":"Reference","previous_headings":"","what":"See also","title":"Fit a model to the numerically optimal configuration — fit_best","text":"last_fit() closely related fit_best(). give access workflow fitted training data situated somewhat differently modeling workflow. fit_best() picks tuning function like tune_grid() take tuning results fitted workflow, ready predict assess . last_fit() assumes made choice hyperparameters finalized workflow take finalized workflow fitted workflow performance assessment test data. fit_best() gives fitted workflow, last_fit() gives performance results. want fitted workflow, can extract result last_fit() via extract_workflow().","code":""},{"path":"https://tune.tidymodels.org/dev/reference/fit_best.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit a model to the numerically optimal configuration — fit_best","text":"","code":"library(recipes) library(rsample) library(parsnip) library(dplyr)  data(meats, package = \"modeldata\") meats <- meats %>% select(-water, -fat)  set.seed(1) meat_split <- initial_split(meats) meat_train <- training(meat_split) meat_test  <- testing(meat_split)  set.seed(2) meat_rs <- vfold_cv(meat_train, v = 10)  pca_rec <-   recipe(protein ~ ., data = meat_train) %>%   step_normalize(all_numeric_predictors()) %>%   step_pca(all_numeric_predictors(), num_comp = tune())  knn_mod <- nearest_neighbor(neighbors = tune()) %>% set_mode(\"regression\")  ctrl <- control_grid(save_workflow = TRUE)  set.seed(128) knn_pca_res <-   tune_grid(knn_mod, pca_rec, resamples = meat_rs, grid = 10, control = ctrl)  knn_fit <- fit_best(knn_pca_res, verbose = TRUE) #> Using rmse as the metric, the optimal parameters were: #>   neighbors: 7 #>   num_comp:  4 #>  #> ℹ Fitting using 161 data points... #> ✔ Done. predict(knn_fit, meat_test) #> # A tibble: 54 × 1 #>    .pred #>    <dbl> #>  1  19.7 #>  2  20.1 #>  3  15.3 #>  4  13.3 #>  5  19.5 #>  6  21.1 #>  7  19.9 #>  8  18.7 #>  9  19.6 #> 10  17.9 #> # ℹ 44 more rows"},{"path":"https://tune.tidymodels.org/dev/reference/fit_resamples.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit multiple models via resampling — fit_resamples","title":"Fit multiple models via resampling — fit_resamples","text":"fit_resamples() computes set performance metrics across one resamples. perform tuning (see tune_grid() tune_bayes() ), instead used fitting single model+recipe model+formula combination across many resamples.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/fit_resamples.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit multiple models via resampling — fit_resamples","text":"","code":"fit_resamples(object, ...)  # S3 method for class 'model_spec' fit_resamples(   object,   preprocessor,   resamples,   ...,   metrics = NULL,   eval_time = NULL,   control = control_resamples() )  # S3 method for class 'workflow' fit_resamples(   object,   resamples,   ...,   metrics = NULL,   eval_time = NULL,   control = control_resamples() )"},{"path":"https://tune.tidymodels.org/dev/reference/fit_resamples.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit multiple models via resampling — fit_resamples","text":"object parsnip model specification unfitted workflow(). tuning parameters allowed; arguments marked tune(), values must finalized. ... Currently unused. preprocessor traditional model formula recipe created using recipes::recipe(). resamples rset resampling object created rsample function, rsample::vfold_cv(). metrics yardstick::metric_set(), NULL compute standard set metrics. eval_time numeric vector time points dynamic event time metrics computed (e.g. time-dependent ROC curve, etc). values must non-negative probably greater largest event time training set (See Details ). control control_resamples() object used fine tune resampling process.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/fit_resamples.html","id":"case-weights","dir":"Reference","previous_headings":"","what":"Case Weights","title":"Fit multiple models via resampling — fit_resamples","text":"models can utilize case weights training. tidymodels currently supports two types case weights: importance weights (doubles) frequency weights (integers). Frequency weights used model fitting evaluation, whereas importance weights used fitting. know model capable using case weights, create model spec test using parsnip::case_weights_allowed(). use , need numeric column data set passed either hardhat:: importance_weights() hardhat::frequency_weights(). functions fit_resamples() tune_*() functions, model must contained inside workflows::workflow(). declare case weights used, invoke workflows::add_case_weights() corresponding (unquoted) column name. , packages appropriately handle weights model fitting (appropriate) performance estimation.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/fit_resamples.html","id":"censored-regression-models","dir":"Reference","previous_headings":"","what":"Censored Regression Models","title":"Fit multiple models via resampling — fit_resamples","text":"Three types metrics can used assess quality censored regression models: static: prediction independent time. dynamic: prediction time-specific probability (e.g., survival probability) measured one particular times. integrated: dynamic metric returns integral different metrics time point. metrics chosen user affects many evaluation times specified. example:   Values eval_time less largest observed event time training data. many non-parametric models, results beyond largest time corresponding event constant (NA).","code":"# Needs no `eval_time` value metric_set(concordance_survival)  # Needs at least one `eval_time` metric_set(brier_survival) metric_set(brier_survival, concordance_survival)  # Needs at least two eval_time` values metric_set(brier_survival_integrated, concordance_survival) metric_set(brier_survival_integrated, concordance_survival) metric_set(brier_survival_integrated, concordance_survival, brier_survival)"},{"path":"https://tune.tidymodels.org/dev/reference/fit_resamples.html","id":"performance-metrics","dir":"Reference","previous_headings":"","what":"Performance Metrics","title":"Fit multiple models via resampling — fit_resamples","text":"use performance metrics, yardstick::metric_set() function can used pick measured model. multiple metrics desired, can bundled. example, estimate area ROC curve well sensitivity specificity (typical probability cutoff 0.50), metrics argument given: metric calculated candidate model. metric set provided, one created: regression models, root mean squared error coefficient determination computed. classification, area ROC curve overall accuracy computed. Note metrics also determine type predictions estimated tuning. example, classification problem, metrics used associated hard class predictions, classification probabilities created. --sample estimates metrics contained list column called .metrics. tibble contains row metric columns value, estimator type, . collect_metrics() can used objects collapse results resampled (obtain final resampling estimates per tuning parameter combination).","code":"metrics = metric_set(roc_auc, sens, spec)"},{"path":"https://tune.tidymodels.org/dev/reference/fit_resamples.html","id":"obtaining-predictions","dir":"Reference","previous_headings":"","what":"Obtaining Predictions","title":"Fit multiple models via resampling — fit_resamples","text":"control_grid(save_pred = TRUE), output tibble contains list column called .predictions --sample predictions parameter combination grid fold (can large). elements tibble tibbles columns tuning parameters, row number original data object (.row), outcome data (name(s) original data), columns created predictions. example, simple regression problems, function generates column called .pred . noted , prediction columns returned determined type metric(s) requested. list column can unnested using tidyr::unnest() using convenience function collect_predictions().","code":""},{"path":"https://tune.tidymodels.org/dev/reference/fit_resamples.html","id":"extracting-information","dir":"Reference","previous_headings":"","what":"Extracting Information","title":"Fit multiple models via resampling — fit_resamples","text":"extract control option result additional function returned called .extracts. list column tibbles containing results user's function tuning parameter combination. can enable returning model /recipe object created resampling. Note result large return object, depending returned. control function contains option (extract) can used retain model recipe created within resamples. argument function single argument. value argument given function resample workflow object (see workflows::workflow() information). Several helper functions can used easily pull preprocessing /model information workflow, extract_preprocessor() extract_fit_parsnip(). example, interest getting parsnip model fit back, one use: Note function given extract argument evaluated every model fit (opposed every model evaluated). noted , cases, model predictions can derived sub-models , cases, every row tuning parameter grid separate R object associated .","code":"extract = function (x) extract_fit_parsnip(x)"},{"path":[]},{"path":"https://tune.tidymodels.org/dev/reference/fit_resamples.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit multiple models via resampling — fit_resamples","text":"","code":"library(recipes) library(rsample) library(parsnip) library(workflows)  set.seed(6735) folds <- vfold_cv(mtcars, v = 5)  spline_rec <- recipe(mpg ~ ., data = mtcars) %>%   step_spline_natural(disp) %>%   step_spline_natural(wt)  lin_mod <- linear_reg() %>%   set_engine(\"lm\")  control <- control_resamples(save_pred = TRUE)  spline_res <- fit_resamples(lin_mod, spline_rec, folds, control = control) #> → A | warning: prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\") #> There were issues with some computations   A: x1 #> There were issues with some computations   A: x5 #>   spline_res #> # Resampling results #> # 5-fold cross-validation  #> # A tibble: 5 × 5 #>   splits         id    .metrics         .notes           .predictions #>   <list>         <chr> <list>           <list>           <list>       #> 1 <split [25/7]> Fold1 <tibble [2 × 4]> <tibble [1 × 4]> <tibble>     #> 2 <split [25/7]> Fold2 <tibble [2 × 4]> <tibble [1 × 4]> <tibble>     #> 3 <split [26/6]> Fold3 <tibble [2 × 4]> <tibble [1 × 4]> <tibble>     #> 4 <split [26/6]> Fold4 <tibble [2 × 4]> <tibble [1 × 4]> <tibble>     #> 5 <split [26/6]> Fold5 <tibble [2 × 4]> <tibble [1 × 4]> <tibble>     #>  #> There were issues with some computations: #>  #>   - Warning(s) x5: prediction from rank-deficient fit; consider predict(., r... #>  #> Run `show_notes(.Last.tune.result)` for more information.  show_best(spline_res, metric = \"rmse\") #> # A tibble: 1 × 6 #>   .metric .estimator   mean     n std_err .config              #>   <chr>   <chr>       <dbl> <int>   <dbl> <chr>                #> 1 rmse    standard   47788.     5  47699. Preprocessor1_Model1  # You can also wrap up a preprocessor and a model into a workflow, and # supply that to `fit_resamples()` instead. Here, a workflows \"variables\" # preprocessor is used, which lets you supply terms using dplyr selectors. # The variables are used as-is, no preprocessing is done to them. wf <- workflow() %>%   add_variables(outcomes = mpg, predictors = everything()) %>%   add_model(lin_mod)  wf_res <- fit_resamples(wf, folds)"},{"path":"https://tune.tidymodels.org/dev/reference/get_metric_time.html","id":null,"dir":"Reference","previous_headings":"","what":"Get time for analysis of dynamic survival metrics — get_metric_time","title":"Get time for analysis of dynamic survival metrics — get_metric_time","text":"Get time analysis dynamic survival metrics","code":""},{"path":"https://tune.tidymodels.org/dev/reference/get_metric_time.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get time for analysis of dynamic survival metrics — get_metric_time","text":"","code":"get_metric_time(metrics, eval_time)"},{"path":"https://tune.tidymodels.org/dev/reference/get_metric_time.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get time for analysis of dynamic survival metrics — get_metric_time","text":"metrics metric set. eval_time vector evaluation times.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/int_pctl.tune_results.html","id":null,"dir":"Reference","previous_headings":"","what":"Bootstrap confidence intervals for performance metrics — int_pctl.tune_results","title":"Bootstrap confidence intervals for performance metrics — int_pctl.tune_results","text":"Using --sample predictions, bootstrap used create percentile confidence intervals.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/int_pctl.tune_results.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bootstrap confidence intervals for performance metrics — int_pctl.tune_results","text":"","code":"# S3 method for class 'tune_results' int_pctl(   .data,   metrics = NULL,   eval_time = NULL,   times = 1001,   parameters = NULL,   alpha = 0.05,   allow_par = TRUE,   event_level = \"first\",   ... )"},{"path":"https://tune.tidymodels.org/dev/reference/int_pctl.tune_results.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bootstrap confidence intervals for performance metrics — int_pctl.tune_results","text":".data object class tune_results save_pred = TRUE option used control function. metrics yardstick::metric_set(). default, uses metrics original object. eval_time vector evaluation times censored regression models. NULL appropriate otherwise. NULL used censored models, evaluation time selected, warning issued. times number bootstrap samples. parameters optional tibble tuning parameter values can used filter predicted values processing. tibble columns tuning parameter identifier (e.g. \"my_param\" tune(\"my_param\") used). alpha Level significance. allow_par logical allow parallel processing (parallel backend registered). event_level single string. Either \"first\" \"second\" specify level truth consider \"event\". ... currently used.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/int_pctl.tune_results.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bootstrap confidence intervals for performance metrics — int_pctl.tune_results","text":"tibble metrics additional columns .lower .upper.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/int_pctl.tune_results.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Bootstrap confidence intervals for performance metrics — int_pctl.tune_results","text":"model configuration (), function takes bootstrap samples --sample predicted values. bootstrap sample, metrics computed used compute confidence intervals. See rsample::int_pctl() references therein details. Note .estimate column likely different results given collect_metrics() since different estimator used. Since random numbers used sampling, set random number seed prior running function. number bootstrap samples large reliable intervals. defaults reflect fewest samples used. computations configuration can extensive. increase computational efficiency parallel processing can used. future package used . execute resampling iterations parallel, specify plan future first. allow_par argument can used avoid parallelism. Also, censored regression model used numerous evaluation times, computations can take long time unless times filtered eval_time argument.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/int_pctl.tune_results.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Bootstrap confidence intervals for performance metrics — int_pctl.tune_results","text":"Davison, ., & Hinkley, D. (1997). Bootstrap Methods Application. Cambridge: Cambridge University Press. doi:10.1017/CBO9780511802843","code":""},{"path":[]},{"path":"https://tune.tidymodels.org/dev/reference/int_pctl.tune_results.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bootstrap confidence intervals for performance metrics — int_pctl.tune_results","text":"","code":"if (rlang::is_installed(\"modeldata\")) {   data(Sacramento, package = \"modeldata\")   library(rsample)   library(parsnip)    set.seed(13)   sac_rs <- vfold_cv(Sacramento)    lm_res <-     linear_reg() %>%     fit_resamples(       log10(price) ~ beds + baths + sqft + type + latitude + longitude,       resamples = sac_rs,       control = control_resamples(save_pred = TRUE)     )    set.seed(31)   int_pctl(lm_res) } #> # A tibble: 2 × 6 #>   .metric .estimator .lower .estimate .upper .config              #>   <chr>   <chr>       <dbl>     <dbl>  <dbl> <chr>                #> 1 rmse    bootstrap   0.141     0.150  0.160 Preprocessor1_Model1 #> 2 rsq     bootstrap   0.520     0.566  0.607 Preprocessor1_Model1"},{"path":"https://tune.tidymodels.org/dev/reference/last_fit.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit the final best model to the training set and evaluate the test set — last_fit","title":"Fit the final best model to the training set and evaluate the test set — last_fit","text":"last_fit() emulates process , determining best model, final fit entire training set needed evaluated test set.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/last_fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit the final best model to the training set and evaluate the test set — last_fit","text":"","code":"last_fit(object, ...)  # S3 method for class 'model_spec' last_fit(   object,   preprocessor,   split,   ...,   metrics = NULL,   eval_time = NULL,   control = control_last_fit(),   add_validation_set = FALSE )  # S3 method for class 'workflow' last_fit(   object,   split,   ...,   metrics = NULL,   eval_time = NULL,   control = control_last_fit(),   add_validation_set = FALSE )"},{"path":"https://tune.tidymodels.org/dev/reference/last_fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit the final best model to the training set and evaluate the test set — last_fit","text":"object parsnip model specification unfitted workflow(). tuning parameters allowed; arguments marked tune(), values must finalized. ... Currently unused. preprocessor traditional model formula recipe created using recipes::recipe(). split rsplit object created rsample::initial_split() rsample::initial_validation_split(). metrics yardstick::metric_set(), NULL compute standard set metrics. eval_time numeric vector time points dynamic event time metrics computed (e.g. time-dependent ROC curve, etc). values must non-negative probably greater largest event time training set (See Details ). control control_last_fit() object used fine tune last fit process. add_validation_set 3-way splits training, validation, test set via rsample::initial_validation_split(), validation set included data set used train model. , training set used.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/last_fit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit the final best model to the training set and evaluate the test set — last_fit","text":"single row tibble emulates structure fit_resamples(). However, list column called .workflow also attached fitted model (recipe, ) used training set. Helper functions formatting tuning results like collect_metrics() collect_predictions() can used last_fit() output.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/last_fit.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fit the final best model to the training set and evaluate the test set — last_fit","text":"function intended used fitting variety models final tuning parameters () finalized. next step fit using entire training set verify performance using test data.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/last_fit.html","id":"case-weights","dir":"Reference","previous_headings":"","what":"Case Weights","title":"Fit the final best model to the training set and evaluate the test set — last_fit","text":"models can utilize case weights training. tidymodels currently supports two types case weights: importance weights (doubles) frequency weights (integers). Frequency weights used model fitting evaluation, whereas importance weights used fitting. know model capable using case weights, create model spec test using parsnip::case_weights_allowed(). use , need numeric column data set passed either hardhat:: importance_weights() hardhat::frequency_weights(). functions fit_resamples() tune_*() functions, model must contained inside workflows::workflow(). declare case weights used, invoke workflows::add_case_weights() corresponding (unquoted) column name. , packages appropriately handle weights model fitting (appropriate) performance estimation.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/last_fit.html","id":"censored-regression-models","dir":"Reference","previous_headings":"","what":"Censored Regression Models","title":"Fit the final best model to the training set and evaluate the test set — last_fit","text":"Three types metrics can used assess quality censored regression models: static: prediction independent time. dynamic: prediction time-specific probability (e.g., survival probability) measured one particular times. integrated: dynamic metric returns integral different metrics time point. metrics chosen user affects many evaluation times specified. example:   Values eval_time less largest observed event time training data. many non-parametric models, results beyond largest time corresponding event constant (NA).","code":"# Needs no `eval_time` value metric_set(concordance_survival)  # Needs at least one `eval_time` metric_set(brier_survival) metric_set(brier_survival, concordance_survival)  # Needs at least two eval_time` values metric_set(brier_survival_integrated, concordance_survival) metric_set(brier_survival_integrated, concordance_survival) metric_set(brier_survival_integrated, concordance_survival, brier_survival)"},{"path":"https://tune.tidymodels.org/dev/reference/last_fit.html","id":"see-also","dir":"Reference","previous_headings":"","what":"See also","title":"Fit the final best model to the training set and evaluate the test set — last_fit","text":"last_fit() closely related fit_best(). give access workflow fitted training data situated somewhat differently modeling workflow. fit_best() picks tuning function like tune_grid() take tuning results fitted workflow, ready predict assess . last_fit() assumes made choice hyperparameters finalized workflow take finalized workflow fitted workflow performance assessment test data. fit_best() gives fitted workflow, last_fit() gives performance results. want fitted workflow, can extract result last_fit() via extract_workflow().","code":""},{"path":"https://tune.tidymodels.org/dev/reference/last_fit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit the final best model to the training set and evaluate the test set — last_fit","text":"","code":"library(recipes) library(rsample) library(parsnip)  set.seed(6735) tr_te_split <- initial_split(mtcars)  spline_rec <- recipe(mpg ~ ., data = mtcars) %>%   step_spline_natural(disp)  lin_mod <- linear_reg() %>%   set_engine(\"lm\")  spline_res <- last_fit(lin_mod, spline_rec, split = tr_te_split) spline_res #> # Resampling results #> # Manual resampling  #> # A tibble: 1 × 6 #>   splits         id           .metrics .notes   .predictions .workflow  #>   <list>         <chr>        <list>   <list>   <list>       <list>     #> 1 <split [24/8]> train/test … <tibble> <tibble> <tibble>     <workflow>  # test set metrics collect_metrics(spline_res) #> # A tibble: 2 × 4 #>   .metric .estimator .estimate .config              #>   <chr>   <chr>          <dbl> <chr>                #> 1 rmse    standard       2.44  Preprocessor1_Model1 #> 2 rsq     standard       0.799 Preprocessor1_Model1  # test set predictions collect_predictions(spline_res) #> # A tibble: 8 × 5 #>   .pred id                .row   mpg .config              #>   <dbl> <chr>            <int> <dbl> <chr>                #> 1  21.8 train/test split     1  21   Preprocessor1_Model1 #> 2  23.2 train/test split     3  22.8 Preprocessor1_Model1 #> 3  17.4 train/test split     7  14.3 Preprocessor1_Model1 #> 4  17.3 train/test split    10  19.2 Preprocessor1_Model1 #> 5  30.3 train/test split    18  32.4 Preprocessor1_Model1 #> 6  22.4 train/test split    25  19.2 Preprocessor1_Model1 #> 7  31.2 train/test split    26  27.3 Preprocessor1_Model1 #> 8  19.5 train/test split    32  21.4 Preprocessor1_Model1  # or use a workflow  library(workflows) spline_wfl <-   workflow() %>%   add_recipe(spline_rec) %>%   add_model(lin_mod)  last_fit(spline_wfl, split = tr_te_split) #> # Resampling results #> # Manual resampling  #> # A tibble: 1 × 6 #>   splits         id           .metrics .notes   .predictions .workflow  #>   <list>         <chr>        <list>   <list>   <list>       <list>     #> 1 <split [24/8]> train/test … <tibble> <tibble> <tibble>     <workflow>"},{"path":"https://tune.tidymodels.org/dev/reference/load_pkgs.html","id":null,"dir":"Reference","previous_headings":"","what":"Quietly load package namespace — load_pkgs","title":"Quietly load package namespace — load_pkgs","text":"one packages, load namespace. used parallel processing since different parallel backends handle package environments differently.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/load_pkgs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Quietly load package namespace — load_pkgs","text":"","code":"load_pkgs(x, ..., infra = TRUE)"},{"path":"https://tune.tidymodels.org/dev/reference/load_pkgs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Quietly load package namespace — load_pkgs","text":"x character vector packages. infra base tidymodels packages loaded well?","code":""},{"path":"https://tune.tidymodels.org/dev/reference/load_pkgs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Quietly load package namespace — load_pkgs","text":"invisible NULL.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/merge.recipe.html","id":null,"dir":"Reference","previous_headings":"","what":"Merge parameter grid values into objects — merge.recipe","title":"Merge parameter grid values into objects — merge.recipe","text":"merge() can used easily update arguments parsnip model recipe.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/merge.recipe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Merge parameter grid values into objects — merge.recipe","text":"","code":"# S3 method for class 'recipe' merge(x, y, ...)  # S3 method for class 'model_spec' merge(x, y, ...)"},{"path":"https://tune.tidymodels.org/dev/reference/merge.recipe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Merge parameter grid values into objects — merge.recipe","text":"x recipe model specification object. y data frame parameter grid resulting one grid_* functions. column names correspond parameter names (annotations) object. ... used required S3 completeness.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/merge.recipe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Merge parameter grid values into objects — merge.recipe","text":"tibble column x many rows y.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/merge.recipe.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Merge parameter grid values into objects — merge.recipe","text":"","code":"library(tibble) library(recipes) library(parsnip) library(dials) #> Loading required package: scales #>  #> Attaching package: ‘scales’ #> The following object is masked from ‘package:purrr’: #>  #>     discard  pca_rec <-   recipe(mpg ~ ., data = mtcars) %>%   step_impute_knn(all_predictors(), neighbors = tune()) %>%   step_pca(all_predictors(), num_comp = tune())  pca_grid <-   tribble(     ~neighbors, ~num_comp,              1,         1,              5,         1,              1,         2,              5,         2   )  merge(pca_rec, pca_grid) #> # A tibble: 4 × 1 #>   x        #>   <list>   #> 1 <recipe> #> 2 <recipe> #> 3 <recipe> #> 4 <recipe>  spline_rec <-   recipe(mpg ~ ., data = mtcars) %>%   step_spline_natural(disp, deg_free = tune(\"disp df\")) %>%   step_spline_natural(wt, deg_free = tune(\"wt df\"))  spline_grid <-   tribble(     ~\"disp df\", ~ \"wt df\",     3,         3,     5,         3,     3,         5,     5,         5   )  merge(pca_rec, pca_grid) #> # A tibble: 4 × 1 #>   x        #>   <list>   #> 1 <recipe> #> 2 <recipe> #> 3 <recipe> #> 4 <recipe>  data(hpc_data, package = \"modeldata\")  xgb_mod <-   boost_tree(trees = tune(), min_n = tune()) %>%   set_engine(\"xgboost\")  set.seed(254) xgb_grid <-   extract_parameter_set_dials(xgb_mod) %>%   finalize(hpc_data) %>%   grid_max_entropy(size = 3) #> Warning: `grid_max_entropy()` was deprecated in dials 1.3.0. #> ℹ Please use `grid_space_filling()` instead.  merge(xgb_mod, xgb_grid) #> # A tibble: 3 × 1 #>   x         #>   <list>    #> 1 <spec[?]> #> 2 <spec[?]> #> 3 <spec[?]>"},{"path":"https://tune.tidymodels.org/dev/reference/message_wrap.html","id":null,"dir":"Reference","previous_headings":"","what":"Write a message that respects the line width — message_wrap","title":"Write a message that respects the line width — message_wrap","text":"Write message respects line width","code":""},{"path":"https://tune.tidymodels.org/dev/reference/message_wrap.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Write a message that respects the line width — message_wrap","text":"","code":"message_wrap(   x,   width = options()$width - 2,   prefix = \"\",   color_text = NULL,   color_prefix = color_text )"},{"path":"https://tune.tidymodels.org/dev/reference/message_wrap.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Write a message that respects the line width — message_wrap","text":"x character string message text. width integer width. prefix optional string go first line message. color_text, color_prefix function (NULL) used color text /prefix.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/message_wrap.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Write a message that respects the line width — message_wrap","text":"processed text returned (invisibly) message written.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/message_wrap.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Write a message that respects the line width — message_wrap","text":"","code":"library(cli) Gaiman <-   paste(     '\"Good point.\" Bod was pleased with himself, and glad he had thought of',     \"asking the poet for advice. Really, he thought, if you couldn't trust a\",     \"poet to offer sensible advice, who could you trust?\",     collapse = \"\"   ) message_wrap(Gaiman) #> \"Good point.\" Bod was pleased with himself, and glad he had thought #> of asking the poet for advice. Really, he thought, if you couldn't #> trust a poet to offer sensible advice, who could you trust? message_wrap(Gaiman, width = 20, prefix = \"-\") #> - \"Good point.\" Bod #>   was pleased with #>   himself, and glad #>   he had thought of #>   asking the poet #>   for advice. #>   Really, he #>   thought, if you #>   couldn't trust a #>   poet to offer #>   sensible advice, #>   who could you #>   trust? message_wrap(Gaiman,   width = 30, prefix = \"-\",   color_text = cli::col_silver ) #> - \"Good point.\" Bod was #>   pleased with himself, and #>   glad he had thought of #>   asking the poet for advice. #>   Really, he thought, if you #>   couldn't trust a poet to #>   offer sensible advice, who #>   could you trust? message_wrap(Gaiman,   width = 30, prefix = \"-\",   color_text = cli::style_underline,   color_prefix = cli::col_green ) #> - \"Good point.\" Bod was #>   pleased with himself, and #>   glad he had thought of #>   asking the poet for advice. #>   Really, he thought, if you #>   couldn't trust a poet to #>   offer sensible advice, who #>   could you trust?"},{"path":"https://tune.tidymodels.org/dev/reference/min_grid.html","id":null,"dir":"Reference","previous_headings":"","what":"Determine the minimum set of model fits — min_grid.model_spec","title":"Determine the minimum set of model fits — min_grid.model_spec","text":"min_grid() determines exactly models fit order evaluate entire set tuning parameter combinations. internal use API may change near future.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/min_grid.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Determine the minimum set of model fits — min_grid.model_spec","text":"","code":"# S3 method for class 'model_spec' min_grid(x, grid, ...)  fit_max_value(x, grid, ...)  # S3 method for class 'boost_tree' min_grid(x, grid, ...)  # S3 method for class 'linear_reg' min_grid(x, grid, ...)  # S3 method for class 'logistic_reg' min_grid(x, grid, ...)  # S3 method for class 'mars' min_grid(x, grid, ...)  # S3 method for class 'multinom_reg' min_grid(x, grid, ...)  # S3 method for class 'nearest_neighbor' min_grid(x, grid, ...)  # S3 method for class 'cubist_rules' min_grid(x, grid, ...)  # S3 method for class 'C5_rules' min_grid(x, grid, ...)  # S3 method for class 'rule_fit' min_grid(x, grid, ...)  # S3 method for class 'pls' min_grid(x, grid, ...)  # S3 method for class 'poisson_reg' min_grid(x, grid, ...)"},{"path":"https://tune.tidymodels.org/dev/reference/min_grid.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Determine the minimum set of model fits — min_grid.model_spec","text":"x model specification. grid tibble tuning parameter combinations. ... currently used.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/min_grid.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Determine the minimum set of model fits — min_grid.model_spec","text":"tibble minimum tuning parameters fit additional list column parameter combinations used prediction.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/min_grid.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Determine the minimum set of model fits — min_grid.model_spec","text":"fit_max_value() can used packages implement min_grid() method.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/min_grid.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Determine the minimum set of model fits — min_grid.model_spec","text":"","code":"library(dplyr) library(dials) library(parsnip)  ## ----------------------------------------------------------------------------- ## No ability to exploit submodels:  svm_spec <-   svm_poly(cost = tune(), degree = tune()) %>%   set_engine(\"kernlab\") %>%   set_mode(\"regression\")  svm_grid <-   svm_spec %>%   extract_parameter_set_dials() %>%   grid_regular(levels = 3)  min_grid(svm_spec, svm_grid) #> # A tibble: 9 × 3 #>        cost degree .submodels #>       <dbl>  <int> <list>     #> 1  0.000977      1 <list [0]> #> 2  0.177         1 <list [0]> #> 3 32             1 <list [0]> #> 4  0.000977      2 <list [0]> #> 5  0.177         2 <list [0]> #> 6 32             2 <list [0]> #> 7  0.000977      3 <list [0]> #> 8  0.177         3 <list [0]> #> 9 32             3 <list [0]>  ## ----------------------------------------------------------------------------- ## Can use submodels  xgb_spec <-   boost_tree(trees = tune(), min_n = tune()) %>%   set_engine(\"xgboost\") %>%   set_mode(\"regression\")  xgb_grid <-   xgb_spec %>%   extract_parameter_set_dials() %>%   grid_regular(levels = 3)  min_grid(xgb_spec, xgb_grid) #> # A tibble: 3 × 3 #>   trees min_n .submodels       #>   <int> <int> <list>           #> 1  2000     2 <named list [1]> #> 2  2000    21 <named list [1]> #> 3  2000    40 <named list [1]>"},{"path":"https://tune.tidymodels.org/dev/reference/outcome_names.html","id":null,"dir":"Reference","previous_headings":"","what":"Determine names of the outcome data in a workflow — outcome_names","title":"Determine names of the outcome data in a workflow — outcome_names","text":"Determine names outcome data workflow","code":""},{"path":"https://tune.tidymodels.org/dev/reference/outcome_names.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Determine names of the outcome data in a workflow — outcome_names","text":"","code":"outcome_names(x, ...)  # S3 method for class 'terms' outcome_names(x, ...)  # S3 method for class 'formula' outcome_names(x, ...)  # S3 method for class 'recipe' outcome_names(x, ...)  # S3 method for class 'workflow' outcome_names(x, ...)  # S3 method for class 'tune_results' outcome_names(x, ...)"},{"path":"https://tune.tidymodels.org/dev/reference/outcome_names.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Determine names of the outcome data in a workflow — outcome_names","text":"x object. ... used.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/outcome_names.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Determine names of the outcome data in a workflow — outcome_names","text":"character string variable names","code":""},{"path":"https://tune.tidymodels.org/dev/reference/outcome_names.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Determine names of the outcome data in a workflow — outcome_names","text":"","code":"library(dplyr) lm(cbind(mpg, wt) ~ ., data = mtcars) %>%   purrr::pluck(terms) %>%   outcome_names() #> [1] \"mpg\" \"wt\""},{"path":"https://tune.tidymodels.org/dev/reference/parallelism.html","id":null,"dir":"Reference","previous_headings":"","what":"Support for parallel processing in tune — parallelism","title":"Support for parallel processing in tune — parallelism","text":"Support parallel backends registered foreach package deprecated tune 1.2.1 favor future package. package now raise warning : parallel backend registered foreach, plan specified future. parallelism configured framework, tune use plan specified future warn. transition code foreach future, remove code registers foreach Backend:   replace :   See future::plan() possible options multisession.","code":"library(doBackend) registerDoBackend(cores = 4) library(future) plan(multisession, workers = 4)"},{"path":"https://tune.tidymodels.org/dev/reference/parameters.workflow.html","id":null,"dir":"Reference","previous_headings":"","what":"Determination of parameter sets for other objects — parameters.workflow","title":"Determination of parameter sets for other objects — parameters.workflow","text":"methods deprecated favor extract_parameter_set_dials().","code":""},{"path":"https://tune.tidymodels.org/dev/reference/parameters.workflow.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Determination of parameter sets for other objects — parameters.workflow","text":"","code":"# S3 method for class 'workflow' parameters(x, ...)  # S3 method for class 'model_spec' parameters(x, ...)  # S3 method for class 'recipe' parameters(x, ...)"},{"path":"https://tune.tidymodels.org/dev/reference/parameters.workflow.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Determination of parameter sets for other objects — parameters.workflow","text":"x object ... currently used.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/parameters.workflow.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Determination of parameter sets for other objects — parameters.workflow","text":"parameter set object","code":""},{"path":"https://tune.tidymodels.org/dev/reference/prob_improve.html","id":null,"dir":"Reference","previous_headings":"","what":"Acquisition function for scoring parameter combinations — prob_improve","title":"Acquisition function for scoring parameter combinations — prob_improve","text":"functions can used score candidate tuning parameter combinations function predicted mean variation.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/prob_improve.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Acquisition function for scoring parameter combinations — prob_improve","text":"","code":"prob_improve(trade_off = 0, eps = .Machine$double.eps)  exp_improve(trade_off = 0, eps = .Machine$double.eps)  conf_bound(kappa = 0.1)"},{"path":"https://tune.tidymodels.org/dev/reference/prob_improve.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Acquisition function for scoring parameter combinations — prob_improve","text":"trade_off number function describes trade-exploitation exploration. Smaller values favor exploitation. eps small constant avoid division zero. kappa positive number (function) corresponds multiplier standard deviation confidence bound (e.g. 1.96 normal-theory 95 percent confidence intervals). Smaller values lean towards exploitation.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/prob_improve.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Acquisition function for scoring parameter combinations — prob_improve","text":"object class prob_improve, exp_improve, conf_bounds along extra class acquisition_function.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/prob_improve.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Acquisition function for scoring parameter combinations — prob_improve","text":"acquisition functions often combine mean variance predictions Gaussian process model objective optimized. documentation, assume metric question better maximized (e.g. accuracy, coefficient determination, etc). expected improvement point x based predicted mean variation point well current best value (denoted x_b). vignette linked contains formulas acquisition function. trade_off parameter greater zero, acquisition function -play effect mean prediction give weight variation. effect searching new parameter combinations areas yet sampled. Note exp_improve() prob_improve(), trade_off value units outcome. functions parameterized trade_off value always non-negative. confidence bound function take account current best results data. function passed  exp_improve() prob_improve(), function can multiple arguments first (current iteration number) given function. words, function argument defaults first argument. See expo_decay() example function.","code":""},{"path":[]},{"path":"https://tune.tidymodels.org/dev/reference/prob_improve.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Acquisition function for scoring parameter combinations — prob_improve","text":"","code":"prob_improve() #> Acquisition Function: probability of improvment"},{"path":"https://tune.tidymodels.org/dev/reference/reexports.html","id":null,"dir":"Reference","previous_headings":"","what":"Objects exported from other packages — reexports","title":"Objects exported from other packages — reexports","text":"objects imported packages. Follow links see documentation. dials parameters dplyr dplyr_reconstruct generics augment, min_grid, required_pkgs, tunable, tune_args ggplot2 autoplot hardhat extract_fit_engine, extract_fit_parsnip, extract_mold, extract_parameter_set_dials, extract_preprocessor, extract_recipe, extract_spec_parsnip, extract_workflow, tune rsample .get_fingerprint, int_pctl","code":""},{"path":"https://tune.tidymodels.org/dev/reference/show_best.html","id":null,"dir":"Reference","previous_headings":"","what":"Investigate best tuning parameters — show_best","title":"Investigate best tuning parameters — show_best","text":"show_best() displays top sub-models performance estimates. select_best() finds tuning parameter combination best performance values. select_by_one_std_err() uses \"one-standard error rule\" (Breiman _el , 1984) selects simple model within one standard error numerically optimal results. select_by_pct_loss() selects simple model whose loss performance within acceptable limit.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/show_best.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Investigate best tuning parameters — show_best","text":"","code":"show_best(x, ...)  # Default S3 method show_best(x, ...)  # S3 method for class 'tune_results' show_best(   x,   ...,   metric = NULL,   eval_time = NULL,   n = 5,   call = rlang::current_env() )  select_best(x, ...)  # Default S3 method select_best(x, ...)  # S3 method for class 'tune_results' select_best(x, ..., metric = NULL, eval_time = NULL)  select_by_pct_loss(x, ...)  # Default S3 method select_by_pct_loss(x, ...)  # S3 method for class 'tune_results' select_by_pct_loss(x, ..., metric = NULL, eval_time = NULL, limit = 2)  select_by_one_std_err(x, ...)  # Default S3 method select_by_one_std_err(x, ...)  # S3 method for class 'tune_results' select_by_one_std_err(x, ..., metric = NULL, eval_time = NULL)"},{"path":"https://tune.tidymodels.org/dev/reference/show_best.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Investigate best tuning parameters — show_best","text":"x results tune_grid() tune_bayes(). ... select_by_one_std_err() select_by_pct_loss(), argument passed directly dplyr::arrange() user can sort models simple complex. , parameter p, pass unquoted expression p smaller values p indicate simpler model, desc(p) larger values indicate simpler model. least one term required two functions. See examples . metric character value metric used sort models. (See https://yardstick.tidymodels.org/articles/metric-types.html details). required single metric exists x. multiple metric none given, first metric set used (warning issued). eval_time single numeric time point dynamic event time metrics chosen (e.g., time-dependent ROC curve, etc). values consistent values used create x. NULL default automatically use first evaluation time used x. n integer number top results/rows return. call call shown errors warnings. limit limit loss performance acceptable (percent units). See details .","code":""},{"path":"https://tune.tidymodels.org/dev/reference/show_best.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Investigate best tuning parameters — show_best","text":"tibble columns parameters. show_best() also includes columns performance metrics.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/show_best.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Investigate best tuning parameters — show_best","text":"percent loss, suppose best model RMSE 0.75 simpler model RMSE 1. percent loss (1.00 - 0.75)/1.00 * 100, 25 percent. Note loss always non-negative.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/show_best.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Investigate best tuning parameters — show_best","text":"Breiman, Leo; Friedman, J. H.; Olshen, R. .; Stone, C. J. (1984). Classification Regression Trees. Monterey, CA: Wadsworth.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/show_best.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Investigate best tuning parameters — show_best","text":"","code":"data(\"example_ames_knn\")  show_best(ames_iter_search, metric = \"rmse\") #> # A tibble: 5 × 12 #>       K weight_func dist_power   lon   lat .metric .estimator   mean #>   <int> <chr>            <dbl> <int> <int> <chr>   <chr>       <dbl> #> 1    33 triweight        0.511    10     3 rmse    standard   0.0728 #> 2     5 rank             0.411     2     7 rmse    standard   0.0740 #> 3    21 triweight        0.909    10     4 rmse    standard   0.0742 #> 4    21 cos              0.626     1     4 rmse    standard   0.0746 #> 5    19 inv              0.117     1     4 rmse    standard   0.0758 #> # ℹ 4 more variables: n <int>, std_err <dbl>, .config <chr>, #> #   .iter <int>  select_best(ames_iter_search, metric = \"rsq\") #> # A tibble: 1 × 6 #>       K weight_func dist_power   lon   lat .config               #>   <int> <chr>            <dbl> <int> <int> <chr>                 #> 1    33 triweight        0.511    10     3 Preprocessor10_Model1  # To find the least complex model within one std error of the numerically # optimal model, the number of nearest neighbors are sorted from the largest # number of neighbors (the least complex class boundary) to the smallest # (corresponding to the most complex model).  select_by_one_std_err(ames_grid_search, metric = \"rmse\", desc(K)) #> # A tibble: 1 × 6 #>       K weight_func dist_power   lon   lat .config               #>   <int> <chr>            <dbl> <int> <int> <chr>                 #> 1    33 triweight        0.511    10     3 Preprocessor10_Model1  # Now find the least complex model that has no more than a 5% loss of RMSE: select_by_pct_loss(   ames_grid_search,   metric = \"rmse\",   limit = 5, desc(K) ) #> # A tibble: 1 × 6 #>       K weight_func dist_power   lon   lat .config               #>   <int> <chr>            <dbl> <int> <int> <chr>                 #> 1    33 triweight        0.511    10     3 Preprocessor10_Model1"},{"path":"https://tune.tidymodels.org/dev/reference/show_notes.html","id":null,"dir":"Reference","previous_headings":"","what":"Display distinct errors from tune objects — show_notes","title":"Display distinct errors from tune objects — show_notes","text":"Display distinct errors tune objects","code":""},{"path":"https://tune.tidymodels.org/dev/reference/show_notes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Display distinct errors from tune objects — show_notes","text":"","code":"show_notes(x, n = 10)"},{"path":"https://tune.tidymodels.org/dev/reference/show_notes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Display distinct errors from tune objects — show_notes","text":"x object class tune_results. n integer many unique notes show.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/show_notes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Display distinct errors from tune objects — show_notes","text":"Invisibly, x. Function called side-effects printing.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/tune-internal-functions.html","id":null,"dir":"Reference","previous_headings":"","what":"Internal functions used by other tidymodels packages — forge_from_workflow","title":"Internal functions used by other tidymodels packages — forge_from_workflow","text":"meant invoked directly users.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/tune-internal-functions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Internal functions used by other tidymodels packages — forge_from_workflow","text":"","code":"forge_from_workflow(new_data, workflow)  finalize_workflow_preprocessor(workflow, grid_preprocessor)  .estimate_metrics(   dat,   metric,   param_names,   outcome_name,   event_level,   metrics_info = metrics_info(metrics) )  .load_namespace(x)  initialize_catalog(control, env = rlang::caller_env())  .catch_and_log(.expr, ..., bad_only = FALSE, notes, catalog = TRUE)  .catch_and_log_fit(.expr, ..., notes)"},{"path":"https://tune.tidymodels.org/dev/reference/tune-internal-functions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Internal functions used by other tidymodels packages — forge_from_workflow","text":"new_data data frame matrix predictors process. workflow workflow. grid_preprocessor tibble parameter information. dat data set. metric metric set. param_names character vector tuning parameter names. outcome_name character string column dat outcome. event_level logical passed control function. metrics_info output tune:::metrics_info(metrics)—included argument allow pre-computing. x character vector package names. .expr Code execute. ... Object pass internal tune_log() function. bad_only logical whether warnings errors caught. notes Character data add logging. catalog logical passed tune_log() giving whether message compatible issue cataloger. Defaults TRUE. Updates always unique represent tuning \"issue\" can bypass cataloger setting catalog = FALSE.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/tune_accessor.html","id":null,"dir":"Reference","previous_headings":"","what":"Various accessor functions — .get_tune_parameters","title":"Various accessor functions — .get_tune_parameters","text":"functions return different attributes objects class tune_result.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/tune_accessor.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Various accessor functions — .get_tune_parameters","text":"","code":".get_tune_parameters(x)  .get_tune_parameter_names(x)  .get_extra_col_names(x)  .get_tune_metrics(x)  .get_tune_metric_names(x)  .get_tune_eval_times(x)  .get_tune_eval_time_target(x)  .get_tune_outcome_names(x)  .get_tune_workflow(x)  # S3 method for class 'tune_results' .get_fingerprint(x, ...)"},{"path":"https://tune.tidymodels.org/dev/reference/tune_accessor.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Various accessor functions — .get_tune_parameters","text":"x object class tune_result.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/tune_accessor.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Various accessor functions — .get_tune_parameters","text":".get_tune_parameters() returns dials parameter object tibble. .get_tune_parameter_names(), .get_tune_metric_names(), .get_tune_outcome_names() return character string. .get_tune_metrics() returns metric set NULL. .get_tune_workflow() returns workflow used fit resamples (save_workflow set TRUE fitting) NULL.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/tune_bayes.html","id":null,"dir":"Reference","previous_headings":"","what":"Bayesian optimization of model parameters. — tune_bayes","title":"Bayesian optimization of model parameters. — tune_bayes","text":"tune_bayes() uses models generate new candidate tuning parameter combinations based previous results.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/tune_bayes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bayesian optimization of model parameters. — tune_bayes","text":"","code":"tune_bayes(object, ...)  # S3 method for class 'model_spec' tune_bayes(   object,   preprocessor,   resamples,   ...,   iter = 10,   param_info = NULL,   metrics = NULL,   eval_time = NULL,   objective = exp_improve(),   initial = 5,   control = control_bayes() )  # S3 method for class 'workflow' tune_bayes(   object,   resamples,   ...,   iter = 10,   param_info = NULL,   metrics = NULL,   eval_time = NULL,   objective = exp_improve(),   initial = 5,   control = control_bayes() )"},{"path":"https://tune.tidymodels.org/dev/reference/tune_bayes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bayesian optimization of model parameters. — tune_bayes","text":"object parsnip model specification unfitted workflow(). tuning parameters allowed; arguments marked tune(), values must finalized. ... Options pass GPfit::GP_fit() (mostly corr argument). preprocessor traditional model formula recipe created using recipes::recipe(). resamples rset resampling object created rsample function, rsample::vfold_cv(). iter maximum number search iterations. param_info dials::parameters() object NULL. none given, parameters set derived arguments. Passing argument can useful parameter ranges need customized. metrics yardstick::metric_set(), NULL compute standard set metrics. first metric metrics one optimized. eval_time numeric vector time points dynamic event time metrics computed (e.g. time-dependent ROC curve, etc). values must non-negative probably greater largest event time training set (See Details ). objective character string metric optimized acquisition function object. initial initial set results tidy format (result tune_grid()) positive integer. suggested number initial results greater number parameters optimized. control control object created control_bayes().","code":""},{"path":"https://tune.tidymodels.org/dev/reference/tune_bayes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bayesian optimization of model parameters. — tune_bayes","text":"tibble results mirror generated tune_grid(). However, results contain .iter column replicate rset object multiple times iterations (limited additional memory costs).","code":""},{"path":"https://tune.tidymodels.org/dev/reference/tune_bayes.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Bayesian optimization of model parameters. — tune_bayes","text":"optimization starts set initial results, generated tune_grid(). none exist, function create several combinations obtain performance estimates. Using one performance estimates model outcome, Gaussian process (GP) model created previous tuning parameter combinations used predictors. large grid potential hyperparameter combinations predicted using model scored using acquisition function. functions usually combine predicted mean variance GP decide best parameter combination try next. information, see documentation exp_improve() corresponding package vignette. best combination evaluated using resampling process continues.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/tune_bayes.html","id":"parallel-processing","dir":"Reference","previous_headings":"","what":"Parallel Processing","title":"Bayesian optimization of model parameters. — tune_bayes","text":"tune supports parallel processing future package. execute resampling iterations parallel, specify plan future first. allow_par argument can used avoid parallelism. part, warnings generated training shown occur associated specific resample control_bayes(verbose = TRUE). (usually) aggregated end processing. Bayesian optimization, parallel processing used estimate resampled performance values new candidate set values estimated.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/tune_bayes.html","id":"initial-values","dir":"Reference","previous_headings":"","what":"Initial Values","title":"Bayesian optimization of model parameters. — tune_bayes","text":"results tune_grid(), previous run tune_bayes() can used initial argument. initial can also positive integer. case, space-filling design used populate preliminary set results. good results, number initial values number parameters optimized.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/tune_bayes.html","id":"parameter-ranges-and-values","dir":"Reference","previous_headings":"","what":"Parameter Ranges and Values","title":"Bayesian optimization of model parameters. — tune_bayes","text":"cases, tuning parameter values depend dimensions data (said contain unknown values). example, mtry random forest models depends number predictors. cases, unknowns tuning parameter object must determined beforehand passed function via param_info argument. dials::finalize() can used derive data-dependent parameters. Otherwise, parameter set can created via dials::parameters(), dials update() function can used specify ranges values.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/tune_bayes.html","id":"performance-metrics","dir":"Reference","previous_headings":"","what":"Performance Metrics","title":"Bayesian optimization of model parameters. — tune_bayes","text":"use performance metrics, yardstick::metric_set() function can used pick measured model. multiple metrics desired, can bundled. example, estimate area ROC curve well sensitivity specificity (typical probability cutoff 0.50), metrics argument given: metric calculated candidate model. metric set provided, one created: regression models, root mean squared error coefficient determination computed. classification, area ROC curve overall accuracy computed. Note metrics also determine type predictions estimated tuning. example, classification problem, metrics used associated hard class predictions, classification probabilities created. --sample estimates metrics contained list column called .metrics. tibble contains row metric columns value, estimator type, . collect_metrics() can used objects collapse results resampled (obtain final resampling estimates per tuning parameter combination).","code":"metrics = metric_set(roc_auc, sens, spec)"},{"path":"https://tune.tidymodels.org/dev/reference/tune_bayes.html","id":"obtaining-predictions","dir":"Reference","previous_headings":"","what":"Obtaining Predictions","title":"Bayesian optimization of model parameters. — tune_bayes","text":"control_bayes(save_pred = TRUE), output tibble contains list column called .predictions --sample predictions parameter combination grid fold (can large). elements tibble tibbles columns tuning parameters, row number original data object (.row), outcome data (name(s) original data), columns created predictions. example, simple regression problems, function generates column called .pred . noted , prediction columns returned determined type metric(s) requested. list column can unnested using tidyr::unnest() using convenience function collect_predictions().","code":""},{"path":"https://tune.tidymodels.org/dev/reference/tune_bayes.html","id":"case-weights","dir":"Reference","previous_headings":"","what":"Case Weights","title":"Bayesian optimization of model parameters. — tune_bayes","text":"models can utilize case weights training. tidymodels currently supports two types case weights: importance weights (doubles) frequency weights (integers). Frequency weights used model fitting evaluation, whereas importance weights used fitting. know model capable using case weights, create model spec test using parsnip::case_weights_allowed(). use , need numeric column data set passed either hardhat:: importance_weights() hardhat::frequency_weights(). functions fit_resamples() tune_*() functions, model must contained inside workflows::workflow(). declare case weights used, invoke workflows::add_case_weights() corresponding (unquoted) column name. , packages appropriately handle weights model fitting (appropriate) performance estimation.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/tune_bayes.html","id":"censored-regression-models","dir":"Reference","previous_headings":"","what":"Censored Regression Models","title":"Bayesian optimization of model parameters. — tune_bayes","text":"Three types metrics can used assess quality censored regression models: static: prediction independent time. dynamic: prediction time-specific probability (e.g., survival probability) measured one particular times. integrated: dynamic metric returns integral different metrics time point. metrics chosen user affects many evaluation times specified. example:   Values eval_time less largest observed event time training data. many non-parametric models, results beyond largest time corresponding event constant (NA).","code":"# Needs no `eval_time` value metric_set(concordance_survival)  # Needs at least one `eval_time` metric_set(brier_survival) metric_set(brier_survival, concordance_survival)  # Needs at least two eval_time` values metric_set(brier_survival_integrated, concordance_survival) metric_set(brier_survival_integrated, concordance_survival) metric_set(brier_survival_integrated, concordance_survival, brier_survival)"},{"path":"https://tune.tidymodels.org/dev/reference/tune_bayes.html","id":"optimizing-censored-regression-models","dir":"Reference","previous_headings":"","what":"Optimizing Censored Regression Models","title":"Bayesian optimization of model parameters. — tune_bayes","text":"dynamic performance metrics (e.g. Brier ROC curves), performance calculated every value eval_time first evaluation time given user (e.g., eval_time[1]) used guide optimization.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/tune_bayes.html","id":"extracting-information","dir":"Reference","previous_headings":"","what":"Extracting Information","title":"Bayesian optimization of model parameters. — tune_bayes","text":"extract control option result additional function returned called .extracts. list column tibbles containing results user's function tuning parameter combination. can enable returning model /recipe object created resampling. Note result large return object, depending returned. control function contains option (extract) can used retain model recipe created within resamples. argument function single argument. value argument given function resample workflow object (see workflows::workflow() information). Several helper functions can used easily pull preprocessing /model information workflow, extract_preprocessor() extract_fit_parsnip(). example, interest getting parsnip model fit back, one use: Note function given extract argument evaluated every model fit (opposed every model evaluated). noted , cases, model predictions can derived sub-models , cases, every row tuning parameter grid separate R object associated .","code":"extract = function (x) extract_fit_parsnip(x)"},{"path":[]},{"path":"https://tune.tidymodels.org/dev/reference/tune_bayes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bayesian optimization of model parameters. — tune_bayes","text":"","code":"library(recipes) library(rsample) library(parsnip)  # define resamples and minimal recipe on mtcars set.seed(6735) folds <- vfold_cv(mtcars, v = 5)  car_rec <-   recipe(mpg ~ ., data = mtcars) %>%   step_normalize(all_predictors())  # define an svm with parameters to tune svm_mod <-   svm_rbf(cost = tune(), rbf_sigma = tune()) %>%   set_engine(\"kernlab\") %>%   set_mode(\"regression\")  # use a space-filling design with 6 points set.seed(3254) svm_grid <- tune_grid(svm_mod, car_rec, folds, grid = 6)  show_best(svm_grid, metric = \"rmse\") #> # A tibble: 5 × 8 #>        cost  rbf_sigma .metric .estimator  mean     n std_err .config   #>       <dbl>      <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>     #> 1 32        0.01       rmse    standard    2.67     5   0.216 Preproce… #> 2  0.5      0.0001     rmse    standard    5.90     5   0.960 Preproce… #> 3  0.00781  1          rmse    standard    5.92     5   0.964 Preproce… #> 4  4        0.00000001 rmse    standard    5.96     5   0.970 Preproce… #> 5  0.000977 0.000001   rmse    standard    5.96     5   0.970 Preproce…  # use bayesian optimization to evaluate at 6 more points set.seed(8241) svm_bayes <- tune_bayes(svm_mod, car_rec, folds, initial = svm_grid, iter = 6)  # note that bayesian optimization evaluated parameterizations # similar to those that previously decreased rmse in svm_grid show_best(svm_bayes, metric = \"rmse\") #> # A tibble: 5 × 9 #>      cost rbf_sigma .metric .estimator  mean     n std_err .config      #>     <dbl>     <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>        #> 1 15.8      0.00998 rmse    standard    2.66     5   0.180 Iter3        #> 2 32        0.01    rmse    standard    2.67     5   0.216 Preprocesso… #> 3 20.8      0.0150  rmse    standard    2.70     5   0.233 Iter6        #> 4 25.6      0.117   rmse    standard    3.21     5   0.793 Iter1        #> 5  0.0404   0.00843 rmse    standard    5.63     5   0.920 Iter5        #> # ℹ 1 more variable: .iter <int>  # specifying `initial` as a numeric rather than previous tuning results # will result in `tune_bayes` initially evaluating an space-filling # grid using `tune_grid` with `grid = initial` set.seed(0239) svm_init <- tune_bayes(svm_mod, car_rec, folds, initial = 6, iter = 6)  show_best(svm_init, metric = \"rmse\") #> # A tibble: 5 × 9 #>    cost rbf_sigma .metric .estimator  mean     n std_err .config  .iter #>   <dbl>     <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>    <int> #> 1  31.2   0.00217 rmse    standard    2.59     5   0.221 Iter4        4 #> 2  18.4   0.00247 rmse    standard    2.59     5   0.225 Iter5        5 #> 3  24.4   0.00288 rmse    standard    2.59     5   0.215 Iter6        6 #> 4  31.5   0.00144 rmse    standard    2.60     5   0.233 Iter3        3 #> 5  32     0.01    rmse    standard    2.67     5   0.216 Preproc…     0"},{"path":"https://tune.tidymodels.org/dev/reference/tune_grid.html","id":null,"dir":"Reference","previous_headings":"","what":"Model tuning via grid search — tune_grid","title":"Model tuning via grid search — tune_grid","text":"tune_grid() computes set performance metrics (e.g. accuracy RMSE) pre-defined set tuning parameters correspond model recipe across one resamples data.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/tune_grid.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Model tuning via grid search — tune_grid","text":"","code":"tune_grid(object, ...)  # S3 method for class 'model_spec' tune_grid(   object,   preprocessor,   resamples,   ...,   param_info = NULL,   grid = 10,   metrics = NULL,   eval_time = NULL,   control = control_grid() )  # S3 method for class 'workflow' tune_grid(   object,   resamples,   ...,   param_info = NULL,   grid = 10,   metrics = NULL,   eval_time = NULL,   control = control_grid() )"},{"path":"https://tune.tidymodels.org/dev/reference/tune_grid.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Model tuning via grid search — tune_grid","text":"object parsnip model specification unfitted workflow(). tuning parameters allowed; arguments marked tune(), values must finalized. ... currently used. preprocessor traditional model formula recipe created using recipes::recipe(). resamples rset resampling object created rsample function, rsample::vfold_cv(). param_info dials::parameters() object NULL. none given, parameters set derived arguments. Passing argument can useful parameter ranges need customized. grid data frame tuning combinations positive integer. data frame columns parameter tuned rows tuning parameter candidates. integer denotes number candidate parameter sets created automatically. metrics yardstick::metric_set(), NULL compute standard set metrics. eval_time numeric vector time points dynamic event time metrics computed (e.g. time-dependent ROC curve, etc). values must non-negative probably greater largest event time training set (See Details ). control object used modify tuning process, likely created control_grid().","code":""},{"path":"https://tune.tidymodels.org/dev/reference/tune_grid.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Model tuning via grid search — tune_grid","text":"updated version resamples extra list columns .metrics .notes (optional columns .predictions .extracts). .notes contains warnings errors occur execution.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/tune_grid.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Model tuning via grid search — tune_grid","text":"Suppose m tuning parameter combinations. tune_grid() may require m model/recipe fits across resample. example: cases single model fit can used make predictions different parameter values grid, one fit used. example, boosted trees, 100 iterations boosting requested, model object 100 iterations can used make predictions iterations less 100 (parameters equal). model tuned conjunction pre-processing /post-processing parameters, minimum number fits used. example, number PCA components recipe step tuned three values (along model tuning parameters), three recipes trained. alternative re-train recipe multiple times model tuning parameter. tune supports parallel processing future package. execute resampling iterations parallel, specify plan future first. allow_par argument can used avoid parallelism. part, warnings generated training shown occur associated specific resample control_grid(verbose = TRUE). (usually) aggregated end processing.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/tune_grid.html","id":"parameter-grids","dir":"Reference","previous_headings":"","what":"Parameter Grids","title":"Model tuning via grid search — tune_grid","text":"tuning grid provided, grid (via dials::grid_space_filling()) created 10 candidate parameter combinations. provided, grid column names parameter named parameter name id. example, parameter marked optimization using penalty = tune(), column named penalty. optional identifier used, penalty = tune(id = 'lambda'), corresponding column name lambda. cases, tuning parameter values depend dimensions data. example, mtry random forest models depends number predictors. case, default tuning parameter object requires upper range. dials::finalize() can used derive data-dependent parameters. Otherwise, parameter set can created (via dials::parameters()) dials update() function can used change values. updated parameter set can passed function via param_info argument.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/tune_grid.html","id":"performance-metrics","dir":"Reference","previous_headings":"","what":"Performance Metrics","title":"Model tuning via grid search — tune_grid","text":"use performance metrics, yardstick::metric_set() function can used pick measured model. multiple metrics desired, can bundled. example, estimate area ROC curve well sensitivity specificity (typical probability cutoff 0.50), metrics argument given: metric calculated candidate model. metric set provided, one created: regression models, root mean squared error coefficient determination computed. classification, area ROC curve overall accuracy computed. Note metrics also determine type predictions estimated tuning. example, classification problem, metrics used associated hard class predictions, classification probabilities created. --sample estimates metrics contained list column called .metrics. tibble contains row metric columns value, estimator type, . collect_metrics() can used objects collapse results resampled (obtain final resampling estimates per tuning parameter combination).","code":"metrics = metric_set(roc_auc, sens, spec)"},{"path":"https://tune.tidymodels.org/dev/reference/tune_grid.html","id":"obtaining-predictions","dir":"Reference","previous_headings":"","what":"Obtaining Predictions","title":"Model tuning via grid search — tune_grid","text":"control_grid(save_pred = TRUE), output tibble contains list column called .predictions --sample predictions parameter combination grid fold (can large). elements tibble tibbles columns tuning parameters, row number original data object (.row), outcome data (name(s) original data), columns created predictions. example, simple regression problems, function generates column called .pred . noted , prediction columns returned determined type metric(s) requested. list column can unnested using tidyr::unnest() using convenience function collect_predictions().","code":""},{"path":"https://tune.tidymodels.org/dev/reference/tune_grid.html","id":"extracting-information","dir":"Reference","previous_headings":"","what":"Extracting Information","title":"Model tuning via grid search — tune_grid","text":"extract control option result additional function returned called .extracts. list column tibbles containing results user's function tuning parameter combination. can enable returning model /recipe object created resampling. Note result large return object, depending returned. control function contains option (extract) can used retain model recipe created within resamples. argument function single argument. value argument given function resample workflow object (see workflows::workflow() information). Several helper functions can used easily pull preprocessing /model information workflow, extract_preprocessor() extract_fit_parsnip(). example, interest getting parsnip model fit back, one use: Note function given extract argument evaluated every model fit (opposed every model evaluated). noted , cases, model predictions can derived sub-models , cases, every row tuning parameter grid separate R object associated .","code":"extract = function (x) extract_fit_parsnip(x)"},{"path":"https://tune.tidymodels.org/dev/reference/tune_grid.html","id":"case-weights","dir":"Reference","previous_headings":"","what":"Case Weights","title":"Model tuning via grid search — tune_grid","text":"models can utilize case weights training. tidymodels currently supports two types case weights: importance weights (doubles) frequency weights (integers). Frequency weights used model fitting evaluation, whereas importance weights used fitting. know model capable using case weights, create model spec test using parsnip::case_weights_allowed(). use , need numeric column data set passed either hardhat:: importance_weights() hardhat::frequency_weights(). functions fit_resamples() tune_*() functions, model must contained inside workflows::workflow(). declare case weights used, invoke workflows::add_case_weights() corresponding (unquoted) column name. , packages appropriately handle weights model fitting (appropriate) performance estimation.","code":""},{"path":"https://tune.tidymodels.org/dev/reference/tune_grid.html","id":"censored-regression-models","dir":"Reference","previous_headings":"","what":"Censored Regression Models","title":"Model tuning via grid search — tune_grid","text":"Three types metrics can used assess quality censored regression models: static: prediction independent time. dynamic: prediction time-specific probability (e.g., survival probability) measured one particular times. integrated: dynamic metric returns integral different metrics time point. metrics chosen user affects many evaluation times specified. example:   Values eval_time less largest observed event time training data. many non-parametric models, results beyond largest time corresponding event constant (NA).","code":"# Needs no `eval_time` value metric_set(concordance_survival)  # Needs at least one `eval_time` metric_set(brier_survival) metric_set(brier_survival, concordance_survival)  # Needs at least two eval_time` values metric_set(brier_survival_integrated, concordance_survival) metric_set(brier_survival_integrated, concordance_survival) metric_set(brier_survival_integrated, concordance_survival, brier_survival)"},{"path":[]},{"path":"https://tune.tidymodels.org/dev/reference/tune_grid.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Model tuning via grid search — tune_grid","text":"","code":"library(recipes) library(rsample) library(parsnip) library(workflows) library(ggplot2)  # ---------------------------------------------------------------------------  set.seed(6735) folds <- vfold_cv(mtcars, v = 5)  # ---------------------------------------------------------------------------  # tuning recipe parameters:  spline_rec <-   recipe(mpg ~ ., data = mtcars) %>%   step_spline_natural(disp, deg_free = tune(\"disp\")) %>%   step_spline_natural(wt, deg_free = tune(\"wt\"))  lin_mod <-   linear_reg() %>%   set_engine(\"lm\")  # manually create a grid spline_grid <- expand.grid(disp = 2:5, wt = 2:5)  # Warnings will occur from making spline terms on the holdout data that are # extrapolations. spline_res <-   tune_grid(lin_mod, spline_rec, resamples = folds, grid = spline_grid) spline_res #> # Tuning results #> # 5-fold cross-validation  #> # A tibble: 5 × 4 #>   splits         id    .metrics          .notes           #>   <list>         <chr> <list>            <list>           #> 1 <split [25/7]> Fold1 <tibble [32 × 6]> <tibble [0 × 4]> #> 2 <split [25/7]> Fold2 <tibble [32 × 6]> <tibble [0 × 4]> #> 3 <split [26/6]> Fold3 <tibble [32 × 6]> <tibble [0 × 4]> #> 4 <split [26/6]> Fold4 <tibble [32 × 6]> <tibble [0 × 4]> #> 5 <split [26/6]> Fold5 <tibble [32 × 6]> <tibble [0 × 4]>   show_best(spline_res, metric = \"rmse\") #> # A tibble: 5 × 8 #>    disp    wt .metric .estimator  mean     n std_err .config            #>   <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>              #> 1     3     2 rmse    standard    2.57     5   0.261 Preprocessor05_Mo… #> 2     3     3 rmse    standard    2.64     5   0.234 Preprocessor06_Mo… #> 3     2     3 rmse    standard    2.74     5   0.262 Preprocessor02_Mo… #> 4     4     3 rmse    standard    2.82     5   0.456 Preprocessor10_Mo… #> 5     2     2 rmse    standard    2.87     5   0.321 Preprocessor01_Mo…  # ---------------------------------------------------------------------------  # tune model parameters only (example requires the `kernlab` package)  car_rec <-   recipe(mpg ~ ., data = mtcars) %>%   step_normalize(all_predictors())  svm_mod <-   svm_rbf(cost = tune(), rbf_sigma = tune()) %>%   set_engine(\"kernlab\") %>%   set_mode(\"regression\")  # Use a space-filling design with 7 points set.seed(3254) svm_res <- tune_grid(svm_mod, car_rec, resamples = folds, grid = 7) svm_res #> # Tuning results #> # 5-fold cross-validation  #> # A tibble: 5 × 4 #>   splits         id    .metrics          .notes           #>   <list>         <chr> <list>            <list>           #> 1 <split [25/7]> Fold1 <tibble [14 × 6]> <tibble [0 × 4]> #> 2 <split [25/7]> Fold2 <tibble [14 × 6]> <tibble [0 × 4]> #> 3 <split [26/6]> Fold3 <tibble [14 × 6]> <tibble [0 × 4]> #> 4 <split [26/6]> Fold4 <tibble [14 × 6]> <tibble [0 × 4]> #> 5 <split [26/6]> Fold5 <tibble [14 × 6]> <tibble [0 × 4]>  show_best(svm_res, metric = \"rmse\") #> # A tibble: 5 × 8 #>        cost   rbf_sigma .metric .estimator  mean     n std_err .config  #>       <dbl>       <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>    #> 1  5.66     0.0215      rmse    standard    2.69     5   0.186 Preproc… #> 2  0.0312   1           rmse    standard    5.82     5   0.946 Preproc… #> 3 32        0.000000215 rmse    standard    5.95     5   0.969 Preproc… #> 4  0.177    0.00001     rmse    standard    5.96     5   0.970 Preproc… #> 5  0.000977 0.000464    rmse    standard    5.96     5   0.970 Preproc…  autoplot(svm_res, metric = \"rmse\") +   scale_x_log10() #> Warning: NaNs produced #> Warning: log-10 transformation introduced infinite values. #> Warning: Removed 10 rows containing missing values or values outside the scale #> range (`geom_point()`).   # ---------------------------------------------------------------------------  # Using a variables preprocessor with a workflow  # Rather than supplying a preprocessor (like a recipe) and a model directly # to `tune_grid()`, you can also wrap them up in a workflow and pass # that along instead (note that this doesn't do any preprocessing to # the variables, it passes them along as-is). wf <- workflow() %>%   add_variables(outcomes = mpg, predictors = everything()) %>%   add_model(svm_mod)  set.seed(3254) svm_res_wf <- tune_grid(wf, resamples = folds, grid = 7)"},{"path":"https://tune.tidymodels.org/dev/news/index.html","id":"tune-development-version","dir":"Changelog","previous_headings":"","what":"tune (development version)","title":"tune (development version)","text":"package now warn parallel processing enabled foreach future. See ?parallelism learn transitioning code future (#878, #866). package now log backtrace errors warnings occur tuning. tuning process encounters issues, see new trace column collect_notes(.Last.tune.result) output find precisely error occurred (#873). automatic grids used, dials::grid_space_filling() now used (instead dials::grid_latin_hypercube()). Overall, new function produces optimized designs (depending random numbers). using Bayesian models, use Latin Hypercube since produce 5,000 candidates, slow pre-optimized designs.","code":""},{"path":"https://tune.tidymodels.org/dev/news/index.html","id":"tune-121","dir":"Changelog","previous_headings":"","what":"tune 1.2.1","title":"tune 1.2.1","text":"CRAN release: 2024-04-18 Addressed issue int_pctl() function error parallelized using makePSOCKcluster() (#885). Addressed issue tuning functions raise error object 'iteration' found plan(multisession) control option parallel_over = \"everything\" (#888).","code":""},{"path":"https://tune.tidymodels.org/dev/news/index.html","id":"tune-120","dir":"Changelog","previous_headings":"","what":"tune 1.2.0","title":"tune 1.2.0","text":"CRAN release: 2024-03-20","code":""},{"path":"https://tune.tidymodels.org/dev/news/index.html","id":"new-features-1-2-0","dir":"Changelog","previous_headings":"","what":"New Features","title":"tune 1.2.0","text":"tune now fully supports models “censored regression” mode. models can fit, tuned, evaluated like regression classification modes. tidymodels.org information tutorials work survival analysis models. Introduced support parallel processing using future framework. tune package previously supported parallelism foreach, users can use either framework now. future release, tune begin deprecation cycle parallelism foreach, encourage users begin migrating code now. See Parallel Processing section “Optimizations” article learn (#866). Added type argument collect_metrics() indicate desired output format. default, type = \"long\", returns output , type = \"wide\" pivots output metric column (#839). Added new function, compute_metrics(), allows computing new metrics evaluating resamples. arguments output formats closely related collect_metrics(), function requires input generated control option save_pred = TRUE additionally takes metrics argument metric set new metrics compute. allows computing new performance metrics without requiring users re-fit re-predict model (#663). method rsample’s int_pctl() function compute percentile confidence intervals performance metrics objects produced fit_resamples(), tune_*(), last_fit(). Brier score now part default metric set classification models.","code":""},{"path":"https://tune.tidymodels.org/dev/news/index.html","id":"bug-fixes-1-2-0","dir":"Changelog","previous_headings":"","what":"Bug Fixes","title":"tune 1.2.0","text":"last_fit() now error supplied fitted workflow (#678). Fixes bug .notes entries sorted wrong order tuning results resampling schemes IDs aren’t already alphabetical order (#728). Fixes bug .config entries .extracts column tune_bayes() output didn’t align entries .metrics .predictions columns (#715). Metrics apparent resamples longer included estimating performance estimate_tune_results() (thus collect_metrics(..., summarize = TRUE) compute_metrics(..., summarize = TRUE), #714). Handles edge cases tune_bayes()’ iter argument soundly. iter = 0, output tune_bayes() match tune_grid(), tune_bayes() now error iter < 0. tune_bayes() now alter state RNG slightly differently, resulting changed Bayesian optimization search output (#720). augment() methods tune_results, resample_results, last_fit objects now always return tibbles (#759).","code":""},{"path":"https://tune.tidymodels.org/dev/news/index.html","id":"other-changes-1-2-0","dir":"Changelog","previous_headings":"","what":"Other Changes","title":"tune 1.2.0","text":"Improved error message needed packages aren’t installed (#727). augment() methods tune_results, resample_results, last_fit objects now always returns tibbles (#759). Improves documentation related hyperparameters associated extracted objects generated submodels. See “Extracting submodels” section ?collect_extracts learn . eval_time eval_time_target attribute added tune objects. also .get_tune_eval_times() .get_tune_eval_time_target() functions. collect_predictions() now reorders columns prediction columns come first (#798). augment() methods tune_results, resample_results, last_fit objects now return prediction results first columns (#761). autoplot() now meaningfully error 1 grid point present, rather producing plot (#775). Added notes case weight usage several functions (#805). iterative optimization routines, autoplot() use integer breaks type = \"performance\" type = \"parameters\".","code":""},{"path":"https://tune.tidymodels.org/dev/news/index.html","id":"breaking-changes-1-2-0","dir":"Changelog","previous_headings":"","what":"Breaking Changes","title":"tune 1.2.0","text":"Several functions gained eval_time argument evaluation time dynamic metrics censored regression. placement argument breaks passing--position one arguments autoplot.tune_results() developer-focused check_initial() (#857). Ellipses (…) now used consistently package require optional arguments named. functions previously ellipses end function signature, moved follow last argument without default value: applies augment.tune_results(), collect_predictions.tune_results(), collect_metrics.tune_results(), select_best.tune_results(), show_best.tune_results(), developer-focused estimate_tune_results(), load_pkgs(), encode_set(). Several functions previously ellipses signatures gained : applies conf_mat_resampled() developer-focused check_workflow(). Optional arguments previously passed position now error informatively prompting named. changes don’t apply cases ellipses currently use forward arguments functions (#863).","code":""},{"path":"https://tune.tidymodels.org/dev/news/index.html","id":"tune-112","dir":"Changelog","previous_headings":"","what":"tune 1.1.2","title":"tune 1.1.2","text":"CRAN release: 2023-08-23 last_fit() now works 3-way validation split objects rsample::initial_validation_split(). last_fit() fit_best() now new argument add_validation_set include exclude validation set dataset used fit model (#701). Disambiguates verbose verbose_iter control options better align documented functionality. former controls logging general progress updates, latter Bayesian search process. (#682)","code":""},{"path":"https://tune.tidymodels.org/dev/news/index.html","id":"tune-111","dir":"Changelog","previous_headings":"","what":"tune 1.1.1","title":"tune 1.1.1","text":"CRAN release: 2023-04-11 Fixed bug introduced tune 1.1.0 collect_() functions .iter column dropped.","code":""},{"path":"https://tune.tidymodels.org/dev/news/index.html","id":"tune-110","dir":"Changelog","previous_headings":"","what":"tune 1.1.0","title":"tune 1.1.0","text":"CRAN release: 2023-04-04 tune 1.1.0 introduces number new features bug fixes, accompanied various optimizations substantially decrease total evaluation time tune hyperparameters tidymodels.","code":""},{"path":"https://tune.tidymodels.org/dev/news/index.html","id":"new-features-1-1-0","dir":"Changelog","previous_headings":"","what":"New features","title":"tune 1.1.0","text":"Introduced new function fit_best() provides shorthand interface fit final model parameter tuning. (#586) Refined machinery logging issues tuning. Rather printing warnings errors appear, package now print unique tuning issues, updating dynamic summary message maintains counts unique issue. feature enabled tuning sequentially can manually toggled verbose option. (#588) Introduced collect_extracts(), function collecting extracted objects tuning results. format results closely mirrors collect_notes(), extracted objects contained list-column alongside resample ID workflow .config. (#579)","code":""},{"path":"https://tune.tidymodels.org/dev/news/index.html","id":"bug-fixes-1-1-0","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"tune 1.1.0","text":"Fixed bug select_by_pct_loss() model greatest loss within limit returned rather simple model whose loss within limit. (#543) Fixed bug tune_bayes() .Last.tune.result return intermediate tuning results. (#613) Extended show_best(), select_best(), select_by_one_std_error(), select_by_pct_loss() accommodate metrics target value zero (notably, yardstick::mpe() yardstick::msd()). (#243)","code":""},{"path":"https://tune.tidymodels.org/dev/news/index.html","id":"other-changes-1-1-0","dir":"Changelog","previous_headings":"","what":"Other changes","title":"tune 1.1.0","text":"Implemented various optimizations tune’s backend substantially decrease total evaluation time tune hyperparameters tidymodels. (#634, #635, #636, #637, #640, #641, #642, #648, #649, #653, #656, #657) Allowed users supply list-columns grid arguments. change allows manually specifying grid values must contained list-columns, like functions lists. (#625) Clarified error messages select_by_* functions. Error messages now note entries ... likely candidates failure arrange(), error messages longer duplicated entry .... Improved condition handling errors occur extraction workflows. messages warnings appropriately handled, errors occurring due misspecified extract() functions supplied control_*() functions silently caught. warnings, errors now surfaced execution print() (#575). Moved forward deprecation parameters() methods workflows, model_specs, recipes. methods now warn every usage defunct later release package. (#650) Various bug fixes improvements documentation.","code":""},{"path":"https://tune.tidymodels.org/dev/news/index.html","id":"tune-101","dir":"Changelog","previous_headings":"","what":"tune 1.0.1","title":"tune 1.0.1","text":"CRAN release: 2022-10-09 last_fit(), fit_resamples(), tune_grid(), tune_bayes() automatically error wrong type control object passed. passed control object superset one needed, function still error. example, passing control_grid() tune_bayes() fail passing control_bayes() tune_grid() . (#449) collect_metrics() method racing objects removed (now finetune package). Improved prompts related parameter tuning. tuning parameters supplied compatible given engine, tune_*() functions now error. (#549) control_bayes() got new argument verbose_iter used control verbosity Bayesian calculations. change means verbose argument passed tune_grid() control verbosity. control_last_fit() function gained argument allow_par defaults FALSE. change addresses failures last_fit() using modeling engines require native serialization, anticipate little increase time--fit resulting change. (#539, tidymodels/bonsai#52) show_notes() better jobs … showing notes. (#558)","code":""},{"path":"https://tune.tidymodels.org/dev/news/index.html","id":"tune-100","dir":"Changelog","previous_headings":"","what":"tune 1.0.0","title":"tune 1.0.0","text":"CRAN release: 2022-07-07 show_notes() new function can better help understand warnings errors. Logging occurs using tuning resampling functions now show multi-line error messages warnings multiple lines. fit_resamples(), last_fit(), tune_grid(), tune_bayes() complete without error (even models fail), results also available via .Last.tune.result. last_fit() now accepts control argument allow users control aspects last fitting process via control_last_fit() (#399). Case weights enabled models can use . internal functions exported use packages. check added fit_resamples() last_fit() give informative error message preprocessor model parameters marked tuning. outcome_names() works correctly recipe NA roles. (#518)","code":""},{"path":"https://tune.tidymodels.org/dev/news/index.html","id":"tune-020","dir":"Changelog","previous_headings":"","what":"tune 0.2.0","title":"tune 0.2.0","text":"CRAN release: 2022-03-18 .notes column now contains information type note (error warning), location occurred, note. Printing tune result different output describing notes. collect_notes() can used gather notes tibble. (#363) Parallel processing PSOCK clusters now efficient, due carefully avoiding sending extraneous information worker (#384, #396). engine arguments xgboost alpha, lambda, scale_pos_weight now tunable. Bayesian optimization data contain missing values, removed fitting GP model. metrics missing, GP fit current results returned. (#432) Moved tune() tune hardhat (#442). parameters() methods recipe, model_spec, workflow objects soft-deprecated favor extract_parameter_set_dials() methods (#428).","code":""},{"path":"https://tune.tidymodels.org/dev/news/index.html","id":"tune-016","dir":"Changelog","previous_headings":"","what":"tune 0.1.6","title":"tune 0.1.6","text":"CRAN release: 2021-07-21 using load_pkgs(), packages use random numbers start-affect state RNG. also added control RNGkind make consistent user’s previous value (#389). New extract_*() functions added supersede many existing pull_*() functions. part larger move across tidymodels packages towards family generic extract_*() functions. Many pull_*() functions soft-deprecated, eventually removed. (#378)","code":""},{"path":"https://tune.tidymodels.org/dev/news/index.html","id":"tune-015","dir":"Changelog","previous_headings":"","what":"tune 0.1.5","title":"tune 0.1.5","text":"CRAN release: 2021-04-23 Fixed bug resampled confusion matrix transposed conf_mat_resamped(tidy = FALSE) (#372) False positive warnings longer occur using doFuture package parallel processing (#377)","code":""},{"path":"https://tune.tidymodels.org/dev/news/index.html","id":"tune-014","dir":"Changelog","previous_headings":"","what":"tune 0.1.4","title":"tune 0.1.4","text":"CRAN release: 2021-04-20 Fixed issue finalize_recipe() failed tuning recipe steps contain multiple tune() parameters single step. Changed conf_mat_resampled() return type object yardstick::conf_mat() tidy = FALSE (#370). automatic parameter machinery sample_size C5.0 engine changes use dials::sample_prop().","code":""},{"path":"https://tune.tidymodels.org/dev/news/index.html","id":"tune-013","dir":"Changelog","previous_headings":"","what":"tune 0.1.3","title":"tune 0.1.3","text":"CRAN release: 2021-02-28 rsample::pretty() methods extended tune_results objects. Added pillar methods formatting tune objects list columns. method .get_fingerprint() added. helps determine tune objects used resamples.","code":""},{"path":"https://tune.tidymodels.org/dev/news/index.html","id":"tune-012","dir":"Changelog","previous_headings":"","what":"tune 0.1.2","title":"tune 0.1.2","text":"CRAN release: 2020-11-17 collect_predictions() made generic. default tuning parameter SVM polynomial degree switched dials::degree() dials::prod_degree() since must integer.","code":""},{"path":"https://tune.tidymodels.org/dev/news/index.html","id":"bug-fixes-0-1-2","dir":"Changelog","previous_headings":"","what":"Bug Fixes","title":"tune 0.1.2","text":"last_fit() workflows::fit() now give identical results workflow underlying model uses random number generation (#300). Fixed issue recipe tuning parameters randomly matched tuning grid incorrectly (#316). last_fit() longer accidentally adjusts random seed (#264). Fixed two bugs acquisition function calculations.","code":""},{"path":"https://tune.tidymodels.org/dev/news/index.html","id":"other-changes-0-1-2","dir":"Changelog","previous_headings":"","what":"Other Changes","title":"tune 0.1.2","text":"New parallel_over control argument adjust parallel processing method tune uses. .config column appears returned tibble tuning fitting resamples changed slightly. now always form \"Preprocessor<>_Model<j>\". predict() can now called workflow returned last_fit() (#294, #295, #296). tune now supports setting event_level option yardstick control objects (.e. control_grid(event_level = \"second\")) (#240, #249). tune now supports workflows created new workflows::add_variables() preprocessor. Better control random number streams parallel tune_grid() fit_resamples() (#11) Allow ... pass options tune_bayes() GPfit::GP_fit(). Additional checks done initial grid given tune_bayes(). initial grid small relative number model terms, warning issued. grid single point, error occurs. (#269) Formatting messages created tune_bayes() now respect width wrap lines using new message_wrap() function. tune functions (tune_grid(), tune_bayes(), etc) now error model specification model workflow given first argument (soft deprecation period ). augment() method added objects generated tune_*(), fit_resamples(), last_fit().","code":""},{"path":"https://tune.tidymodels.org/dev/news/index.html","id":"tune-011","dir":"Changelog","previous_headings":"","what":"tune 0.1.1","title":"tune 0.1.1","text":"CRAN release: 2020-07-08","code":""},{"path":"https://tune.tidymodels.org/dev/news/index.html","id":"breaking-changes-0-1-1","dir":"Changelog","previous_headings":"","what":"Breaking Changes","title":"tune 0.1.1","text":"autoplot.tune_results() now requires objects made version 0.1.0 higher tune. tune objects longer keep rset class resamples argument.","code":""},{"path":"https://tune.tidymodels.org/dev/news/index.html","id":"other-changes-0-1-1","dir":"Changelog","previous_headings":"","what":"Other Changes","title":"tune 0.1.1","text":"autoplot.tune_results() now produces different plot tuning grid regular grid (.e. factorial nearly factorial nature). 5+ parameters, standard plot produced. Non-regular grids plotted way (although see next bullet point). See ?autoplot.tune_results information. autoplot.tune_results() now transforms parameter values plot. example, penalty parameter used regularized regression, points plotted log-10 scale (default transformation). non-regular grids, facet labels show transformation type (e.g. \"penalty (log-10)\" \"cost (log-2)\"). regular grid, x-axis scaled using scale_x_continuous(). Finally, autoplot.tune_results() now shows parameter labels plot. example, k-nearest neighbors model used neighbors = tune(), parameter labeled \"# Nearest Neighbors\". ID used, neighbors = tune(\"K\"), used identify parameter. plotting news, coord_obs_pred() included regression models. plotting observed predicted values model, forces x- y-axis range uses aspect ratio 1. outcome names saved attribute called outcomes objects class tune_results. Also, several accessor functions (named `.get_tune_*()) added easily access attributes. conf_mat_resampled() computes average confusion matrix across resampling statistics single model. show_best(), select_*() functions now use first metric metric set metric supplied. filter_parameters() can trim .metrics column unwanted results (well columns .predictions .extracts) tune_* objects. concert dials > 0.0.7, tuning engine-specific arguments possible. Many known engine-specific tuning parameters handled automatically. grid given, parameters need finalized used tune_*() functions. Added save_workflow argument control_* functions result workflow object used carry tuning/fitting (regardless whether formula recipe given input function) appended resulting tune_results object workflow attribute. new .get_tune_workflow() function can used access workflow. Many output columns tune_results object additional column called .config. meant unique, qualitative value used sorting merging. values also correspond messages logging produced verbose = TRUE.","code":""},{"path":"https://tune.tidymodels.org/dev/news/index.html","id":"tune-010","dir":"Changelog","previous_headings":"","what":"tune 0.1.0","title":"tune 0.1.0","text":"CRAN release: 2020-04-02","code":""},{"path":"https://tune.tidymodels.org/dev/news/index.html","id":"breaking-changes-0-1-0","dir":"Changelog","previous_headings":"","what":"Breaking Changes","title":"tune 0.1.0","text":"arguments main tuning/fitting functions (tune_grid(), tune_bayes(), etc) reordered better align parsnip’s fit(). first argument functions now model specification model workflow. previous versions soft-deprecated 0.1.0 deprecated 0.1.2.","code":""},{"path":"https://tune.tidymodels.org/dev/news/index.html","id":"other-changes-0-1-0","dir":"Changelog","previous_headings":"","what":"Other Changes","title":"tune 0.1.0","text":"Added packages fully loaded workers run parallel using doParallel (#157), (#159), (#160) collect_predictions() gains two new arguments. parameters allows pre-filtering hold-predictions tuning parameters values. interested one sub-model, makes things much faster. option summarize used resampling method training set rows predicted multiple holdout sets. select_best(), select_by_one_std_err(), select_by_pct_loss() longer redundant maximize argument (#176). metric set yardstick now direction (maximize vs. minimize) built .","code":""},{"path":"https://tune.tidymodels.org/dev/news/index.html","id":"bug-fixes-0-1-0","dir":"Changelog","previous_headings":"","what":"Bug Fixes","title":"tune 0.1.0","text":"tune_bayes() longer errors recipe, tuning parameters, combination parameter set, defaults contain unknown values (#168).","code":""},{"path":"https://tune.tidymodels.org/dev/news/index.html","id":"tune-001","dir":"Changelog","previous_headings":"","what":"tune 0.0.1","title":"tune 0.0.1","text":"CRAN release: 2020-02-11 CRAN release. Changed license MIT","code":""},{"path":"https://tune.tidymodels.org/dev/news/index.html","id":"tune-0009002","dir":"Changelog","previous_headings":"","what":"tune 0.0.0.9002","title":"tune 0.0.0.9002","text":"... arguments tune_grid() tune_bayes() moved forward force optional arguments named. New fit_resamples() fitting set resamples don’t require tuning. Changed summarise.tune_results() back estimate.tune_results()","code":""},{"path":"https://tune.tidymodels.org/dev/news/index.html","id":"tune-0009000","dir":"Changelog","previous_headings":"","what":"tune 0.0.0.9000","title":"tune 0.0.0.9000","text":"Added NEWS.md file track changes package.","code":""}]
